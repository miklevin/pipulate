# ðŸš¨ **THE ULTIMATE PIPULATE GUIDE - PART 2: ADVANCED PATTERNS** ðŸš¨

## **PRIORITY 11: The Finalization System (WORKFLOW LIFECYCLE)**

**Question**: How does workflow finalization work and why is it important?

**Answer**: Finalization locks workflows to prevent accidental changes:

```python
# Checking finalization status
finalize_data = pip.get_step_data(pipeline_id, 'finalize', {})
is_finalized = 'finalized' in finalize_data

# Finalizing a workflow
await pip.finalize_workflow(pipeline_id)
# Sets: state['finalize']['finalized'] = True

# Unfinalizing (unlocking)
await pip.unfinalize_workflow(pipeline_id)
# Removes the finalized flag

# In step handlers - Phase 1 (Finalize Phase)
if 'finalized' in finalize_data:
    return Div(
        Card(H3(f'ðŸ”’ {step.show}: {user_val}')),  # Locked view
        Div(id=next_step_id, hx_get=f'/{app_name}/{next_step_id}', hx_trigger='load'),
        id=step_id
    )
```

### **ðŸš¨ CRITICAL RULES:**
- Finalized workflows show locked views (ðŸ”’) for all steps
- Users must explicitly unfinalize to make changes
- Prevents accidental data loss in completed workflows
- The finalize step is always the last step in `self.steps`

### **âœ… FINALIZE STEP IMPLEMENTATION:**
```python
async def finalize(self, request):
    pip, db, steps, app_name = (self.pipulate, self.db, self.steps, self.APP_NAME)
    pipeline_id = db.get('pipeline_id', 'unknown')
    state = pip.read_state(pipeline_id)
    finalize_data = pip.get_step_data(pipeline_id, 'finalize', {})
    
    if 'finalized' in finalize_data:
        # Already finalized - show unfinalize option
        return Card(
            H3('ðŸ”’ Workflow Finalized'),
            P('This workflow is locked to prevent changes.'),
            Form(
                Button('ðŸ”“ Unfinalize', type='submit', cls='secondary'),
                hx_post=f'/{app_name}/unfinalize',
                hx_target='#finalize'
            ),
            id='finalize'
        )
    else:
        # Show finalize option
        return Card(
            H3('Finalize Workflow'),
            P('Lock this workflow to prevent further changes.'),
            Form(
                Button('ðŸ”’ Finalize', type='submit', cls='primary'),
                hx_post=f'/{app_name}/finalize_submit',
                hx_target='#finalize'
            ),
            id='finalize'
        )
```

---

## **PRIORITY 12: The Revert System (USER EXPERIENCE)**

**Question**: How does the revert functionality work and what are the UI patterns?

**Answer**: Reverting allows users to go back and modify previous steps:

```python
# Revert button in completed steps
pip.display_revert_header(
    step_id=step_id,
    app_name=app_name, 
    message=f'{step.show}: {user_val}',
    steps=steps
)

# Revert handler
async def handle_revert(self, request):
    pip, db, app_name = (self.pipulate, self.db, self.APP_NAME)
    form = await request.form()
    step_id = form.get('step_id')
    pipeline_id = db.get('pipeline_id', 'unknown')
    
    if not step_id:
        return P('Error: No step specified for revert.', style=pip.get_style('error'))
    
    # Clear all steps after the reverted step
    await pip.clear_steps_from(pipeline_id, step_id, self.steps)
    
    # Set revert target flag
    state = pip.read_state(pipeline_id)
    state['_revert_target'] = step_id
    pip.write_state(pipeline_id, state)
    
    # Rebuild entire workflow UI
    return pip.rebuild(app_name, self.steps)
```

### **Revert Flow:**
1. User clicks revert button on completed step
2. All subsequent step data is cleared
3. `_revert_target` flag is set
4. Entire workflow UI rebuilds
5. Target step shows input form (Phase 3)
6. Chain reaction resumes from that point

### **ðŸš¨ CRITICAL RULE:**
The `_revert_target` flag determines which step shows input form vs completed view in the three-phase pattern.

---

## **PRIORITY 13: File Operations and Downloads (DATA HANDLING)**

**Question**: How does Pipulate handle file uploads and downloads?

**Answer**: Plugin-specific namespaced file handling:

```python
# File uploads in forms
Form(
    Input(type="file", name="uploaded_files", multiple=True),
    Button("Upload", type="submit"),
    enctype="multipart/form-data",  # Required for file uploads
    hx_post=f'/{app_name}/{step_id}_submit',
    hx_target=f'#{step_id}'
)

# Processing uploads
async def step_XX_submit(self, request):
    form = await request.form()
    uploaded_files = form.getlist("uploaded_files")
    
    for file_upload in uploaded_files:
        if file_upload.filename:
            # Plugin-specific directory structure
            save_path = Path("downloads") / self.APP_NAME / pipeline_id / file_upload.filename
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Stream large files
            with open(save_path, "wb") as f:
                async for chunk in file_upload.chunks():
                    f.write(chunk)

# Downloads with security
@rt(f'/{self.APP_NAME}/download_file')
async def serve_file(self, request):
    file_ref = request.query_params.get('file_ref')
    # CRITICAL: Validate and sanitize file_ref to prevent directory traversal
    file_path = self.validate_file_path(file_ref)  # Your validation logic
    return FileResponse(file_path, filename=file_path.name)
```

### **Directory Structure:**
```
downloads/
  my_workflow/           # APP_NAME namespace
    default-myworkflow-01/  # pipeline_id
      uploaded_file.csv
    default-myworkflow-02/
      another_file.txt
  other_workflow/        # Different plugin namespace
    data/
      output.csv
```

### **ðŸš¨ SECURITY CRITICAL:**
Always validate file paths to prevent directory traversal attacks:

```python
def validate_file_path(self, file_ref):
    # Sanitize and validate file reference
    safe_path = Path("downloads") / self.APP_NAME / file_ref
    # Ensure path is within allowed directory
    if not str(safe_path.resolve()).startswith(str(Path("downloads").resolve())):
        raise ValueError("Invalid file path")
    return safe_path
```

---

## **PRIORITY 14: Environment and Nix Integration (DEVELOPMENT SETUP)**

**Question**: How does the Nix environment work and what are the key commands?

**Answer**: Pipulate uses Nix Flakes for reproducible environments:

```bash
# ALWAYS run this first in new terminal
nix develop                    # Full startup: git pull, install deps, start servers
nix develop .#quiet           # Quiet mode: just activate environment
nix develop .#quiet --command python script.py  # Run specific command

# What nix develop does:
# 1. git pull (auto-updates)
# 2. Sets up Python .venv
# 3. Installs/updates requirements.txt
# 4. Starts JupyterLab (background)
# 5. Starts Pipulate server (foreground)
# 6. Opens browser tabs
```

### **Key Files:**
- `flake.nix` - Environment definition (Python version, system packages)
- `requirements.txt` - Python packages (managed by pip within Nix)
- `data/environment.txt` - Current environment name

### **ðŸš¨ CRITICAL RULES:**
- ALWAYS use `nix develop` before any project commands
- The environment is completely isolated and reproducible
- Auto-updates on entry ensure latest code
- Cross-platform: macOS, Linux, Windows (WSL)

### **Environment Variables:**
```bash
# Set in flake.nix
EFFECTIVE_OS="linux"     # or "darwin" for macOS
PIPULATE_ENV="development"
JUPYTER_PORT="8888"
PIPULATE_PORT="5001"
```

---

## **PRIORITY 15: Plugin Development Lifecycle (WORKFLOW CREATION)**

**Question**: What's the recommended workflow for developing new plugins?

**Answer**: Structured development process:

```bash
# 1. Start with template copy
cp plugins/500_hello_workflow.py plugins/xx_my_new_workflow.py

# 2. Development naming (prevents auto-registration)
# Use: xx_ prefix OR parentheses in filename
# Examples: xx_my_workflow.py, my_workflow (Copy).py

# 3. Customize core elements
class MyWorkflow:
    APP_NAME = 'my_workflow_internal'  # Stable internal ID
    DISPLAY_NAME = 'My Workflow'       # UI display name
    
    steps = [
        Step(id='step_01', done='my_data', show='My Step', refill=True),
        # ... more steps
    ]

# 4. Test iteratively
# Remove xx_ or parentheses to enable auto-registration
mv plugins/xx_my_workflow.py plugins/999_my_workflow.py

# 5. Finalize position
# Choose final numeric prefix for menu order
git mv plugins/999_my_workflow.py plugins/075_my_workflow.py
```

### **File Naming Rules:**
- `{number}_{name}.py` - Auto-registered, menu order by number
- `xx_{name}.py` or `XX_{name}.py` - Skipped (development)
- `{name} (Copy).py` - Skipped (contains parentheses)
- `__*.py` - Skipped (Python convention)

### **âœ… DEVELOPMENT CHECKLIST:**
1. **Copy template** - Start with working example
2. **Use development naming** - Prevent auto-registration during development
3. **Customize APP_NAME** - Unique, stable identifier
4. **Define steps** - Include finalize step
5. **Test incrementally** - Enable registration when ready
6. **Choose final position** - Numeric prefix for menu order

---

## **PRIORITY 16: Error Handling and Debugging (TROUBLESHOOTING)**

**Question**: How do you debug workflow issues and what are common problems?

**Answer**: Multi-layered debugging approach:

```python
# 1. Enable debug mode in server.py
DEBUG_MODE = True
STATE_TABLES = True  # Prints db state after each request

# 2. Browser Developer Tools
# Network tab: Check HTMX requests/responses
# Console: Look for JavaScript errors
# Elements: Inspect DOM structure and HTMX attributes

# 3. Server logs
# Console output: Colored Loguru logging
# File logs: logs/{APP_NAME}.log

# 4. Add debug logging in workflows
from loguru import logger
logger.debug(f"Step {step_id} state: {step_data}")
logger.bind(plugin=self.APP_NAME).info("Processing step")

# 5. State inspection
pip.append_to_history(f"DEBUG: State = {state}", role='system')
```

### **Common Issues and Solutions:**

1. **Missing `hx_trigger='load'`** - Breaks chain reaction
   ```python
   # Check: Does completed step have hx_trigger?
   Div(id=next_step_id, hx_get=f'/{app_name}/{next_step_id}', hx_trigger='load')
   ```

2. **Missing `request` parameter** - Routing errors  
   ```python
   # Check: All handlers accept request?
   async def step_01(self, request):  # âœ… Correct
   ```

3. **Wrong `hx_target`** - UI updates wrong element
   ```python
   # Check: Target matches step ID?
   hx_target=f'#{step_id}'  # Must match id=step_id
   ```

4. **Missing auto-key generation** - Empty input handling
   ```python
   # Check: Empty input returns HX-Refresh?
   if not user_input:
       response = Response('')
       response.headers['HX-Refresh'] = 'true'
       return response
   ```

5. **Incorrect three-phase logic** - Steps don't display properly
   ```python
   # Check: Correct order?
   if 'finalized' in finalize_data:      # Phase 1
   elif user_val and state.get('_revert_target') != step_id:  # Phase 2
   else:                                 # Phase 3
   ```

### **Debug Helper Functions:**
```python
def debug_state(self, pipeline_id, step_id):
    """Debug helper to inspect workflow state"""
    state = self.pipulate.read_state(pipeline_id)
    step_data = self.pipulate.get_step_data(pipeline_id, step_id, {})
    logger.debug(f"Full state: {state}")
    logger.debug(f"Step {step_id} data: {step_data}")
    return state, step_data

def debug_htmx_attributes(self, element):
    """Debug helper to check HTMX attributes"""
    attrs = element.attrs if hasattr(element, 'attrs') else {}
    htmx_attrs = {k: v for k, v in attrs.items() if k.startswith('hx_')}
    logger.debug(f"HTMX attributes: {htmx_attrs}")
    return htmx_attrs
```

---

## **PRIORITY 17: The WET vs DRY Philosophy (DESIGN PRINCIPLE)**

**Question**: Why does Pipulate use "WET" (Write Everything Twice) for workflows?

**Answer**: Intentional design choice for maintainability:

```python
# WET Workflows - Explicit and customizable
async def step_01(self, request):
    # Full three-phase implementation in each step
    if 'finalized' in finalize_data:
        # Explicit finalize phase
    elif user_val and state.get('_revert_target') != step_id:
        # Explicit revert phase  
    else:
        # Explicit input phase

# vs DRY CRUD - Reusable base classes
class TaskApp(BaseCrud):
    # Inherits all CRUD operations
    def render_item(self, item):
        # Only customize display logic
```

### **Rationale:**
- **Workflows**: Complex, unique business logic â†’ WET for clarity
- **CRUD**: Standard data operations â†’ DRY for efficiency
- **Portability**: Easy to copy/modify from Jupyter notebooks
- **Debugging**: Clear, explicit code paths
- **Customization**: No hidden abstractions

### **When to Use Each:**
- **WET for workflows**: Each step has unique logic, UI, validation
- **DRY for utilities**: Shared helpers, common patterns, infrastructure
- **WET for plugins**: Self-contained, portable, customizable
- **DRY for framework**: Core Pipulate functionality, shared services

---

## **PRIORITY 18: Browser Automation Integration (SELENIUM)**

**Question**: How does Pipulate integrate with Selenium for browser automation?

**Answer**: Cross-platform WebDriver setup:

```python
# Environment detection
import os
effective_os = os.environ.get('EFFECTIVE_OS', 'unknown')

# Platform-specific driver setup
if effective_os == 'linux':
    # Nix provides chromedriver in PATH
    service = ChromeService()
elif effective_os == 'darwin':
    # macOS uses webdriver-manager
    from webdriver_manager.chrome import ChromeDriverManager
    service = ChromeService(ChromeDriverManager().install())

# Chrome options
chrome_options = Options()
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--start-maximized")

# WebDriver initialization
driver = webdriver.Chrome(service=service, options=chrome_options)
try:
    driver.get("https://example.com")
    # Automation logic
finally:
    driver.quit()  # CRITICAL: Always cleanup

# Async integration (avoid blocking)
await asyncio.to_thread(driver.get, url)
title = await asyncio.to_thread(lambda: driver.title)
```

### **Profile Management:**
```python
# Persistent profiles for login sessions
profile_dir = Path.home() / ".pipulate_selenium_profiles" / self.APP_NAME
user_data_dir = profile_dir / "user_data"
profile_name = f"profile_{pipeline_id.replace('-', '_')}"

chrome_options.add_argument(f"--user-data-dir={user_data_dir}")
chrome_options.add_argument(f"--profile-directory={profile_name}")
```

### **ðŸš¨ CRITICAL PATTERNS:**
1. **Always use try/finally** - Ensure driver cleanup
2. **Use asyncio.to_thread** - Prevent blocking the event loop
3. **Platform-specific setup** - Handle different environments
4. **Profile isolation** - Separate profiles per workflow
5. **Error handling** - Graceful degradation on automation failures

---

## **PRIORITY 19: Local LLM Integration (OLLAMA)**

**Question**: How does Pipulate integrate with local LLMs via Ollama?

**Answer**: WebSocket-based streaming chat:

```python
# Chat integration in workflows
await self.message_queue.add(
    pip, 
    "User completed step 1 with value: John",
    verbatim=True,  # Add to history without LLM response
    role='system'
)

# LLM streaming (in server.py)
async def chat_with_llm(MODEL: str, messages: list) -> AsyncGenerator[str, None]:
    async with httpx.AsyncClient() as client:
        response = await client.post(
            'http://localhost:11434/api/chat',
            json={'model': MODEL, 'messages': messages, 'stream': True}
        )
        async for line in response.aiter_lines():
            if line:
                chunk = json.loads(line)
                if 'message' in chunk:
                    yield chunk['message']['content']

# WebSocket chat handler
async def handle_websocket(self, websocket: WebSocket):
    await websocket.accept()
    async for message in websocket.iter_text():
        # Stream LLM response back to client
        async for chunk in chat_with_llm(MODEL, conversation_history):
            await websocket.send_text(chunk)
```

### **Training Context:**
```python
# Workflow-specific training
TRAINING_PROMPT = 'my_workflow.md'  # File in training/ directory

# Auto-loaded context
def build_endpoint_training(endpoint):
    training_file = f"training/{endpoint}.md"
    if Path(training_file).exists():
        return Path(training_file).read_text()
    return ""
```

### **Message Queue Integration:**
```python
# System messages for LLM context
await self.message_queue.add(pip, f"[WORKFLOW STATE] Step {step_id} completed", verbatim=True, role='system')

# User messages for LLM interaction
await self.message_queue.add(pip, "User needs help with next step", verbatim=False, role='user')

# Assistant responses
await self.message_queue.add(pip, "Here's how to proceed...", verbatim=True, role='assistant')
```

**Critical**: The LLM sees workflow state through the message queue, enabling context-aware assistance.

---

## **PRIORITY 20: Advanced State Management Patterns**

**Question**: What are the advanced patterns for complex state management?

**Answer**: Beyond basic step data, Pipulate supports sophisticated state patterns:

### **Conditional Step Logic:**
```python
async def step_02(self, request):
    # Get previous step data
    step_01_data = pip.get_step_data(pipeline_id, 'step_01', {})
    user_choice = step_01_data.get('choice', '')
    
    # Conditional next step
    if user_choice == 'advanced':
        next_step_id = 'step_03_advanced'
    else:
        next_step_id = 'step_03_basic'
    
    # Rest of three-phase logic...
```

### **Cross-Step Data Dependencies:**
```python
# Using transform in Step definition
Step(
    id='step_03',
    done='processed_data',
    show='Processed Result',
    transform=lambda prev_data: f"Processed: {prev_data['raw_input']}"
)

# In step handler
async def step_03(self, request):
    # Get suggestion from transform
    suggestion = await self.get_suggestion(step_id, state)
    # Use in form pre-fill
```

### **Temporary State Management:**
```python
# Store temporary data (not persisted in main state)
temp_data = {
    'processing_status': 'in_progress',
    'temp_files': ['file1.tmp', 'file2.tmp']
}
pip.write_temp_state(pipeline_id, step_id, temp_data)

# Retrieve temporary data
temp_data = pip.read_temp_state(pipeline_id, step_id, {})
```

### **State Validation:**
```python
def validate_workflow_state(self, pipeline_id):
    """Validate entire workflow state for consistency"""
    state = pip.read_state(pipeline_id)
    
    # Check required fields
    for step in self.steps[:-1]:  # Exclude finalize
        step_data = state.get(step.id, {})
        if step.done not in step_data:
            return False, f"Missing data for {step.show}"
    
    # Check data consistency
    if 'step_01' in state and 'step_02' in state:
        # Custom validation logic
        pass
    
    return True, "Valid"
```

---

## **ADVANCED DEBUGGING TECHNIQUES**

### **State Inspection Tools:**
```python
# Add to any step for debugging
def debug_full_context(self, pipeline_id, step_id):
    """Complete context dump for debugging"""
    state = pip.read_state(pipeline_id)
    step_data = pip.get_step_data(pipeline_id, step_id, {})
    finalize_data = pip.get_step_data(pipeline_id, 'finalize', {})
    
    debug_info = {
        'pipeline_id': pipeline_id,
        'current_step': step_id,
        'full_state': state,
        'step_data': step_data,
        'is_finalized': 'finalized' in finalize_data,
        'revert_target': state.get('_revert_target'),
        'completed_steps': [s for s in state.keys() if s != '_revert_target']
    }
    
    logger.debug(f"Full context: {debug_info}")
    return debug_info
```

### **HTMX Request Tracing:**
```python
# Add to step handlers for HTMX debugging
def trace_htmx_request(self, request, step_id):
    """Trace HTMX request details"""
    headers = dict(request.headers)
    htmx_headers = {k: v for k, v in headers.items() if k.lower().startswith('hx-')}
    
    trace_info = {
        'step_id': step_id,
        'method': request.method,
        'url': str(request.url),
        'htmx_headers': htmx_headers,
        'is_htmx': 'hx-request' in headers
    }
    
    logger.debug(f"HTMX trace: {trace_info}")
    return trace_info
```

---

## **PERFORMANCE OPTIMIZATION PATTERNS**

### **Lazy Loading:**
```python
# Only load heavy data when needed
async def step_heavy_data(self, request):
    step_data = pip.get_step_data(pipeline_id, step_id, {})
    
    if 'cached_result' not in step_data:
        # Heavy computation only on first access
        result = await self.expensive_operation()
        await pip.set_step_data(pipeline_id, step_id, {'cached_result': result}, steps)
    else:
        result = step_data['cached_result']
    
    # Use cached result...
```

### **Background Processing:**
```python
# Long-running tasks with polling
async def start_background_task(self, pipeline_id, step_id):
    """Start background task and return immediately"""
    task_id = f"{pipeline_id}_{step_id}_{int(time.time())}"
    
    # Store task reference
    await pip.set_step_data(pipeline_id, step_id, {
        'task_id': task_id,
        'status': 'running',
        'started_at': time.time()
    }, steps)
    
    # Start background task
    asyncio.create_task(self.background_worker(task_id, pipeline_id, step_id))
    
    return task_id

async def check_task_status(self, request):
    """Polling endpoint for task status"""
    step_data = pip.get_step_data(pipeline_id, step_id, {})
    status = step_data.get('status', 'unknown')
    
    if status == 'completed':
        # Return completed view with chain reaction
        return self.completed_view_with_chain()
    else:
        # Return polling view
        return self.polling_view()
```

This completes Part 2 of the Ultimate Pipulate Guide, covering advanced patterns, technical integrations, and sophisticated workflow features. Part 3 will cover specialized use cases, advanced UI patterns, and expert-level techniques. 