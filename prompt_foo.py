# prompt_foo.py (Corrected Synthesis)

import os
import re
import sys
import pydot
import yaml # NEW: For parsing article front matter
import argparse
import tiktoken
import subprocess
import tempfile
import shutil
from datetime import datetime # NEW: For parsing article dates
from typing import Dict, List, Optional, Tuple

try:
    import jupytext
    JUPYTEXT_AVAILABLE = True
except ImportError:
    JUPYTEXT_AVAILABLE = False

# Hello there, AI! This is a tool for generating a single, comprehensive prompt
# from the command line, bundling codebase files and auto-generated context
# into a structured Markdown format for effective AI assistance.

# ============================================================================
# --- Configuration ---
# ============================================================================
def find_repo_root(start_path: str) -> str:
Â  Â  """Find the git repository root from a starting path."""
Â  Â  path = os.path.abspath(start_path)
Â  Â  while path != os.path.dirname(path):
Â  Â  Â  Â  if os.path.isdir(os.path.join(path, '.git')):
Â  Â  Â  Â  Â  Â  return path
Â  Â  Â  Â  path = os.path.dirname(path)
Â  Â  raise FileNotFoundError("Could not find the repository root (.git directory).")

REPO_ROOT = find_repo_root(os.path.dirname(__file__))

# Centralized configuration as recommended in Architectural Analysis (Section VI-B)
CONFIG = {
Â  Â  "PROJECT_NAME": "pipulate",
Â  Â  "POSTS_DIRECTORY": "/home/mike/repos/MikeLev.in/_posts" # NEW: For list_articles logic
}

# ============================================================================
# --- Accurate Literary Size Scale (Word Count Based) ---
# ============================================================================
LITERARY_SIZE_SCALE = [
Â  Â  (3000, "Short Essay"),
Â  Â  (7500, "Short Story"),
Â  Â  (20000, "Novelette"),
Â  Â  (50000, "Novella or a Master's Dissertation"),
Â  Â  (80000, "Average Paperback Novel or a Ph.D. Dissertation"),
Â  Â  (120000, "Long Novel"),
Â  Â  (200000, "Epic Fantasy Novel"),
Â  Â  (500000, "Seriously Long Epic (like 'Infinite Jest')"),
]

def get_literary_perspective(word_count: int, token_word_ratio: float) -> str:
Â  Â  """Get a human-readable literary comparison for the codebase size."""
Â  Â  description = f"Longer than {LITERARY_SIZE_SCALE[-1][1]}"
Â  Â  for words, desc in LITERARY_SIZE_SCALE:
Â  Â  Â  Â  if word_count <= words:
Â  Â  Â  Â  Â  Â  description = desc
Â  Â  Â  Â  Â  Â  break

Â  Â  density_warning = ""
Â  Â  if token_word_ratio > 1.8:
Â  Â  Â  Â  density_warning = (
Â  Â  Â  Â  Â  Â  f" (Note: With a token/word ratio of {token_word_ratio:.2f}, "
Â  Â  Â  Â  Â  Â  f"this content is far denser and more complex than typical prose of this length)."
Â  Â  Â  Â  )

Â  Â  return f"ðŸ“š Equivalent in length to a **{description}**{density_warning}"

# ============================================================================
# --- Restored & Corrected: UML and DOT Context Generation ---
# ============================================================================
def generate_uml_and_dot(target_file: str, project_name: str) -> Dict:
Â  Â  """Generates a UML ASCII diagram and a DOT dependency graph for a target Python file."""
Â  Â  pyreverse_exec = shutil.which("pyreverse")
Â  Â  plantuml_exec = shutil.which("plantuml")

Â  Â  if not pyreverse_exec or not plantuml_exec:
Â  Â  Â  Â  msg = []
Â  Â  Â  Â  if not pyreverse_exec: msg.append("`pyreverse` (from pylint)")
Â  Â  Â  Â  if not plantuml_exec: msg.append("`plantuml`")
Â  Â  Â  Â  return {"ascii_uml": f"Skipping: Required command(s) not found: {', '.join(msg)}."}

Â  Â  target_path = os.path.join(REPO_ROOT, target_file)
Â  Â  if not os.path.exists(target_path):
Â  Â  Â  Â  return {"ascii_uml": f"Skipping: Target file for UML generation not found: {target_path}"}

Â  Â  with tempfile.TemporaryDirectory() as temp_dir:
Â  Â  Â  Â  dot_file_path = os.path.join(temp_dir, "classes.dot")
Â  Â  Â  Â  puml_file_path = os.path.join(temp_dir, "diagram.puml")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- Step 1: Run pyreverse ---
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  pyreverse_cmd = [
Â  Â  Â  Â  Â  Â  Â  Â  pyreverse_exec,
Â  Â  Â  Â  Â  Â  Â  Â  "-f", "dot",
Â  Â  Â  Â  Â  Â  Â  Â  "-o", "dot", # This format is just a prefix
Â  Â  Â  Â  Â  Â  Â  Â  "-p", project_name,
Â  Â  Â  Â  Â  Â  Â  Â  target_path
Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  subprocess.run(
Â  Â  Â  Â  Â  Â  Â  Â  pyreverse_cmd,
Â  Â  Â  Â  Â  Â  Â  Â  check=True,
Â  Â  Â  Â  Â  Â  Â  Â  capture_output=True,
Â  Â  Â  Â  Â  Â  Â  Â  text=True,
Â  Â  Â  Â  Â  Â  Â  Â  cwd=temp_dir
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  generated_dot_name = f"classes_{project_name}.dot"
Â  Â  Â  Â  Â  Â  os.rename(os.path.join(temp_dir, generated_dot_name), dot_file_path)

Â  Â  Â  Â  except (subprocess.CalledProcessError, FileNotFoundError) as e:
Â  Â  Â  Â  Â  Â  error_msg = e.stderr if hasattr(e, 'stderr') else str(e)
Â  Â  Â  Â  Â  Â  return {"ascii_uml": f"Error: pyreverse failed. {error_msg}", "dot_graph": None}

Â  Â  Â  Â  # --- Step 2: Convert DOT to PlantUML ---
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  graphs = pydot.graph_from_dot_file(dot_file_path)
Â  Â  Â  Â  Â  Â  if not graphs:
Â  Â  Â  Â  Â  Â  Â  Â  return {"ascii_uml": f"Note: No classes found in {target_file} to generate a diagram.", "dot_graph": None}
Â  Â  Â  Â  Â  Â  graph = graphs[0]
Â  Â  Â  Â  Â  Â  dot_content = graph.to_string()

Â  Â  Â  Â  Â  Â  puml_lines = ["@startuml", "skinparam linetype ortho", ""]

Â  Â  Â  Â  Â  Â  def sanitize_line(line):
Â  Â  Â  Â  Â  Â  Â  Â  clean = re.sub(r'<br[^>]*>', '', line)
Â  Â  Â  Â  Â  Â  Â  Â  clean = re.sub(r'<[^>]+>', '', clean)
Â  Â  Â  Â  Â  Â  Â  Â  return clean.strip()

Â  Â  Â  Â  Â  Â  for node in graph.get_nodes():
Â  Â  Â  Â  Â  Â  Â  Â  label = node.get_label()
Â  Â  Â  Â  Â  Â  Â  Â  if not label: continue

Â  Â  Â  Â  Â  Â  Â  Â  parts = label.strip('<>{} ').split('|')
Â  Â  Â  Â  Â  Â  Â  Â  class_name = sanitize_line(parts[0])
Â  Â  Â  Â  Â  Â  Â  Â  puml_lines.append(f"class {class_name} {{")

Â  Â  Â  Â  Â  Â  Â  Â  if len(parts) > 1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for attr in re.split(r'<br[^>]*>', parts[1]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  clean_attr = sanitize_line(attr).split(':')[0].strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if clean_attr:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  puml_lines.append(f"Â  - {clean_attr}")

Â  Â  Â  Â  Â  Â  Â  Â  if len(parts) > 2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  method_block = parts[2].strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for method_line in re.split(r'<br[^>]*>', method_block):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  clean_method = sanitize_line(method_line)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if clean_method:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  puml_lines.append(f"Â  + {clean_method}")

Â  Â  Â  Â  Â  Â  Â  Â  puml_lines.append("}\n")

Â  Â  Â  Â  Â  Â  for edge in graph.get_edges():
Â  Â  Â  Â  Â  Â  Â  Â  source_name = edge.get_source().strip('"').split('.')[-1]
Â  Â  Â  Â  Â  Â  Â  Â  dest_name = edge.get_destination().strip('"').split('.')[-1]
Â  Â  Â  Â  Â  Â  Â  Â  puml_lines.append(f"{source_name} ..> {dest_name}")

Â  Â  Â  Â  Â  Â  puml_lines.append("@enduml")
Â  Â  Â  Â  Â  Â  with open(puml_file_path, 'w') as f:
Â  Â  Â  Â  Â  Â  Â  Â  f.write('\n'.join(puml_lines))

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  with open(dot_file_path, 'r') as f:
Â  Â  Â  Â  Â  Â  Â  Â  dot_content_on_error = f.read()
Â  Â  Â  Â  Â  Â  return {"ascii_uml": f"Error: DOT to PUML conversion failed. {str(e)}", "dot_graph": dot_content_on_error}
Â 
Â  Â  Â  Â  # --- Step 3: Run PlantUML ---
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  plantuml_cmd = ["plantuml", "-tutxt", puml_file_path]
Â  Â  Â  Â  Â  Â  subprocess.run(plantuml_cmd, check=True, capture_output=True, text=True, cwd=temp_dir)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  utxt_file_path = puml_file_path.replace(".puml", ".utxt")
Â  Â  Â  Â  Â  Â  with open(utxt_file_path, 'r') as f:
Â  Â  Â  Â  Â  Â  Â  Â  ascii_uml = f.read()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- Normalize whitespace from plantuml output ---
Â  Â  Â  Â  Â  Â  lines = ascii_uml.splitlines()
Â  Â  Â  Â  Â  Â  non_empty_lines = [line for line in lines if line.strip()]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if non_empty_lines:
Â  Â  Â  Â  Â  Â  Â  Â  min_indent = min(len(line) - len(line.lstrip(' ')) for line in non_empty_lines)
Â  Â  Â  Â  Â  Â  Â  Â  dedented_lines = [line[min_indent:] for line in lines]
Â  Â  Â  Â  Â  Â  Â  Â  stripped_lines = [line.rstrip() for line in dedented_lines]
Â  Â  Â  Â  Â  Â  Â  Â  ascii_uml = '\n'.join(stripped_lines)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Prepend a newline to "absorb the chop" from rendering
Â  Â  Â  Â  Â  Â  Â  Â  if ascii_uml:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ascii_uml = '\n' + ascii_uml

Â  Â  Â  Â  except (subprocess.CalledProcessError, FileNotFoundError) as e:
Â  Â  Â  Â  Â  Â  error_msg = e.stderr if hasattr(e, 'stderr') else str(e)
Â  Â  Â  Â  Â  Â  return {"ascii_uml": f"Error: plantuml failed. {error_msg}", "dot_graph": dot_content}

Â  Â  return {"ascii_uml": ascii_uml, "dot_graph": dot_content}


# ============================================================================
# --- NEW: Logic ported from list_articles.py for Narrative Context ---
# ============================================================================
def _get_article_list_data(posts_dir: str = CONFIG["POSTS_DIRECTORY"]) -> List[Dict]:
Â  Â  """
Â  Â  Parses Jekyll posts, sorts them chronologically, and returns a list of dictionaries.
Â  Â  This is a self-contained version of the logic from the `list_articles.py` script.
Â  Â  """
Â  Â  posts_data = []
Â  Â  if not os.path.isdir(posts_dir):
Â  Â  Â  Â  print(f"Warning: Article directory not found at {posts_dir}", file=sys.stderr)
Â  Â  Â  Â  return []

Â  Â  for filename in os.listdir(posts_dir):
Â  Â  Â  Â  if not filename.endswith(('.md', '.markdown')):
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  filepath = os.path.join(posts_dir, filename)
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  date_str = filename[:10]
Â  Â  Â  Â  Â  Â  post_date = datetime.strptime(date_str, '%Y-%m-%d').date()
Â  Â  Â  Â  Â  Â  with open(filepath, 'r', encoding='utf-8') as f:
Â  Â  Â  Â  Â  Â  Â  Â  content = f.read()
Â  Â  Â  Â  Â  Â  if content.startswith('---'):
Â  Â  Â  Â  Â  Â  Â  Â  parts = content.split('---', 2)
Â  Â  Â  Â  Â  Â  Â  Â  front_matter = yaml.safe_load(parts[1]) or {}
Â  Â  Â  Â  Â  Â  Â  Â  posts_data.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'path': filepath,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'date': post_date,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'sort_order': int(front_matter.get('sort_order', 0)),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'title': front_matter.get('title', 'Untitled'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'summary': front_matter.get('meta_description', '')
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  except (ValueError, yaml.YAMLError, IndexError):
Â  Â  Â  Â  Â  Â  continue

Â  Â  sorted_posts = sorted(posts_data, key=lambda p: (p['date'], p['sort_order']))
Â  Â  return sorted_posts

def parse_slice_arg(arg_str: str):
Â  Â  """Parses a string like '[5:10]' or '[5]' into a slice object or an int."""
Â  Â  if not arg_str or not arg_str.startswith('[') or not arg_str.endswith(']'):
Â  Â  Â  Â  return None
Â  Â  content = arg_str[1:-1].strip()
Â  Â  if ':' in content:
Â  Â  Â  Â  parts = content.split(':', 1)
Â  Â  Â  Â  start = int(parts[0].strip()) if parts[0].strip() else None
Â  Â  Â  Â  end = int(parts[1].strip()) if parts[1].strip() else None
Â  Â  Â  Â  return slice(start, end)
Â  Â  elif content:
Â  Â  Â  Â  return int(content)
Â  Â  return None

# ============================================================================
# --- Helper Functions (Tokenizing, File Parsing, Clipboard) ---
# ============================================================================
def count_tokens(text: str, model: str = "gpt-4o") -> int:
Â  Â  """Counts tokens in a text string using tiktoken."""
Â  Â  try:
Â  Â  Â  Â  encoding = tiktoken.encoding_for_model(model)
Â  Â  Â  Â  return len(encoding.encode(text))
Â  Â  except Exception:
Â  Â  Â  Â  return len(text.split())

def count_words(text: str) -> int:
Â  Â  """Counts words in a text string."""
Â  Â  return len(text.split())

def parse_file_list_from_config() -> List[Tuple[str, str]]:
Â  Â  """Loads and parses the file list from foo_files.py."""
Â  Â  try:
Â  Â  Â  Â  import foo_files
Â  Â  Â  Â  files_raw = foo_files.FILES_TO_INCLUDE_RAW
Â  Â  except (ImportError, AttributeError):
Â  Â  Â  Â  print("ERROR: foo_files.py not found or doesn't contain FILES_TO_INCLUDE_RAW.")
Â  Â  Â  Â  sys.exit(1)

Â  Â  lines = files_raw.strip().splitlines()
Â  Â  seen_files, parsed_files = set(), []

Â  Â  for line in lines:
Â  Â  Â  Â  line = line.strip()
Â  Â  Â  Â  if not line or line.startswith('#'):
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  parts = re.split(r'\s*<--\s*|\s*#\s*', line, 1)
Â  Â  Â  Â  file_path = parts[0].strip()
Â  Â  Â  Â  comment = parts[1].strip() if len(parts) > 1 else ""

Â  Â  Â  Â  if file_path and file_path not in seen_files:
Â  Â  Â  Â  Â  Â  seen_files.add(file_path)
Â  Â  Â  Â  Â  Â  parsed_files.append((file_path, comment))
Â  Â  return parsed_files

def copy_to_clipboard(text: str):
Â  Â  """Copies text to the system clipboard using 'xclip'."""
Â  Â  if not shutil.which('xclip'):
Â  Â  Â  Â  print("\nWarning: 'xclip' not found. Cannot copy to clipboard.")
Â  Â  Â  Â  return
Â  Â  try:
Â  Â  Â  Â  subprocess.run(['xclip', '-selection', 'clipboard'], input=text.encode('utf-8'), check=True)
Â  Â  Â  Â  print("Markdown output copied to clipboard")
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"\nWarning: Could not copy to clipboard: {e}")

def run_tree_command() -> str:
Â  Â  """Runs the 'eza' command to generate a tree view that respects .gitignore."""
Â  Â  eza_exec = shutil.which("eza")
Â  Â  if not eza_exec:
Â  Â  Â  Â  return "Skipping: `eza` command not found."
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  result = subprocess.run(
Â  Â  Â  Â  Â  Â  [eza_exec, '--tree', '--git-ignore', '--color=never'],
Â  Â  Â  Â  Â  Â  capture_output=True,
Â  Â  Â  Â  Â  Â  text=True,
Â  Â  Â  Â  Â  Â  cwd=REPO_ROOT,
Â  Â  Â  Â  Â  Â  check=True
Â  Â  Â  Â  )
Â  Â  Â  Â  return result.stdout
Â  Â  except subprocess.CalledProcessError as e:
Â  Â  Â  Â  return f"Error running eza command: {e.stderr}"
Â  Â  except Exception as e:
Â  Â  Â  Â  return f"An unexpected error occurred while running eza: {str(e)}"

def check_dependencies():
Â  Â  """Verifies that all required external command-line tools are installed."""
Â  Â  print("Checking for required external dependencies...")
Â  Â  dependencies = {
Â  Â  Â  Â  "pyreverse": "Provided by `pylint`. Install with: pip install pylint",
Â  Â  Â  Â  "plantuml": "A Java-based tool. See https://plantuml.com/starting",
Â  Â  Â  Â  "eza": "A modern replacement for `ls`. See https://eza.rocks/install",
Â  Â  Â  Â  "xclip": "Clipboard utility for Linux. Install with your package manager (e.g., sudo apt-get install xclip)",
Â  Â  }
Â  Â  missing = []
Â  Â Â 
Â  Â  for tool, instructions in dependencies.items():
Â  Â  Â  Â  if not shutil.which(tool):
Â  Â  Â  Â  Â  Â  missing.append((tool, instructions))
Â  Â Â 
Â  Â  if not missing:
Â  Â  Â  Â  print("âœ… All dependencies found.")
Â  Â  else:
Â  Â  Â  Â  print("\nâŒ Missing dependencies detected:")
Â  Â  Â  Â  for tool, instructions in missing:
Â  Â  Â  Â  Â  Â  print(f"Â  - Command not found: `{tool}`")
Â  Â  Â  Â  Â  Â  print(f"Â  Â  â†³ {instructions}")
Â  Â  Â  Â  print("\nPlease install the missing tools and ensure they are in your system's PATH.")
Â  Â  Â  Â  sys.exit(1)

# ============================================================================
# --- Intelligent PromptBuilder Class ---
# ============================================================================
class PromptBuilder:
Â  Â  """
Â  Â  Builds a complete, structured Markdown prompt including file manifests,
Â  Â  auto-generated context, file contents, and the user's final prompt.
Â  Â  """
Â  Â  def __init__(self, processed_files: List[Dict], prompt_text: str, context_only: bool = False):
Â  Â  Â  Â  self.processed_files = processed_files
Â  Â  Â  Â  self.prompt_text = prompt_text
Â  Â  Â  Â  self.context_only = context_only
Â  Â  Â  Â  self.auto_context = {}

Â  Â  def add_auto_context(self, title: str, content: str):
Â  Â  Â  Â  """Adds auto-generated context like UML diagrams to the prompt."""
Â  Â  Â  Â  if content and "error" not in content.lower() and "skipping" not in content.lower():
Â  Â  Â  Â  Â  Â  self.auto_context[title] = {
Â  Â  Â  Â  Â  Â  Â  Â  'content': content,
Â  Â  Â  Â  Â  Â  Â  Â  'tokens': count_tokens(content),
Â  Â  Â  Â  Â  Â  Â  Â  'words': count_words(content)
Â  Â  Â  Â  Â  Â  }

Â  Â  def _generate_manifest_header(self) -> str:
Â  Â  Â  Â  lines = ["# Codebase Context & Manifest", ""]
Â  Â  Â  Â  for f in self.processed_files:
Â  Â  Â  Â  Â  Â  purpose = f" ({f['comment']})" if f['comment'] else ""
Â  Â  Â  Â  Â  Â  token_display = f" ({f['tokens']:,} tokens)" if not self.context_only else ""
Â  Â  Â  Â  Â  Â  lines.append(f"- **{f['path']}**{purpose}{token_display}")
Â  Â  Â  Â  return "\n".join(lines)

Â  Â  def _generate_auto_context_section(self) -> str:
Â  Â  Â  Â  if not self.auto_context:
Â  Â  Â  Â  Â  Â  return ""
Â  Â  Â  Â  lines = ["", "---", "", "# Auto-Generated Context", ""]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- NEW: Prioritize printing Narrative Context first ---
Â  Â  Â  Â  narrative_title = "Recent Narrative Context"
Â  Â  Â  Â  if narrative_title in self.auto_context:
Â  Â  Â  Â  Â  Â  content = self.auto_context[narrative_title]['content']
Â  Â  Â  Â  Â  Â  lines.append(f"## {narrative_title}")
Â  Â  Â  Â  Â  Â  lines.append(content.strip())
Â  Â  Â  Â  Â  Â  lines.append("") # Add a blank line for spacing

Â  Â  Â  Â  if "Codebase Structure (eza --tree)" in self.auto_context:
Â  Â  Â  Â  Â  Â  title = "Codebase Structure (eza --tree)"
Â  Â  Â  Â  Â  Â  content = self.auto_context[title]['content']
Â  Â  Â  Â  Â  Â  lines.append(f"## {title}")
Â  Â  Â  Â  Â  Â  lines.append("```text")
Â  Â  Â  Â  Â  Â  lines.append(content.strip())
Â  Â  Â  Â  Â  Â  lines.append("```")
Â  Â  Â  Â Â 
Â  Â  Â  Â  for title, data in self.auto_context.items():
Â  Â  Â  Â  Â  Â  if title not in ["Codebase Structure (eza --tree)", narrative_title]:
Â  Â  Â  Â  Â  Â  Â  Â  lines.append(f"## {title}")
Â  Â  Â  Â  Â  Â  Â  Â  lines.append("```text")
Â  Â  Â  Â  Â  Â  Â  Â  lines.append(data['content'])
Â  Â  Â  Â  Â  Â  Â  Â  lines.append("```")
Â  Â  Â  Â  return "\n".join(lines)

Â  Â  def _generate_file_contents(self) -> str:
Â  Â  Â  Â  lines = []
Â  Â  Â  Â  for f in self.processed_files:
Â  Â  Â  Â  Â  Â  lines.append(f"```{f['lang']}:{f['path']}")
Â  Â  Â  Â  Â  Â  lines.append(f['content'])
Â  Â  Â  Â  Â  Â  lines.append("```")
Â  Â  Â  Â  Â  Â  lines.append(f"\n# End of {f['path']}\n")
Â  Â  Â  Â  return "\n".join(lines)

Â  Â  def _generate_ai_checklist(self) -> str:
Â  Â  Â  Â  """Creates a standardized checklist for the AI to verify the prompt's context."""
Â  Â  Â  Â  return '''
# AI Self-Correction Checklist

Before addressing the user's prompt, perform the following verification steps:

1.Â  **Review the Manifest vs. Codebase Structure:** Compare the file list in the manifest against the `eza --tree` output. Are there any obviously related, critical files missing from the manifest that would be necessary to understand the user's request? (e.g., if `core.py` is present, is `__init__.py` also present?).
2.Â  **Analyze Import Statements:** Briefly scan the `import` statements in the provided Python files. Do they suggest dependencies on local files that were *not* included in the manifest?
3.Â  **Check for Incompleteness:** If you determine that critical files are missing, do not proceed. Instead, your **primary task** is to inform me of the missing files and recommend adding them to `foo_files.py` to create a more complete context.
4.Â  **Confirm Understanding:** If the context appears complete, state "Context verified." and then proceed with the user's request.
'''

Â  Â  def build_final_prompt(self) -> str:
Â  Â  Â  Â  """Assembles all parts into the final Markdown string."""
Â  Â  Â  Â  parts = [
Â  Â  Â  Â  Â  Â  self._generate_manifest_header(),
Â  Â  Â  Â  Â  Â  self._generate_auto_context_section(),
Â  Â  Â  Â  ]

Â  Â  Â  Â  if not self.context_only:
Â  Â  Â  Â  Â  Â  parts.extend([
Â  Â  Â  Â  Â  Â  Â  Â  "\n---\n\n# File Contents\n",
Â  Â  Â  Â  Â  Â  Â  Â  self._generate_file_contents(),
Â  Â  Â  Â  Â  Â  ])

Â  Â  Â  Â  parts.extend([
Â  Â  Â  Â  Â  Â  "---\n\n# User Prompt\n",
Â  Â  Â  Â  Â  Â  self._generate_ai_checklist(),
Â  Â  Â  Â  Â  Â  self.prompt_text
Â  Â  Â  Â  ])
Â  Â  Â  Â Â 
Â  Â  Â  Â  return "\n".join(filter(None, parts))

Â  Â  def print_summary(self):
Â  Â  Â  Â  """Calculates and prints an accurate, comprehensive summary to the console."""
Â  Â  Â  Â  # --- Calculate token counts for all components ---
Â  Â  Â  Â  manifest_str = self._generate_manifest_header()
Â  Â  Â  Â  manifest_tokens = count_tokens(manifest_str)
Â  Â  Â  Â  manifest_words = count_words(manifest_str)
Â  Â  Â  Â Â 
Â  Â  Â  Â  prompt_tokens = count_tokens(self.prompt_text)
Â  Â  Â  Â  prompt_words = count_words(self.prompt_text)

Â  Â  Â  Â  checklist_str = self._generate_ai_checklist()
Â  Â  Â  Â  checklist_tokens = count_tokens(checklist_str)
Â  Â  Â  Â  checklist_words = count_words(checklist_str)

Â  Â  Â  Â  auto_context_total_tokens = sum(v['tokens'] for v in self.auto_context.values())
Â  Â  Â  Â  auto_context_total_words = sum(v['words'] for v in self.auto_context.values())

Â  Â  Â  Â  file_content_total_tokens = sum(f['tokens'] for f in self.processed_files)
Â  Â  Â  Â  file_content_total_words = sum(f['words'] for f in self.processed_files)

Â  Â  Â  Â  # --- Display the breakdown ---
Â  Â  Â  Â  print("--- Files Included ---")
Â  Â  Â  Â  for f in self.processed_files:
Â  Â  Â  Â  Â  Â  if self.context_only:
Â  Â  Â  Â  Â  Â  Â  Â  print(f"â€¢ {f['path']} (content omitted)")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  print(f"â€¢ {f['path']} ({f['tokens']:,} tokens)")
Â  Â  Â  Â Â 
Â  Â  Â  Â  if self.auto_context:
Â  Â  Â  Â  Â  Â  print("\n--- Auto-Context Included ---")
Â  Â  Â  Â  Â  Â  for title, data in self.auto_context.items():
Â  Â  Â  Â  Â  Â  Â  Â  print(f"â€¢ {title} ({data['tokens']:,} tokens)")

Â  Â  Â  Â  # --- Calculate and display the final summary ---
Â  Â  Â  Â  print("\n--- Prompt Summary ---")
Â  Â  Â  Â  if self.context_only:
Â  Â  Â  Â  Â  Â  print("NOTE: Running in --context-only mode. File contents are excluded.")
Â  Â  Â  Â  Â  Â  total_tokens = manifest_tokens + auto_context_total_tokens + prompt_tokens + checklist_tokens
Â  Â  Â  Â  Â  Â  total_words = manifest_words + auto_context_total_words + prompt_words + checklist_words
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  total_tokens = manifest_tokens + auto_context_total_tokens + file_content_total_tokens + prompt_tokens + checklist_tokens
Â  Â  Â  Â  Â  Â  total_words = manifest_words + auto_context_total_words + file_content_total_words + prompt_words + checklist_words

Â  Â  Â  Â  print(f"Total Tokens: {total_tokens:,}")
Â  Â  Â  Â  print(f"Total Words:Â  {total_words:,}")

Â  Â  Â  Â  ratio = total_tokens / total_words if total_words > 0 else 0
Â  Â  Â  Â  perspective = get_literary_perspective(total_words, ratio)
Â  Â  Â  Â  print("\n--- Size Perspective ---")
Â  Â  Â  Â  print(perspective)
Â  Â  Â  Â  print()

# ============================================================================
# --- Main Execution Logic ---
# ============================================================================
def main():
Â  Â  """Main function to parse args, process files, and generate output."""
Â  Â  parser = argparse.ArgumentParser(description='Generate a Markdown context file for AI code assistance.')
Â  Â  parser.add_argument('prompt', nargs='?', default=None, help='A prompt string or path to a prompt file (e.g., prompt.md).')
Â  Â  parser.add_argument('-o', '--output', type=str, help='Optional: Output filename.')
Â  Â  parser.add_argument('--no-clipboard', action='store_true', help='Disable copying output to clipboard.')
Â  Â  parser.add_argument('--check-dependencies', action='store_true', help='Verify that all required external tools are installed.')
Â  Â  parser.add_argument('--context-only', action='store_true', help='Generate a context-only prompt without file contents.')
Â  Â  # --- NEW: Argument for including narrative context ---
Â  Â  parser.add_argument(
Â  Â  Â  Â  '-l', '--list',
Â  Â  Â  Â  nargs='?',
Â  Â  Â  Â  const='[-5:]',
Â  Â  Â  Â  default=None,
Â  Â  Â  Â  help='Include a list of recent articles. Optionally provide a slice, e.g., "[-10:]". Defaults to "[-5:]".'
Â  Â  )
Â  Â  args = parser.parse_args()

Â  Â  if args.check_dependencies:
Â  Â  Â  Â  check_dependencies()
Â  Â  Â  Â  sys.exit(0)

Â  Â  # 1. Handle user prompt
Â  Â  prompt_content = "Please review the provided context and assist with the codebase."
Â  Â  if args.prompt:
Â  Â  Â  Â  if os.path.exists(args.prompt):
Â  Â  Â  Â  Â  Â  with open(args.prompt, 'r', encoding='utf-8') as f:
Â  Â  Â  Â  Â  Â  Â  Â  prompt_content = f.read()
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  prompt_content = args.prompt
Â  Â  elif os.path.exists("prompt.md"):
Â  Â  Â  Â  with open("prompt.md", 'r', encoding='utf-8') as f:
Â  Â  Â  Â  Â  Â  prompt_content = f.read()

Â  Â  # 2. Process all specified files
Â  Â  files_to_process = parse_file_list_from_config()
Â  Â  processed_files_data = []
Â  Â  print("--- Processing Files ---")
Â  Â  for path, comment in files_to_process:
Â  Â  Â  Â  full_path = os.path.join(REPO_ROOT, path) if not os.path.isabs(path) else path
Â  Â  Â  Â  if not os.path.exists(full_path):
Â  Â  Â  Â  Â  Â  print(f"Warning: File not found and will be skipped: {full_path}")
Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  content = ""
Â  Â  Â  Â  lang = "text"
Â  Â  Â  Â  ext = os.path.splitext(path)[1].lower()

Â  Â  Â  Â  if ext == '.ipynb':
Â  Â  Â  Â  Â  Â  if JUPYTEXT_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  print(f"Â  -> Converting notebook: {path}")
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  notebook = jupytext.read(full_path)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content = jupytext.writes(notebook, fmt='py:percent')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lang = 'python'
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content = f"# FAILED TO CONVERT NOTEBOOK: {path}\n# ERROR: {e}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"Warning: Failed to convert {path}: {e}")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  content = f"# SKIPPING NOTEBOOK CONVERSION: jupytext not installed for {path}"
Â  Â  Â  Â  Â  Â  Â  Â  print(f"Warning: `jupytext` library not found. Skipping conversion for {path}.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  with open(full_path, 'r', encoding='utf-8') as f:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content = f.read()
Â  Â  Â  Â  Â  Â  Â  Â  lang_map = {'.py': 'python', '.js': 'javascript', '.html': 'html', '.css': 'css', '.md': 'markdown', '.json': 'json', '.nix': 'nix', '.sh': 'bash'}
Â  Â  Â  Â  Â  Â  Â  Â  lang = lang_map.get(ext, 'text')
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  print(f"ERROR: Could not read or process {full_path}: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  sys.exit(1)

Â  Â  Â  Â  processed_files_data.append({
Â  Â  Â  Â  Â  Â  "path": path, "comment": comment, "content": content,
Â  Â  Â  Â  Â  Â  "tokens": count_tokens(content), "words": count_words(content),
Â  Â  Â  Â  Â  Â  "lang": lang
Â  Â  Â  Â  })

Â  Â  # 3. Build the prompt and add auto-generated context
Â  Â  builder = PromptBuilder(processed_files_data, prompt_content, context_only=args.context_only)
Â  Â Â 
Â  Â  # --- Add the Codebase Tree ---
Â  Â  print("\n--- Generating Auto-Context ---")
Â  Â  print("Generating codebase tree diagram...", end='', flush=True)
Â  Â  tree_output = run_tree_command()
Â  Â  title = "Codebase Structure (eza --tree)"
Â  Â  builder.add_auto_context(title, tree_output)
Â  Â  if title in builder.auto_context:
Â  Â  Â  Â  token_count = builder.auto_context[title]['tokens']
Â  Â  Â  Â  print(f" ({token_count:,} tokens)")
Â  Â  else:
Â  Â  Â  Â  print(" (skipped)")


Â  Â  # --- NEW: Add narrative context if requested ---
Â  Â  if args.list:
Â  Â  Â  Â  print("Adding narrative context from articles...", end='', flush=True)
Â  Â  Â  Â  all_articles = _get_article_list_data()
Â  Â  Â  Â  sliced_articles = []
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  slice_or_index = parse_slice_arg(args.list)
Â  Â  Â  Â  Â  Â  if isinstance(slice_or_index, int):
Â  Â  Â  Â  Â  Â  Â  Â  sliced_articles = [all_articles[slice_or_index]]
Â  Â  Â  Â  Â  Â  elif isinstance(slice_or_index, slice):
Â  Â  Â  Â  Â  Â  Â  Â  sliced_articles = all_articles[slice_or_index]
Â  Â  Â  Â  except (ValueError, IndexError):
Â  Â  Â  Â  Â  Â  print(f" (invalid slice '{args.list}')")
Â  Â  Â  Â  Â  Â  sliced_articles = []

Â  Â  Â  Â  if sliced_articles:
Â  Â  Â  Â  Â  Â  narrative_content = "\n".join(
Â  Â  Â  Â  Â  Â  Â  Â  f"### {article['title']} ({article['date']})\n> {article['summary']}\n"
Â  Â  Â  Â  Â  Â  Â  Â  for article in sliced_articles
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  builder.add_auto_context("Recent Narrative Context", narrative_content)
Â  Â  Â  Â  Â  Â  print(f" ({len(sliced_articles)} articles)")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  print(" (no articles found or invalid slice)")


Â  Â  # --- Generate UML for all included Python files ---
Â  Â  python_files_to_diagram = [
Â  Â  Â  Â  f['path'] for f in processed_files_data if f['path'].endswith('.py')
Â  Â  ]

Â  Â  if python_files_to_diagram:
Â  Â  Â  Â  print("Python file(s) detected. Generating UML diagrams...")
Â  Â  Â  Â  for py_file_path in python_files_to_diagram:
Â  Â  Â  Â  Â  Â  print(f"Â  -> Generating for {py_file_path}...", end='', flush=True)
Â  Â  Â  Â  Â  Â  uml_context = generate_uml_and_dot(
Â  Â  Â  Â  Â  Â  Â  Â  target_file=py_file_path,
Â  Â  Â  Â  Â  Â  Â  Â  project_name=CONFIG["PROJECT_NAME"]
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  uml_content = uml_context.get("ascii_uml")
Â  Â  Â  Â  Â  Â  title = f"UML Class Diagram (ASCII for {py_file_path})"
Â  Â  Â  Â  Â  Â  builder.add_auto_context(title, uml_content)

Â  Â  Â  Â  Â  Â  if title in builder.auto_context:
Â  Â  Â  Â  Â  Â  Â  Â  token_count = builder.auto_context[title]['tokens']
Â  Â  Â  Â  Â  Â  Â  Â  print(f" ({token_count:,} tokens)")
Â  Â  Â  Â  Â  Â  elif uml_content and "note: no classes" in uml_content.lower():
Â  Â  Â  Â  Â  Â  Â  Â  print(" (no classes found)")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  print(" (skipped)")

Â  Â  Â  Â  print("...UML generation complete.\n")
Â  Â Â 
Â  Â  # 4. Generate final output and print summary
Â  Â  final_output = builder.build_final_prompt()
Â  Â  builder.print_summary()

Â  Â  # 5. Handle output
Â  Â  if args.output:
Â  Â  Â  Â  with open(args.output, 'w', encoding='utf-8') as f:
Â  Â  Â  Â  Â  Â  f.write(final_output)
Â  Â  Â  Â  print(f"Output written to '{args.output}'")
Â  Â  if not args.no_clipboard:
Â  Â  Â  Â  copy_to_clipboard(final_output)


if __name__ == "__main__":
Â  Â  main()
