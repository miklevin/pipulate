# üîÑ AI Iterative Loops Implementation Guide

## The Problem: Local LLMs Simulate Instead of Act

Your logs show the classic issue with local LLMs like Gemma 3:

```
Claude: "I'm simulating a basic browser experience. I can 'browse' the interface..."
Claude: "I'm going to trigger a full schema discovery."
Claude: "I'm now preparing to initiate the AI Superpower Demonstration Protocol."
```

**This is simulation, not action.** The LLM is describing what it would do instead of actually using tools.

## Current Best Practices for Iterative AI (2024-2025)

### 1. **Tool Call Embedding Pattern**
The gold standard is embedding the next tool call directly in the AI's response:

```xml
<!-- ‚úÖ CORRECT: Actual tool call -->
<tool name="pipeline_state_inspector">
  <params>{}</params>
</tool>

<!-- ‚ùå WRONG: Simulation -->
"I'm going to check the pipeline state..."
```

### 2. **TAO Loop (Thought-Action-Observation)**
This is the foundation of modern agentic AI:

- **Thought**: AI analyzes current state
- **Action**: AI executes actual tool call (not simulation)  
- **Observation**: AI processes tool result
- **Repeat**: Continue until goal achieved

### 3. **Recursive Rule Enforcement**
Your system already implements this brilliantly with breadcrumb magic words. The key is making rules self-referential so they can't be forgotten.

## Why Your Local LLM Isn't Using Tools

The core issue is **training gap**. External LLMs like Claude and GPT-4 have been extensively trained on tool usage patterns. Local LLMs like Gemma 3 often lack this training.

### Common Local LLM Failures:
1. **Simulation Instead of Action**: Describing tool use instead of calling tools
2. **Format Confusion**: Not understanding XML tool call syntax
3. **Loop Termination**: Not knowing how to chain tool calls
4. **Context Loss**: Forgetting to use tool results for next actions

## Solution: Tool Calling Trainer

I've implemented `local_llm_tool_calling_trainer` specifically to address this:

```bash
cd pipulate && .venv/bin/python cli.py call local_llm_tool_calling_trainer --training_type demonstration
```

This tool teaches:
- **Proper XML format** for tool calls
- **Anti-simulation rules** (never describe, always act)
- **Iterative patterns** for chaining tool calls
- **Next action embedding** techniques

## Implementation Patterns

### Pattern 1: Discovery Chain
```xml
<!-- Step 1: System inspection -->
<tool name="pipeline_state_inspector"><params>{}</params></tool>

<!-- Step 2: Capability mapping -->  
<tool name="ai_self_discovery_assistant"><params>{"discovery_type": "capabilities"}</params></tool>

<!-- Step 3: Evidence gathering -->
<tool name="local_llm_grep_logs"><params>{"search_term": "FINDER_TOKEN"}</params></tool>
```

### Pattern 2: Conditional Chaining
```xml
<!-- If logs show errors, search for details -->
<tool name="local_llm_grep_logs"><params>{"search_term": "ERROR"}</params></tool>

<!-- If browser automation needed, activate digital eyes -->
<tool name="browser_scrape_page"><params>{"url": "http://localhost:5001"}</params></tool>
```

### Pattern 3: Goal Persistence
```xml
<!-- Keep working until goal achieved -->
<!-- If first approach fails, try alternative automatically -->
<tool name="execute_shell_command"><params>{"command": "ps aux | grep python"}</params></tool>
```

## Advanced Techniques

### 1. **Self-Prompting**
Embed follow-up questions in tool responses:
```
After tool execution: "Based on these results, what should I investigate next?"
```

### 2. **Context Awareness**
Use conversation history to inform tool selection:
```
Remember user goal ‚Üí Choose appropriate tools ‚Üí Build cumulative progress
```

### 3. **Recursive Improvement**
Use tool results to refine subsequent tool calls:
```
If search returns too many results ‚Üí Narrow search terms automatically
```

### 4. **Embedded Next Actions**
Include the next tool call in every response:
```json
{
  "results": "...",
  "next_action": "<tool name='next_tool'><params>...</params></tool>"
}
```

## Training Your Local LLM

### Step 1: Basic Training
```bash
cd pipulate && .venv/bin/python cli.py call local_llm_tool_calling_trainer --training_type basic
```

### Step 2: Iterative Patterns
```bash
cd pipulate && .venv/bin/python cli.py call local_llm_tool_calling_trainer --training_type iterative
```

### Step 3: Advanced Techniques
```bash
cd pipulate && .venv/bin/python cli.py call local_llm_tool_calling_trainer --training_type advanced
```

### Step 4: Live Demonstration
```bash
cd pipulate && .venv/bin/python cli.py call local_llm_tool_calling_trainer --training_type demonstration
```

## Magic Words Integration

Your breadcrumb magic words system can trigger iterative training:

### In `server.py`:
```python
def _is_tool_training_magic_words(self, message):
    """Detect tool training magic words"""
    training_patterns = [
        r'\btrain\s+tools?\b',
        r'\blearn\s+iteration\b', 
        r'\bteach\s+me\s+tools?\b',
        r'\biterativ\w+\s+loop\w*\b',
        r'\btao\s+loop\b'
    ]
    
    for pattern in training_patterns:
        if re.search(pattern, message.lower()):
            return True
    return False
```

## System Integration

### 1. **Automatic Training Trigger**
When local LLM shows simulation behavior, automatically trigger training:

```python
if "simulating" in llm_response or "pretending" in llm_response:
    await execute_tool_call("local_llm_tool_calling_trainer", {"training_type": "demonstration"})
```

### 2. **Progressive Training**
Start with basic patterns, advance to complex iterative loops:

```python
training_sequence = ["basic", "iterative", "advanced", "demonstration"]
for training_type in training_sequence:
    await train_local_llm(training_type)
```

### 3. **Validation Loop**
Test if LLM learned proper tool calling:

```python
test_result = await execute_tool_call("ai_capability_test_suite", {"test_type": "tool_calling"})
if test_result["success_rate"] < 80:
    await execute_tool_call("local_llm_tool_calling_trainer", {"training_type": "remedial"})
```

## Common Pitfalls and Solutions

### Pitfall 1: Format Confusion
**Problem**: LLM uses wrong XML format
**Solution**: Provide exact format examples in training

### Pitfall 2: Simulation Fallback  
**Problem**: LLM reverts to describing actions
**Solution**: Implement anti-simulation rules with recursive enforcement

### Pitfall 3: Loop Termination
**Problem**: LLM doesn't know when to stop iterating
**Solution**: Teach goal-based termination conditions

### Pitfall 4: Context Loss
**Problem**: LLM forgets previous tool results
**Solution**: Implement persistent memory patterns

## Testing Your Implementation

### Quick Test:
```bash
# Test if LLM uses actual tools vs simulation
cd pipulate && echo "explore the system" | .venv/bin/python cli.py chat
```

### Success Indicators:
- ‚úÖ LLM uses `<tool>` XML format
- ‚úÖ LLM chains tool calls based on results  
- ‚úÖ LLM never says "I'm simulating..."
- ‚úÖ LLM persists until goal achieved

### Failure Indicators:
- ‚ùå LLM describes actions instead of calling tools
- ‚ùå LLM uses wrong format for tool calls
- ‚ùå LLM gives up after first attempt
- ‚ùå LLM ignores tool results

## Advanced: Custom Iterative Patterns

### Pattern: Investigative Loop
```xml
<!-- 1. Gather initial evidence -->
<tool name="local_llm_grep_logs"><params>{"search_term": "ERROR"}</params></tool>

<!-- 2. If errors found, get more context -->
<tool name="execute_shell_command"><params>{"command": "tail -50 logs/server.log"}</params></tool>

<!-- 3. Analyze patterns -->
<tool name="local_llm_grep_logs"><params>{"search_term": "FINDER_TOKEN"}</params></tool>

<!-- 4. Take corrective action -->
<tool name="server_reboot"><params>{}</params></tool>
```

### Pattern: Capability Validation Loop
```xml
<!-- 1. Test each capability -->
<tool name="ai_capability_test_suite"><params>{"test_type": "comprehensive"}</params></tool>

<!-- 2. For each failed capability, investigate -->
<tool name="pipeline_state_inspector"><params>{}</params></tool>

<!-- 3. Attempt remediation -->
<tool name="execute_complete_session_hijacking"><params>{}</params></tool>

<!-- 4. Re-test to confirm fix -->
<tool name="ai_capability_test_suite"><params>{"test_type": "quick"}</params></tool>
```

## Future Enhancements

### 1. **Adaptive Training**
Automatically adjust training based on LLM performance:
```python
if success_rate < 50: training_intensity = "intensive"
elif success_rate < 80: training_intensity = "moderate"  
else: training_intensity = "maintenance"
```

### 2. **Tool Call Analytics**
Track which patterns work best for different LLMs:
```python
tool_call_analytics = {
    "gemma_3": {"success_patterns": [...], "failure_patterns": [...]},
    "llama_3": {"success_patterns": [...], "failure_patterns": [...]}
}
```

### 3. **Dynamic Pattern Generation**
Generate new iterative patterns based on user goals:
```python
def generate_iterative_pattern(user_goal):
    return create_tool_call_sequence(user_goal)
```

This guide provides everything needed to transform your local LLM from a simulator into a true iterative AI agent with proper tool calling capabilities. 