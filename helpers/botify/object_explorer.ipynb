{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Load configuration and API key\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {open('botify_token.txt').read().strip().split('\\n')[0].strip()}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def preview_data(org, project, analysis, depth=1):\n",
    "    \"\"\"Preview data availability before committing to full download\"\"\"\n",
    "    # Get analysis date from the slug (assuming YYYYMMDD format)\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    # Calculate period start (7 days before analysis date)\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"collections\": [\n",
    "            f\"crawl.{analysis}\",\n",
    "            \"search_console\"\n",
    "        ],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\"\n",
    "            ],\n",
    "            \"metrics\": [\n",
    "                \"search_console.period_0.count_impressions\",\n",
    "                \"search_console.period_0.count_clicks\"\n",
    "            ],\n",
    "            \"filters\": {\n",
    "                \"field\": f\"crawl.{analysis}.depth\",\n",
    "                \"predicate\": \"lte\",\n",
    "                \"value\": depth\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": \"search_console.period_0.count_impressions\",\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"periods\": [\n",
    "            [\n",
    "                period_start,\n",
    "                period_end\n",
    "            ]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüîç Sampling data for {org}/{project}/{analysis}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    if response.status_code != 200:\n",
    "        print(\"‚ùå Preview failed:\", response.status_code)\n",
    "        return False\n",
    "        \n",
    "    data = response.json()\n",
    "    if not data.get('results'):\n",
    "        print(\"‚ö†Ô∏è  No preview data available\")\n",
    "        return False\n",
    "        \n",
    "    print(\"\\nüìä Data Sample Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    metrics_found = 0\n",
    "    for result in data['results'][:3]:  # Show just top 3 for cleaner output\n",
    "        url = result['dimensions'][0]\n",
    "        impressions = result['metrics'][0]\n",
    "        clicks = result['metrics'][1]\n",
    "        metrics_found += bool(impressions or clicks)\n",
    "        print(f\"‚Ä¢ URL: {url[:60]}...\")\n",
    "        print(f\"  ‚îî‚îÄ Performance: {impressions:,} impressions, {clicks:,} clicks\")\n",
    "    \n",
    "    print(\"\\nüéØ Data Quality Check\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"‚úì URLs found: {len(data['results'])}\")\n",
    "    print(f\"‚úì Search metrics: {'Available' if metrics_found else 'Not found'}\")\n",
    "    print(f\"‚úì Depth limit: {depth}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis):\n",
    "    \"\"\"Fetch data based on BQLv2 query for a specific Botify analysis.\"\"\"\n",
    "    collection = f\"crawl.{analysis}\"\n",
    "    \n",
    "    # Calculate dates\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # Base dimensions that should always be available in crawl\n",
    "    base_dimensions = [\n",
    "        f\"{collection}.url\",\n",
    "        f\"{collection}.depth\",\n",
    "    ]\n",
    "    \n",
    "    # Test which optional dimensions are actually available\n",
    "    optional_dimensions_to_test = [\n",
    "        f\"{collection}.segments.pagetype.value\",\n",
    "        f\"{collection}.compliant.is_compliant\", \n",
    "        f\"{collection}.compliant.main_reason\",\n",
    "        f\"{collection}.canonical.to.equal\",\n",
    "        f\"{collection}.sitemaps.present\",\n",
    "        f\"{collection}.js.rendering.exec\",\n",
    "        f\"{collection}.js.rendering.ok\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üîç Testing field availability...\")\n",
    "    available_dimensions = base_dimensions.copy()\n",
    "    \n",
    "    # Test each optional dimension individually\n",
    "    for field in optional_dimensions_to_test:\n",
    "        test_query = {\n",
    "            \"collections\": [collection],\n",
    "            \"query\": {\n",
    "                \"dimensions\": [field],\n",
    "                \"filters\": {\n",
    "                    \"field\": f\"{collection}.depth\",\n",
    "                    \"predicate\": \"eq\", \n",
    "                    \"value\": 0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            test_response = httpx.post(url, headers=headers, json=test_query, timeout=30)\n",
    "            if test_response.status_code == 200:\n",
    "                available_dimensions.append(field)\n",
    "                print(f\"‚úì {field.split('.')[-1]} - Available\")\n",
    "            else:\n",
    "                print(f\"‚úó {field.split('.')[-1]} - Not available\")\n",
    "        except Exception as e:\n",
    "            print(f\"? {field.split('.')[-1]} - Error testing: {e}\")\n",
    "    \n",
    "    # Optional metrics from other collections\n",
    "    optional_metrics = []\n",
    "    collections = [collection]\n",
    "    \n",
    "    # Test if search_console is available\n",
    "    try:\n",
    "        test_sc_query = {\n",
    "            \"collections\": [collection, \"search_console\"],\n",
    "            \"query\": {\n",
    "                \"dimensions\": [f\"{collection}.url\"],\n",
    "                \"metrics\": [\"search_console.period_0.count_impressions\"],\n",
    "                \"filters\": {\n",
    "                    \"field\": f\"{collection}.depth\",\n",
    "                    \"predicate\": \"eq\",\n",
    "                    \"value\": 0\n",
    "                }\n",
    "            },\n",
    "            \"periods\": [[period_start, period_end]]\n",
    "        }\n",
    "        \n",
    "        test_response = httpx.post(url, headers=headers, json=test_sc_query, timeout=30)\n",
    "        if test_response.status_code == 200:\n",
    "            collections.append(\"search_console\")\n",
    "            optional_metrics = [\n",
    "                \"search_console.period_0.count_impressions\",\n",
    "                \"search_console.period_0.count_clicks\"\n",
    "            ]\n",
    "            print(\"‚úì Search Console data - Available\")\n",
    "        else:\n",
    "            print(\"‚úó Search Console data - Not available\")\n",
    "    except Exception as e:\n",
    "        print(f\"? Search Console data - Error testing: {e}\")\n",
    "    \n",
    "    # Build the final query with only available fields\n",
    "    data_payload = {\n",
    "        \"collections\": collections,\n",
    "        \"query\": {\n",
    "            \"dimensions\": available_dimensions,\n",
    "            \"metrics\": optional_metrics,\n",
    "            \"filters\": {\n",
    "                \"field\": f\"{collection}.depth\",\n",
    "                \"predicate\": \"lte\",\n",
    "                \"value\": 2\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": optional_metrics[0] if optional_metrics else f\"{collection}.depth\",\n",
    "                    \"order\": \"desc\" if optional_metrics else \"asc\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add periods only if we have search console data\n",
    "    if \"search_console\" in collections:\n",
    "        data_payload[\"periods\"] = [[period_start, period_end]]\n",
    "\n",
    "    print(f\"Query payload: {json.dumps(data_payload, indent=2)}\")\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error response: {response.text}\")\n",
    "        response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Create column names based on what we actually requested\n",
    "    columns = []\n",
    "    for dim in available_dimensions:\n",
    "        field_name = dim.split('.')[-1]  # Get the last part of the field name\n",
    "        columns.append(field_name)\n",
    "    \n",
    "    for metric in optional_metrics:\n",
    "        field_name = metric.split('.')[-1]  # Get the last part of the metric name\n",
    "        columns.append(field_name)\n",
    "    \n",
    "    # Create DataFrame with available data\n",
    "    results = []\n",
    "    for item in data['results']:\n",
    "        row = item['dimensions']\n",
    "        if 'metrics' in item:\n",
    "            row.extend(item['metrics'])\n",
    "        results.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fetch_fields(org: str, project: str, collection: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch available fields for a given collection from the Botify API.\n",
    "    \n",
    "    Args:\n",
    "        org: Organization slug\n",
    "        project: Project slug  \n",
    "        collection: Collection name (e.g. 'crawl.20241108')\n",
    "        \n",
    "    Returns:\n",
    "        List of field IDs available in the collection\n",
    "    \"\"\"\n",
    "    fields_url = f\"https://api.botify.com/v1/projects/{org}/{project}/collections/{collection}\"\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(fields_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        fields_data = response.json()\n",
    "        return [\n",
    "            field['id'] \n",
    "            for dataset in fields_data.get('datasets', [])\n",
    "            for field in dataset.get('fields', [])\n",
    "        ]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching fields for collection '{collection}': {e}\")\n",
    "        return []\n",
    "\n",
    "def check_compliance_fields(org, project, analysis):\n",
    "    \"\"\"Check available compliance fields in a more structured way.\"\"\"\n",
    "    collection = f\"crawl.{analysis}\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # Group compliance fields by category\n",
    "    compliance_categories = {\n",
    "        'Basic Compliance': [\n",
    "            'compliant.is_compliant',\n",
    "            'compliant.main_reason',\n",
    "            'compliant.reason.http_code',\n",
    "            'compliant.reason.content_type',\n",
    "            'compliant.reason.canonical',\n",
    "            'compliant.reason.noindex',\n",
    "            'compliant.detailed_reason'\n",
    "        ],\n",
    "        'Performance': [\n",
    "            'scoring.issues.slow_first_to_last_byte_compliant',\n",
    "            'scoring.issues.slow_render_time_compliant',\n",
    "            'scoring.issues.slow_server_time_compliant',\n",
    "            'scoring.issues.slow_load_time_compliant'\n",
    "        ],\n",
    "        'SEO': [\n",
    "            'scoring.issues.duplicate_query_kvs_compliant'\n",
    "        ],\n",
    "        'Outlinks': [\n",
    "            'outlinks_errors.non_compliant.nb.follow.unique',\n",
    "            'outlinks_errors.non_compliant.nb.follow.total',\n",
    "            'outlinks_errors.non_compliant.urls'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüîç Field Availability Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    available_count = 0\n",
    "    total_count = sum(len(fields) for fields in compliance_categories.values())\n",
    "    \n",
    "    available_fields = []\n",
    "    for category, fields in compliance_categories.items():\n",
    "        available_in_category = 0\n",
    "        print(f\"\\nüìë {category}\")\n",
    "        print(\"-\" * 30)\n",
    "        for field in fields:\n",
    "            full_field = f\"{collection}.{field}\"\n",
    "            # Test field availability with a minimal query\n",
    "            test_query = {\n",
    "                \"collections\": [collection],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [full_field],\n",
    "                    \"filters\": {\"field\": f\"{collection}.depth\", \"predicate\": \"eq\", \"value\": 0}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = httpx.post(url, headers=headers, json=test_query, timeout=60)\n",
    "                if response.status_code == 200:\n",
    "                    available_in_category += 1\n",
    "                    available_count += 1\n",
    "                    print(f\"‚úì {field.split('.')[-1]}\")\n",
    "                    available_fields.append(field)\n",
    "                else:\n",
    "                    print(f\"√ó {field.split('.')[-1]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"? {field.split('.')[-1]} (error checking)\")\n",
    "    \n",
    "    coverage = (available_count / total_count) * 100\n",
    "    print(f\"\\nüìä Field Coverage: {coverage:.1f}%\")\n",
    "    return available_fields\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution logic\"\"\"\n",
    "    try:\n",
    "        with open('config.json') as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: config.json file not found\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: config.json is not valid JSON\")\n",
    "        return\n",
    "    \n",
    "    org = config.get('org')\n",
    "    project = config.get('project')\n",
    "    analysis = config.get('analysis')\n",
    "    \n",
    "    if not all([org, project, analysis]):\n",
    "        print(\"Error: Missing required fields in config.json (org, project, analysis)\")\n",
    "        return\n",
    "    \n",
    "    print(\"Previewing data availability...\")\n",
    "    if preview_data(org, project, analysis, depth=2):\n",
    "        print(\"Data preview successful. Proceeding with full export...\")\n",
    "        print(\"Fetching BQLv2 data...\")\n",
    "        df = get_bqlv2_data(org, project, analysis)\n",
    "        print(\"\\nData Preview:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to CSV\n",
    "        Path(\"downloads\").mkdir(parents=True, exist_ok=True)\n",
    "        output_file = f\"downloads/{org}_{project}_{analysis}_metadata.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nData saved to {output_file}\")\n",
    "        \n",
    "        # Use check_compliance_fields\n",
    "        check_compliance_fields(org, project, analysis)\n",
    "    else:\n",
    "        print(\"Data preview failed. Please check configuration and try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
