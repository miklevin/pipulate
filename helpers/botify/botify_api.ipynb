{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# What is Botify?\n",
    "\n",
    "Botify is an enterprise SEO platform that crawls websites and provides detailed analytics about their technical SEO health. The Botify API allows you to:\n",
    "\n",
    "- Access crawl data and site metrics\n",
    "- Generate custom reports and analyses \n",
    "- Build automated SEO workflows\n",
    "- Track site performance over time\n",
    "\n",
    "## Authentication: Getting Started with the API\n",
    "\n",
    "Before running any queries, you'll need to:\n",
    "\n",
    "1. Get your API token from [Botify Account Settings](https://app.botify.com/account/)\n",
    "2. Store it securely (we'll show you how)\n",
    "3. Test the connection with a basic API call\n",
    "\n",
    "## Secure Token Storage\n",
    "\n",
    "**⚠️ SECURITY WARNING**: API tokens provide full access to your Botify account. Never:\n",
    "- Commit tokens to version control\n",
    "- Share notebooks with tokens\n",
    "- Run token-using code on public platforms\n",
    "\n",
    "For local development:\n",
    "\n",
    "1. Create a `.gitignore` file if you don't have one\n",
    "2. Add these lines:\n",
    "```\n",
    "botify_token.txt\n",
    "config.json\n",
    ".ipynb_checkpoints/\n",
    "__pycache__/\n",
    "```\n",
    "\n",
    "3. If using Cursor IDE, also create a `.cursorignore` file with:\n",
    "```\n",
    "botify_token.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Introduction to BQL (Botify Query Language)\n",
    "\n",
    "BQL is a specialized query language for accessing and analyzing website crawl data through the Botify API. This guide will help you understand how to effectively use BQL for website analysis and SEO insights.\n",
    "\n",
    "## Core Capabilities\n",
    "- **Data Extraction**: Query and analyze large-scale crawl datasets\n",
    "- **Query Filtering**: Target specific page segments or URL attributes\n",
    "- **Metric Creation**: Define and calculate custom analytics metrics\n",
    "- **Site Segmentation**: Organize and categorize URLs by structural patterns\n",
    "\n",
    "## Technical Requirements\n",
    "Required dependencies:\n",
    "- `httpx` library for API communication\n",
    "- `pandas` for data manipulation\n",
    "- Authentication files:\n",
    "  - `botify_token.txt` for API authentication\n",
    "  - `config.json` for project configuration\n",
    "\n",
    "## Key Operations\n",
    "- Execute API queries against crawl data\n",
    "- Generate customized analytical reports\n",
    "- Create data pipelines for site analysis\n",
    "- Convert crawl data into actionable insights\n",
    "\n",
    "## Query Structure\n",
    "Every BQL query consists of four main components:\n",
    "1. **Collections**: Source datasets for analysis\n",
    "2. **Dimensions**: Data segmentation parameters\n",
    "3. **Metrics**: Measurement criteria\n",
    "4. **Sorting**: Result organization rules\n",
    "\n",
    "### Example BQL Query\n",
    "\n",
    "This example retrieves URL counts by page type:\n",
    "\n",
    "```python\n",
    "import httpx\n",
    "\n",
    "# Define API variables\n",
    "org = \"your_organization_slug\"\n",
    "project = \"your_project_slug\"\n",
    "collection = \"your_collection_name\"\n",
    "api_key = \"your_api_key_here\"\n",
    "url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# BQL query payload\n",
    "payload = {\n",
    "    \"collections\": [collection],\n",
    "    \"query\": {\n",
    "        \"dimensions\": [{\"field\": \"segments.pagetype.value\"}],\n",
    "        \"metrics\": [{\"field\": f\"{collection}.count_urls_crawl\"}],\n",
    "        \"sort\": [\n",
    "            {\"type\": \"metrics\", \"index\": 0, \"order\": \"desc\"},\n",
    "            {\"type\": \"dimensions\", \"index\": 0, \"order\": \"asc\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send POST request\n",
    "response = httpx.post(url, headers=headers, json=payload)\n",
    "response.raise_for_status()\n",
    "print(response.json())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from getpass import getpass\n",
    "\n",
    "def validate_token(token):\n",
    "    \"\"\"Check if the Botify API token is valid and return the username if successful.\"\"\"\n",
    "    url = \"https://api.botify.com/v1/authentication/profile\"\n",
    "    headers = {\"Authorization\": f\"Token {token}\"}\n",
    "    try:\n",
    "        response = httpx.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        # Extract username if the token is valid\n",
    "        user_data = response.json()\n",
    "        username = user_data[\"data\"][\"username\"]\n",
    "        return username\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        if e.response.status_code == 401:\n",
    "            print(\"Authentication failed: Invalid or expired token\")\n",
    "        else:\n",
    "            print(f\"API request failed: {e}\")\n",
    "        return None\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define token file\n",
    "token_file = \"botify_token.txt\"\n",
    "\n",
    "# Attempt to read the token from the file\n",
    "try:\n",
    "    with open(token_file) as f:\n",
    "        token = f.read().strip()\n",
    "    username = validate_token(token)\n",
    "    if username:\n",
    "        print(f\"Using saved token. Welcome, {username}!\")\n",
    "    else:\n",
    "        print(\"Invalid saved token. Please provide a new token.\")\n",
    "        token = None\n",
    "except FileNotFoundError:\n",
    "    token = None\n",
    "    print(f\"No token file found at {token_file}\")\n",
    "\n",
    "# If no valid token, prompt once\n",
    "if not token:\n",
    "    print(\"\\nTo get your API token, visit: https://app.botify.com/account\")\n",
    "    token = getpass(\"Enter your API token: \").strip()\n",
    "    \n",
    "    if token:\n",
    "        username = validate_token(token)\n",
    "        if username:\n",
    "            with open(token_file, 'w') as f:\n",
    "                f.write(token)\n",
    "            print(f\"API Token validated and saved. Welcome, {username}!\")\n",
    "        else:\n",
    "            print(\"Token validation failed. Please try again later.\")\n",
    "    else:\n",
    "        print(\"No token provided.\")\n",
    "\n",
    "print(\"Done. And remember, never let your browser save this token!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Enter your API token or hit 'Esc 00' & Restart to quit (it's a Jupyter thing):  ········\n",
    "    Invalid token, please try again.\n",
    "    Enter your API token or hit 'Esc 00' & Restart to quit (it's a Jupyter thing):  ········\n",
    "    API Token validated and saved. Welcome, michael.levin!\n",
    "    Done. And remember, never let your browser save this token!\n",
    "\n",
    "**Rationale**: Botify API-calls need access to your Botify API Token. We therefore retreive it smack it right down in the same folder where your script runs. This way, we can open it with a Python 1-liner, dump it into a global-scope `api_key` variable your functons can use anywhere. There's so much wrong with this except for the fact that it works every time. Don't deploy to production. Now go away or I shall taunt you a second time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Use API: How To Have Your First Initial Success With Botify API By Getting Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "\n",
    "headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "user_data = httpx.get(\"https://api.botify.com/v1/authentication/profile\", headers=headers).json()\n",
    "\n",
    "username = user_data[\"data\"][\"username\"]\n",
    "print(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "**Sample Output**: \n",
    "\n",
    "    first.last\n",
    "\n",
    "**Rationale**: To create a first successful experience connecting to the Botify API. If you run this and see your name, congratulations! You're a Botify employee. Also, you're successfully connecting to the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# List Orgs: How To Get the List of Projects And Their Orgs Given Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "# Load API key\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "\n",
    "def get_username():\n",
    "    \"\"\"Fetch the username associated with the API key.\"\"\"\n",
    "    try:\n",
    "        response = httpx.get(\"https://api.botify.com/v1/authentication/profile\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"data\"][\"username\"]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching username: {e}\")\n",
    "\n",
    "def fetch_projects(username):\n",
    "    \"\"\"Fetch all projects for a given username from Botify API.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{username}\"\n",
    "    projects = []\n",
    "    try:\n",
    "        while url:\n",
    "            response = httpx.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            projects.extend(\n",
    "                (p['name'], p['slug'], p['user']['login']) for p in data.get('results', [])\n",
    "            )\n",
    "            url = data.get('next')\n",
    "        return sorted(projects)\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching projects for {username}: {e}\")\n",
    "        return []\n",
    "\n",
    "username = get_username()\n",
    "if username:\n",
    "    projects = fetch_projects(username)\n",
    "    print(f\"{'Project Name':<30} {'Project Slug':<35} {'User or Org':<15}\")\n",
    "    print(\"=\" * 80)\n",
    "    for name, slug, user in projects:\n",
    "        print(f\"{name:<30} {slug:<35} {user:<15}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve username or projects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "Username: first.last\n",
    "Project Name                   Project Slug                        User or Org    \n",
    "================================================================================\n",
    "Foo Test                       foo.com                             first.last       \n",
    "Bar Test                       bar.com                             bar-org       \n",
    "Baz Test                       baz.com                             baz-org       \n",
    "```\n",
    "\n",
    "**Rationale**: You need an Organization slug (**org**) for these exercises. It goes in your **config.json** to get started. Your personal login username will usually be used for one Project, but then an offical ***org slug*** (aka group) will usually appear on the others. By convention, these values often end with `-org`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# List Projects: How To Get the List of Projects Given an Organization\n",
    "\n",
    "Note: \n",
    "- If you're running this in VSCode or Cursor IDE, the `config.json` file should be in the root directory of your project.\n",
    "- If you're running this in Jupyter Notebook, the `config.json` file should be in the same directory as this notebook.\n",
    "\n",
    "Your config.json should look like:\n",
    "```json\n",
    "{\n",
    "    \"org\": \"org_name\",\n",
    "    \"project\": \"project_name\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "\n",
    "def fetch_projects(org):\n",
    "    \"\"\"Fetch all projects for a given organization from Botify API.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    projects = []\n",
    "    \n",
    "    try:\n",
    "        while url:\n",
    "            response = httpx.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            projects.extend((p['name'], p['slug'], p['user']['login']) for p in data.get('results', []))\n",
    "            url = data.get('next')\n",
    "        return sorted(projects)\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching projects: {e}\")\n",
    "        return []\n",
    "\n",
    "projects = fetch_projects(org)\n",
    "\n",
    "print(f\"{'Project Name':<30} {'Project Slug':<35} {'Login':<15}\")\n",
    "print(\"=\" * 80)\n",
    "for name, slug, user in projects:\n",
    "    print(f\"{name:<30} {slug:<35} {user:<15}\")\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "Organization: foo-bar\n",
    "Project Name                   Project Slug                        Login     \n",
    "================================================================================\n",
    "Legendary Product Vault        legendary-product-vault             foo-org       \n",
    "Hidden Content Cove            hidden-content-cove                 foo-org       \n",
    "Fabled Catalog of Curiosities  fabled-catalog-of-curiosities       foo-org       \n",
    "```\n",
    "\n",
    "**Rationale**: Next, you need Project slugs for these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# List Analyses: How To Get the List of Analysis Slugs Given a Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org, project = config['org'], config['project']\n",
    "\n",
    "def fetch_analyses(org, project):\n",
    "    \"\"\"Fetch analysis slugs for a given project from Botify API.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/analyses/{org}/{project}/light\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    slugs = []\n",
    "    \n",
    "    try:\n",
    "        while url:\n",
    "            response = httpx.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            slugs.extend(a['slug'] for a in data.get('results', []))\n",
    "            url = data.get('next')\n",
    "        return slugs\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching analyses: {e}\")\n",
    "        return []\n",
    "\n",
    "# Output analysis slugs\n",
    "for slug in fetch_analyses(org, project):\n",
    "    print(slug)\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "20240301\n",
    "20240201\n",
    "20240101-2\n",
    "20240101\n",
    "20231201\n",
    "20231101\n",
    "```\n",
    "\n",
    "**Rationale**: Analysis slugs are dates in YYYYMMDD format but sometimes get incremeted with `-n` extensions starting with `-2`. They're the third thing you typically need in **config.json** for these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# List URLs: How To Get a List of the First 500 URLs\n",
    "\n",
    "**Important**: For this step to work, you need to have an `analysis` value set in your `config.json` file. \n",
    "\n",
    "Your `config.json` should include:\n",
    "```json\n",
    "{\n",
    "    \"org\": \"your-organization\",\n",
    "    \"project\": \"your-project-slug\",\n",
    "    \"analysis\": \"your-analysis-slug\"\n",
    "}\n",
    "```\n",
    "\n",
    "The analysis slug is typically a date in YYYYMMDD format (like \"20240301\") as shown in the sample output below. Without this value in your config file, you'll encounter a KeyError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis, api_key):\n",
    "    \"\"\"Fetch data based on BQLv2 query for a specific Botify analysis.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # BQLv2 query payload\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\"\n",
    "            ],\n",
    "            \"metrics\": []  # Don't come crying to me when you delete this and it stops working.\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Send the request\n",
    "    response = httpx.post(url, headers=headers, json=data_payload, timeout=60.0)\n",
    "    response.raise_for_status()  # Check for errors\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Run the query and load results\n",
    "data = get_bqlv2_data(org, project, analysis, api_key)\n",
    "\n",
    "list_of_urls = [url['dimensions'][0] for url in data['results']]\n",
    "\n",
    "for i, url in enumerate(list_of_urls):\n",
    "    print(i + 1, url)\n",
    "    if i >= 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "1 https://example.com/page1\n",
    "2 https://example.com/page2\n",
    "3 https://example.com/page3\n",
    "4 https://example.com/page4\n",
    "5 https://example.com/page5\n",
    "6 https://example.com/page6\n",
    "7 https://example.com/page7\n",
    "8 https://example.com/page8\n",
    "9 https://example.com/page9\n",
    "10 https://example.com/page10\n",
    "```\n",
    "\n",
    "**Rationale**: To explicitly tell you that you have to leave the `metrics\": []` field in this example even though it's empty. Don't believe me? Try it. Ugh! Also, I'm not here to teach you Python, but it's worth noting:\n",
    "\n",
    "- `enumerate()` exposes the internal counter index.\n",
    "- Python uses zero-based indexes, thus the `+1` for humans and `>= 9` to cut off at 10.\n",
    "- The `print()` function takes multiple (un-labeled) inputs—counter & url in this case.\n",
    "- The other way to use the counter & url together is ***f-strings***: `f\"{i+1} {url}\"`, which would also work.\n",
    "\n",
    "You're welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# List SEO Fields: How To Get a List of the First 500 URLs, Titles, Meta Descriptions and H1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis, api_key):\n",
    "    \"\"\"Fetch data for URLs with title, meta description, and H1 fields.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # BQLv2 query payload\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\",\n",
    "                f\"crawl.{analysis}.metadata.title.content\",\n",
    "                f\"crawl.{analysis}.metadata.description.content\",\n",
    "                f\"crawl.{analysis}.metadata.h1.contents\"\n",
    "            ],\n",
    "            \"metrics\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Send the request with a timeout of 60 seconds\n",
    "    response = httpx.post(url, headers=headers, json=data_payload, timeout=60)\n",
    "    response.raise_for_status()  # Check for errors\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Run the query and load results\n",
    "data = get_bqlv2_data(org, project, analysis, api_key)\n",
    "\n",
    "# Flatten the data into a DataFrame\n",
    "columns = [\"url\", \"title\", \"meta_description\", \"h1\"]\n",
    "df = pd.DataFrame([item['dimensions'] for item in data['results']], columns=columns)\n",
    "\n",
    "# Display the first 500 URLs\n",
    "df.head(500).to_csv(\"first_500_urls.csv\", index=False)\n",
    "print(\"Data saved to first_500_urls.csv\")\n",
    "\n",
    "# Show a preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "\n",
    "| url                              | title               | meta_description                           | h1                 |\n",
    "|----------------------------------|---------------------|--------------------------------------------|---------------------|\n",
    "| https://example.com/foo          | Foo Title          | This is a description of Foo.              | Foo Heading        |\n",
    "| https://example.com/bar          | Bar Overview       | Bar is a collection of great resources.    | Bar Insights       |\n",
    "| https://example.com/baz          | Baz Guide          | Learn all about Baz and its applications.  | Baz Essentials     |\n",
    "\n",
    "...\n",
    "\n",
    "*Data saved to `first_500_urls.csv`*\n",
    "\n",
    "**Rationale**: To show you the main endpoint for listing 500 lines at a time, paging and quick aggregate queries. To show you how `org` and `project` are in the url (so you notice them disappearing later when we export csv downloads). To introduce the infinitely popular and useful `pandas` data library for manipulating ***row & column*** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "def format_analysis_date(analysis_slug):\n",
    "    \"\"\"Convert analysis slug (e.g. '20241108' or '20241108-2') to YYYY-MM-DD format.\"\"\"\n",
    "    # Strip any suffix after hyphen\n",
    "    base_date = analysis_slug.split('-')[0]\n",
    "    \n",
    "    # Insert hyphens for YYYY-MM-DD format\n",
    "    return f\"{base_date[:4]}-{base_date[4:6]}-{base_date[6:8]}\"\n",
    "\n",
    "def get_previous_analysis(org, project, current_analysis, api_key):\n",
    "    \"\"\"Get the analysis slug immediately prior to the given one.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/analyses/{org}/{project}/light\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        analyses = response.json().get('results', [])\n",
    "        \n",
    "        # Get base date without suffix\n",
    "        current_base = current_analysis.split('-')[0]\n",
    "        \n",
    "        # Find the first analysis that's before our current one\n",
    "        for analysis in analyses:\n",
    "            slug = analysis['slug']\n",
    "            base_slug = slug.split('-')[0]\n",
    "            if base_slug < current_base:\n",
    "                return slug\n",
    "                \n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching analyses: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis, api_key):\n",
    "    \"\"\"Fetch data for URLs with titles and search console metrics, sorted by impressions.\"\"\"\n",
    "    # Get date range for search console data\n",
    "    end_date = format_analysis_date(analysis)\n",
    "    prev_analysis = get_previous_analysis(org, project, analysis, api_key)\n",
    "    if prev_analysis:\n",
    "        start_date = format_analysis_date(prev_analysis)\n",
    "    else:\n",
    "        # Fallback to 7 days before if no previous analysis found\n",
    "        start_date = end_date  # You may want to subtract 7 days here\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"collections\": [\n",
    "            f\"crawl.{analysis}\",\n",
    "            \"search_console\"\n",
    "        ],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\",\n",
    "                f\"crawl.{analysis}.metadata.title.content\"\n",
    "            ],\n",
    "            \"metrics\": [\n",
    "                \"search_console.period_0.count_impressions\",\n",
    "                \"search_console.period_0.count_clicks\"\n",
    "            ],\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": \"search_console.period_0.count_impressions\",\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"periods\": [\n",
    "            [start_date, end_date]\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    response = httpx.post(url, headers=headers, json=data_payload, timeout=60.0)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Run the query and load results\n",
    "data = get_bqlv2_data(org, project, analysis, api_key)\n",
    "\n",
    "# Flatten the data into a DataFrame with URL, title, and search console metrics\n",
    "columns = [\"url\", \"title\", \"impressions\", \"clicks\"]\n",
    "df = pd.DataFrame([\n",
    "    item['dimensions'] + item['metrics'] \n",
    "    for item in data['results']\n",
    "], columns=columns)\n",
    "\n",
    "# Display the first 500 URLs and titles\n",
    "df.head(500).to_csv(\"first_500_urls_titles.csv\", index=False)\n",
    "print(\"Data saved to first_500_urls_titles.csv\")\n",
    "\n",
    "# Show a preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "| | url                               | title              | impressions | clicks |\n",
    "|-|-----------------------------------|--------------------|-------------|--------|\n",
    "|0| https://example.com/foo           | Foo Page Title    | 1200        | 35     |\n",
    "|1| https://example.com/bar           | Bar Page Title    | 1150        | 40     |\n",
    "|2| https://example.com/baz           | Baz Page Title    | 980         | 25     |\n",
    "\n",
    "\n",
    "**Rationale**: So that I can jump up and down screaming that BQL is not SQL and tell the LLMs to stop showing me SQL examples for BQL. Surely SQL is down there somewhere, but it's ***API-wrapped***. Though this does not spare us from some SQL methodology. For example, table-joins across Collections are a thing—demonstrated here as `search_console` joined with `crawl.YYMMDD`, left-outer if I'm reading it correctly (I may have to amend that). If you really wanna know, Collections are table aliases that help with the API-wrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# # Query Segments: How to Get Pagetype Segment Data for a Project With URL Counts\n",
    "\n",
    "This query requires the \"collection\" field in your config.json file, in addition to \"org\", \"project\", and \"analysis\".\n",
    "Example config.json:\n",
    "```json\n",
    "{\n",
    "    \"org\": \"org_name\",\n",
    "    \"project\": \"project_name\",\n",
    "    \"analysis\": \"20241230\",\n",
    "    \"collection\": \"crawl.20241230\"\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "\n",
    "# Load configuration values from config.json\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Extract configuration details\n",
    "org = config[\"org\"]\n",
    "project = config[\"project\"]\n",
    "analysis = config[\"analysis\"]\n",
    "collection = config[\"collection\"]\n",
    "\n",
    "# Load the API key from botify_token.txt\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "\n",
    "# Define the URL for the API request\n",
    "url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "\n",
    "# Set headers for authorization and content type\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the payload for the API query\n",
    "payload = {\n",
    "    \"collections\": [collection],\n",
    "    \"query\": {\n",
    "        \"dimensions\": [{\"field\": \"segments.pagetype.value\"}],\n",
    "        \"metrics\": [{\"field\": f\"{collection}.count_urls_crawl\"}],\n",
    "        \"sort\": [\n",
    "            {\"type\": \"metrics\", \"index\": 0, \"order\": \"desc\"},\n",
    "            {\"type\": \"dimensions\", \"index\": 0, \"order\": \"asc\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the POST request to the API\n",
    "response = httpx.post(url, headers=headers, json=payload, timeout=60)\n",
    "response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "# Get the results from the response JSON\n",
    "results = response.json()\n",
    "\n",
    "# Use json.dumps with separators and indent for compact pretty printing\n",
    "print(json.dumps(results, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"results\": [\n",
    "        {\n",
    "            \"dimensions\": [\"pdp\"],\n",
    "            \"metrics\": [82150]\n",
    "        },\n",
    "        {\n",
    "            \"dimensions\": [\"plp\"],\n",
    "            \"metrics\": [53400]\n",
    "        },\n",
    "        {\n",
    "            \"dimensions\": [\"category\"],\n",
    "            \"metrics\": [44420]\n",
    "        },\n",
    "        [...]\n",
    "    ],\n",
    "    \"previous\": null,\n",
    "    \"next\": \"https://api.botify.com/v1/org/project/query?page=1\",\n",
    "    \"page\": 1,\n",
    "    \"size\": 10\n",
    "}\n",
    "```\n",
    "\n",
    "**Rationale**: To give you an example that uses dimensions, metrics and sorting all at once. Also to show you the `page` parameter on the querystring making you think it's the **GET method**, `org` & `project` arguments posing as folders, and finally a JSON `payload` showing you it's actually using the **POST method**. Ahhh, *gotta love the Botify API*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# List Collections: How To Get the List of Collections Given a Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of Collections Given a Project\n",
    "\n",
    "import json\n",
    "import httpx\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "\n",
    "def fetch_collections(org, project, api_key):\n",
    "    \"\"\"Fetch collection IDs for a given project from the Botify API.\"\"\"\n",
    "    collections_url = f\"https://api.botify.com/v1/projects/{org}/{project}/collections\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(collections_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        collections_data = response.json()\n",
    "        return [\n",
    "            (collection['id'], collection['name']) for collection in collections_data\n",
    "        ]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching collections for project '{project}': {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Fetch collections\n",
    "collections = fetch_collections(org, project, api_key)\n",
    "for collection_id, collection_name in collections:\n",
    "    print(f\"ID: {collection_id}, Name: {collection_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "ID: crawl.20240917, Name: 2024 Sept. 17th\n",
    "ID: actionboard_ml.20240917, Name: ActionBoard ML\n",
    "ID: crawl.20240715, Name: 2024 July 15th\n",
    "ID: search_engines_orphans.20240715, Name: Search Engines Orphans\n",
    "```\n",
    "\n",
    "**Rationale**: To let you know how tough Collections are once you start digging in. The first challenge is simply knowing what collections you have and what you can do with them—though 9 out of 10 times it's `crawl.YYYYMMDD` and `search_console`. If not, come talk to me, I wanna pick your brain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# List Fields: How To Get The List of Fields Given a Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of Fields Given a Collection\n",
    "\n",
    "import json\n",
    "import httpx\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "collection = config['collection']\n",
    "\n",
    "def fetch_fields(org, project, collection, api_key):\n",
    "    \"\"\"Fetch available fields for a given collection from the Botify API.\"\"\"\n",
    "    fields_url = f\"https://api.botify.com/v1/projects/{org}/{project}/collections/{collection}\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(fields_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        fields_data = response.json()\n",
    "        return [\n",
    "            (field['id'], field['name'])\n",
    "            for dataset in fields_data.get('datasets', [])\n",
    "            for field in dataset.get('fields', [])\n",
    "        ]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching fields for collection '{collection}' in project '{project}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch and print fields\n",
    "fields = fetch_fields(org, project, collection, api_key)\n",
    "print(f\"Fields for collection '{collection}':\")\n",
    "for field_id, field_name in fields:\n",
    "    print(f\"ID: {field_id}, Name: {field_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "Fields for collection 'crawl.20241101':\n",
    "ID: field_of_vision, Name: Survey the Landscape\n",
    "ID: field_of_dreams, Name: The Mind's Eye\n",
    "ID: straying_far_afield, Name: Go Home Spiderman\n",
    "ID: afield_a_complaint, Name: Red Swingline\n",
    "```\n",
    "\n",
    "**Rationale**: So you've got a collection and have no idea what to do with it? Well, you can always start by listing its fields. Yeah, let's list the fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Get Pagetypes: How To Get the Unfiltered URL Counts by Pagetype for a Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "website = config['project']  # Assuming 'project' here refers to the website\n",
    "analysis_date = config['analysis']  # Date of analysis, formatted as 'YYYYMMDD'\n",
    "\n",
    "def get_first_page_pagetype_url_counts(org, website, analysis_date, api_key, size=5000):\n",
    "    \"\"\"Fetch pagetype segmentation counts for the first page only, sorted by URL count in descending order.\"\"\"\n",
    "    url = f\"https://app.botify.com/api/v1/projects/{org}/{website}/query?size={size}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Token {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-botify-client\": \"spa\",\n",
    "    }\n",
    "    \n",
    "    # Payload to retrieve pagetype segmentation counts, ordered by URL count\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis_date}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [{\"field\": \"segments.pagetype.value\"}],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis_date}.count_urls_crawl\"}],\n",
    "            \"sort\": [\n",
    "                {\"type\": \"metrics\", \"index\": 0, \"order\": \"desc\"},\n",
    "                {\"type\": \"dimensions\", \"index\": 0, \"order\": \"asc\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Make a single request for the first page of results\n",
    "        response = httpx.post(url, headers=headers, json=data_payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Process the first page of results\n",
    "        results = []\n",
    "        for item in data.get(\"results\", []):\n",
    "            pagetype = item[\"dimensions\"][0] if item[\"dimensions\"] else \"Unknown\"\n",
    "            count = item[\"metrics\"][0] if item[\"metrics\"] else 0\n",
    "            results.append({\"Pagetype\": pagetype, \"URL Count\": count})\n",
    "    \n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching pagetype URL counts: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Fetch pagetype URL counts for the first page only\n",
    "results = get_first_page_pagetype_url_counts(org, website, analysis_date, api_key)\n",
    "\n",
    "# Convert results to a DataFrame and save\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"pagetype_url_counts.csv\", index=False)\n",
    "print(\"Data saved to pagetype_url_counts.csv\")\n",
    "\n",
    "# Display a preview\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "Data saved to pagetype_url_counts.csv\n",
    "                 Pagetype  URL Count\n",
    "0                    pdp     250000\n",
    "1                    plp      50000\n",
    "2                    hub       5000\n",
    "3                   blog       2500\n",
    "4                    faq        500\n",
    "```\n",
    "\n",
    "**Rationale**: Do you ever get the feeling a website's folder-structure can tell you something about how it's organized? Yeah, me too. Thankfully, we here at Botify do the ***Regular Expressions*** so you don't have to. And it makes really great great color-coding in the link-graph visualizations. Psst! Wanna see the Death Star?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Get Short Titles: How To Get the First 500 URLs With Short Titles Given Pagetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the First 500 URLs With Short Titles Given Pagetype\n",
    "\n",
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis, api_key):\n",
    "    \"\"\"Fetch data based on BQLv2 query for a specific Botify analysis.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # BQLv2 query payload\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\",\n",
    "                f\"crawl.{analysis}.metadata.title.len\",\n",
    "                f\"crawl.{analysis}.metadata.title.content\",\n",
    "                f\"crawl.{analysis}.metadata.title.quality\",\n",
    "                f\"crawl.{analysis}.metadata.description.content\",\n",
    "                f\"crawl.{analysis}.metadata.structured.breadcrumb.tree\",\n",
    "                f\"crawl.{analysis}.metadata.h1.contents\",\n",
    "                f\"crawl.{analysis}.metadata.h2.contents\"\n",
    "            ],\n",
    "            \"metrics\": [],\n",
    "            \"filters\": {\n",
    "                \"and\": [\n",
    "                    {\n",
    "                        \"field\": f\"crawl.{analysis}.scoring.issues.title_len\",\n",
    "                        \"predicate\": \"eq\",\n",
    "                        \"value\": True\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": f\"crawl.{analysis}.segments.pagetype.depth_1\",\n",
    "                        \"value\": \"pdp\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": f\"crawl.{analysis}.metadata.title.len\",\n",
    "                    \"order\": \"asc\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Send the request\n",
    "    # Allow up to a minute for the API to respond\n",
    "    response = httpx.post(url, headers=headers, json=data_payload, timeout=60.0)\n",
    "    response.raise_for_status()  # Check for errors\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Run the query and load results\n",
    "data = get_bqlv2_data(org, project, analysis, api_key)\n",
    "\n",
    "# Flatten the data into a DataFrame\n",
    "# Define column names for each dimension in the data\n",
    "columns = [\n",
    "    \"url\", \"title_len\", \"title_content\", \"title_quality\",\n",
    "    \"description_content\", \"breadcrumb_tree\", \"h1_contents\", \"h2_contents\"\n",
    "]\n",
    "df = pd.DataFrame([item['dimensions'] for item in data['results']], columns=columns)\n",
    "\n",
    "print(\"Data saved to titles_too_short.csv\")\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display all rows \n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Increase column width to avoid truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "**Data saved to titles_too_short.csv**\n",
    "\n",
    "| url                                      | title_len | title_content     | title_quality | description_content                                              | breadcrumb_tree                                       | h1_contents                     | h2_contents                             |\n",
    "|------------------------------------------|-----------|-------------------|---------------|------------------------------------------------------------------|--------------------------------------------------------|----------------------------------|-----------------------------------------|\n",
    "| https://www.example.com/site/socks/12345 | 8         | Sock Store        | unique        | Best socks for every season, unbeatable comfort and style.      | Example Store/Footwear/Accessories/Socks               | [Our Socks]                     | [Features, Sizes, Related Items]        |\n",
    "| https://www.example.com/site/hats/98765  | 9         | Top Hats Here!    | duplicate     | Stylish hats available year-round with exclusive discounts.     | Example Store/Apparel/Accessories/Hats                 | [Hat Selection]                 | [Details, Reviews, Top Picks]           |\n",
    "| https://www.example.com/site/shirts/54321| 10        | - Shirt Emporium  | duplicate     | Discover comfortable and stylish shirts at great prices.        | Example Store/Apparel/Topwear/Shirts                   | [Shirt Central]                 | [Sizing, Similar Styles, Reviews]       |\n",
    "\n",
    "...\n",
    "\n",
    "**Rationale**: Ahh, ***title tags***. They show in browser bookmarks, tabs and SERPs—the only relevancy factor that will remain standing after SEO Armageddon. You could ditch every other factor but ***anchor text***, set your uber-crawler go off-site, use a click-depth of 4—and harvest yourself a pretty good link-graph of the entire Internet... were it not for spammers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "# Count Short Titles: How To Count Number of URLs Having Short Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "def count_short_titles(org, project, analysis, api_key):\n",
    "    \"\"\"Count URLs with short titles for a specific Botify analysis.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Token {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Data payload for the count of URLs with short titles\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [],  # No dimensions needed, as we only need a count\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"function\": \"count\",\n",
    "                    \"args\": [f\"crawl.{analysis}.url\"]\n",
    "                }\n",
    "            ],\n",
    "            \"filters\": {\n",
    "                \"field\": f\"crawl.{analysis}.scoring.issues.title_len\",\n",
    "                \"predicate\": \"eq\",\n",
    "                \"value\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send the request\n",
    "        response = httpx.post(url, headers=headers, json=data_payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract the count of URLs with short titles\n",
    "        short_title_count = data[\"results\"][0][\"metrics\"][0]\n",
    "        return short_title_count\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching short title count for analysis '{analysis}' in project '{project}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the count query and display the result\n",
    "short_title_count = count_short_titles(org, project, analysis, api_key)\n",
    "\n",
    "if short_title_count is not None:\n",
    "    print(f\"Number of URLs with short titles: {short_title_count:,}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the count of URLs with short titles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Number of URLs with short titles: 675,080\n",
    "\n",
    "**Rationale**: Sometimes ya gotta count what you're trying to get before you go try and download it. Plus, learn ***filtering*** in the Botify API! But I think really I just wanted to show you how easy it is to format `f\"{big_numbers:,}\"` with commas using ***f-strings*** (I'm talking to you humans—because the LLMs *already know*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "# Download CSV: How To Download Up to 10K URLs Having Short Titles As a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import time\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "\n",
    "def start_export_job_for_short_titles(org, project, analysis, api_key):\n",
    "    \"\"\"Start an export job for URLs with short titles, downloading key metadata fields.\"\"\"\n",
    "    url = \"https://api.botify.com/v1/jobs\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Token {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Data payload for the export job with necessary fields\n",
    "    data_payload = {\n",
    "        \"job_type\": \"export\",\n",
    "        \"payload\": {\n",
    "            \"username\": org,\n",
    "            \"project\": project,\n",
    "            \"connector\": \"direct_download\",\n",
    "            \"formatter\": \"csv\",\n",
    "            \"export_size\": 10000,  # Adjust as needed\n",
    "            \"query\": {\n",
    "                \"collections\": [f\"crawl.{analysis}\"],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [\n",
    "                        f\"crawl.{analysis}.url\",\n",
    "                        f\"crawl.{analysis}.metadata.title.len\",\n",
    "                        f\"crawl.{analysis}.metadata.title.content\",\n",
    "                        f\"crawl.{analysis}.metadata.title.quality\",\n",
    "                        f\"crawl.{analysis}.metadata.description.content\",\n",
    "                        f\"crawl.{analysis}.metadata.h1.contents\"\n",
    "                    ],\n",
    "                    \"metrics\": [],\n",
    "                    \"filters\": {\n",
    "                        \"field\": f\"crawl.{analysis}.scoring.issues.title_len\",\n",
    "                        \"predicate\": \"eq\",\n",
    "                        \"value\": True\n",
    "                    },\n",
    "                    \"sort\": [\n",
    "                        {\n",
    "                            \"field\": f\"crawl.{analysis}.metadata.title.len\",\n",
    "                            \"order\": \"asc\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Starting export job for short titles...\")\n",
    "        # Use a longer timeout to prevent ReadTimeout errors\n",
    "        response = httpx.post(url, headers=headers, json=data_payload, timeout=300.0)\n",
    "        response.raise_for_status()\n",
    "        export_job_details = response.json()\n",
    "        \n",
    "        # Extract job URL for polling\n",
    "        job_url = f\"https://api.botify.com{export_job_details.get('job_url')}\"\n",
    "        \n",
    "        # Polling for job completion\n",
    "        print(\"Polling for job completion:\", end=\" \")\n",
    "        while True:\n",
    "            time.sleep(5)\n",
    "            # Use a longer timeout for polling requests as well\n",
    "            poll_response = httpx.get(job_url, headers=headers, timeout=120.0)\n",
    "            poll_response.raise_for_status()\n",
    "            job_status_details = poll_response.json()\n",
    "            \n",
    "            if job_status_details[\"job_status\"] == \"DONE\":\n",
    "                download_url = job_status_details[\"results\"][\"download_url\"]\n",
    "                print(\"\\nDownload URL:\", download_url)\n",
    "                return download_url\n",
    "            elif job_status_details[\"job_status\"] == \"FAILED\":\n",
    "                print(\"\\nJob failed. Error details:\", job_status_details)\n",
    "                return None\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "    except httpx.ReadTimeout as e:\n",
    "        print(f\"\\nTimeout error during export job: {e}\")\n",
    "        print(\"The API request timed out. Consider increasing the timeout value or try again later.\")\n",
    "        return None\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"\\nError starting or polling export job: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Start export job and get download URL\n",
    "download_url = start_export_job_for_short_titles(org, project, analysis, api_key)\n",
    "\n",
    "# Download and decompress the file if the download URL is available\n",
    "if download_url:\n",
    "    gz_filename = \"short_titles_export.csv.gz\"\n",
    "    csv_filename = \"short_titles_export.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Download the gzipped CSV file with increased timeout\n",
    "        response = httpx.get(download_url, timeout=300.0)\n",
    "        with open(gz_filename, \"wb\") as gz_file:\n",
    "            gz_file.write(response.content)\n",
    "        print(f\"File downloaded as '{gz_filename}'\")\n",
    "        \n",
    "        # Step 2: Decompress the .gz file to .csv\n",
    "        with gzip.open(gz_filename, \"rb\") as f_in:\n",
    "            with open(csv_filename, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"File decompressed and saved as '{csv_filename}'\")\n",
    "    except httpx.ReadTimeout as e:\n",
    "        print(f\"Timeout error during file download: {e}\")\n",
    "        print(\"The download request timed out. Try again later or download manually from the URL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during file download or decompression: {e}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the download URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Starting export job for short titles...  \n",
    "    Polling for job completion: .  \n",
    "    Download URL: https://cdn.example.com/export_data/abc/def/ghi/xyz1234567890/funfetti-2024-11-08.csv.gz  \n",
    "    File downloaded as 'short_titles_export.csv.gz'  \n",
    "    File decompressed and saved as 'short_titles_export.csv'  \n",
    "\n",
    "**Rationale**: Is it pulling or pooling? I could never remember. In either case, exporting and downloading csv-files is not as straightforward as you think. First, you make the request. Then you look for ***where*** to check progress, then keep re-checking until done. Then you sacrifice a chicken to help you debug useless errors. Lastly, you notice how your endpoint has changed to`https://api.botify.com/v1/jobs` with `org` and `project` moved into the JSON payload. Or is that firstly? Yeah, definitely firstly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# Get Total Count: How To Get Aggregate Count of All URLs Crawled During Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Load necessary details from files (adjust paths if needed)\n",
    "# Basic error handling for loading is good practice even in tutorials\n",
    "try:\n",
    "    config = json.load(open(\"config.json\"))\n",
    "    api_key = open('botify_token.txt').read().strip()\n",
    "    # Get specific config values needed\n",
    "    ORG_SLUG = config['org']\n",
    "    PROJECT_SLUG = config['project']\n",
    "    ANALYSIS_SLUG = config['analysis']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.json' or 'botify_token.txt' not found.\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Key '{e}' not found in 'config.json'.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    exit()\n",
    "\n",
    "# TARGET_MAX_DEPTH is no longer needed for this calculation\n",
    "\n",
    "# --- 2. Function to Get URL Counts Per Depth ---\n",
    "def get_all_urls_by_depth(org, project, analysis, key):\n",
    "    \"\"\"\n",
    "    Fetches URL counts for ALL depths from the Botify API.\n",
    "    Returns a dictionary {depth: count} or None on error.\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\"Authorization\": f\"Token {key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # This payload asks for counts grouped by depth, for the entire crawl\n",
    "    payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [f\"crawl.{analysis}.depth\"],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis}.count_urls_crawl\"}],\n",
    "            \"sort\": [{\"field\": f\"crawl.{analysis}.depth\", \"order\": \"asc\"}]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Requesting total URL count of site from Botify API...\")\n",
    "        response = httpx.post(api_url, headers=headers, json=payload, timeout=120.0)\n",
    "        response.raise_for_status() # Check for HTTP errors (like 4xx, 5xx)\n",
    "        print(\"Data received successfully.\")\n",
    "\n",
    "        # Convert the response list into a more usable {depth: count} dictionary\n",
    "        results = response.json().get(\"results\", [])\n",
    "        depth_counts = {\n",
    "            row[\"dimensions\"][0]: row[\"metrics\"][0]\n",
    "            for row in results\n",
    "            if \"dimensions\" in row and len(row[\"dimensions\"]) > 0 and \\\n",
    "               \"metrics\" in row and len(row[\"metrics\"]) > 0 and \\\n",
    "               isinstance(row[\"dimensions\"][0], int) # Ensure depth is an int\n",
    "        }\n",
    "        return depth_counts\n",
    "\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"API Error: {e.response.status_code} - {e.response.text}\")\n",
    "        return None # Indicate failure\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during API call or processing: {e}\")\n",
    "        return None # Indicate failure\n",
    "\n",
    "# --- 3. Main Calculation (Grand Total) ---\n",
    "# Get the depth data using the function\n",
    "all_depth_data = get_all_urls_by_depth(ORG_SLUG, PROJECT_SLUG, ANALYSIS_SLUG, api_key)\n",
    "\n",
    "# Proceed only if we got data back\n",
    "if all_depth_data is not None:\n",
    "    # Calculate the grand total by summing all counts\n",
    "    grand_total_urls = 0\n",
    "    print(f\"\\nCalculating the grand total number of URLs from all depths...\")\n",
    "\n",
    "    # Loop through all counts returned in the dictionary values and add them up\n",
    "    for count in all_depth_data.values():\n",
    "        grand_total_urls += count\n",
    "\n",
    "    # --- Alternatively, you could use the sum() function directly: ---\n",
    "    # grand_total_urls = sum(all_depth_data.values())\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    # Print the final result\n",
    "    print(f\"\\nResult: Grand Total URLs in Crawl = {grand_total_urls:,}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate total because API data retrieval failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "Requesting total URL count of site from Botify API...\n",
    "Data received successfully.\n",
    "\n",
    "Calculating the grand total number of URLs from all depths...\n",
    "\n",
    "Result: Grand Total URLs in Crawl = 3,000,000\n",
    "\n",
    "**Rationale**: Before doing a download of a CSV it is often worth checking if the number of rows returned will be under the 1-million row API limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Get Depth Count: How To Get Aggregate Count of URLs at Particular Click Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Load necessary details from files (adjust paths if needed)\n",
    "# Basic error handling for loading is good practice even in tutorials\n",
    "try:\n",
    "    config = json.load(open(\"config.json\"))\n",
    "    api_key = open('botify_token.txt').read().strip()\n",
    "    # Get specific config values needed\n",
    "    ORG_SLUG = config['org']\n",
    "    PROJECT_SLUG = config['project']\n",
    "    ANALYSIS_SLUG = config['analysis']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.json' or 'botify_token.txt' not found.\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Key '{e}' not found in 'config.json'.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the maximum depth for our calculation\n",
    "TARGET_MAX_DEPTH = 5\n",
    "\n",
    "# --- 2. Function to Get URL Counts Per Depth ---\n",
    "def get_all_urls_by_depth(org, project, analysis, key):\n",
    "    \"\"\"\n",
    "    Fetches URL counts for ALL depths from the Botify API.\n",
    "    Returns a dictionary {depth: count} or None on error.\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\"Authorization\": f\"Token {key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # This payload asks for counts grouped by depth, for the entire crawl\n",
    "    payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [f\"crawl.{analysis}.depth\"],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis}.count_urls_crawl\"}],\n",
    "            \"sort\": [{\"field\": f\"crawl.{analysis}.depth\", \"order\": \"asc\"}]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Requesting data from Botify API...\")\n",
    "        response = httpx.post(api_url, headers=headers, json=payload, timeout=120.0)\n",
    "        response.raise_for_status() # Check for HTTP errors (like 4xx, 5xx)\n",
    "        print(\"Data received successfully.\")\n",
    "\n",
    "        # Convert the response list into a more usable {depth: count} dictionary\n",
    "        results = response.json().get(\"results\", [])\n",
    "        depth_counts = {\n",
    "            row[\"dimensions\"][0]: row[\"metrics\"][0]\n",
    "            for row in results\n",
    "            if \"dimensions\" in row and len(row[\"dimensions\"]) > 0 and \\\n",
    "               \"metrics\" in row and len(row[\"metrics\"]) > 0 and \\\n",
    "               isinstance(row[\"dimensions\"][0], int) # Ensure depth is an int\n",
    "        }\n",
    "        return depth_counts\n",
    "\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"API Error: {e.response.status_code} - {e.response.text}\")\n",
    "        return None # Indicate failure\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during API call or processing: {e}\")\n",
    "        return None # Indicate failure\n",
    "\n",
    "# --- 3. Main Calculation ---\n",
    "# Get the depth data using the function\n",
    "all_depth_data = get_all_urls_by_depth(ORG_SLUG, PROJECT_SLUG, ANALYSIS_SLUG, api_key)\n",
    "\n",
    "# Proceed only if we got data back\n",
    "if all_depth_data is not None:\n",
    "    total_count_at_or_below_depth = 0\n",
    "    print(f\"\\nCalculating total for depth <= {TARGET_MAX_DEPTH}...\")\n",
    "\n",
    "    # Loop through the dictionary and sum counts for relevant depths\n",
    "    for depth, count in all_depth_data.items():\n",
    "        if depth <= TARGET_MAX_DEPTH:\n",
    "            total_count_at_or_below_depth += count\n",
    "\n",
    "    # Print the final result\n",
    "    print(f\"\\nResult: Total URLs at depth {TARGET_MAX_DEPTH} or less = {total_count_at_or_below_depth:,}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate total because API data retrieval failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "Attempting to get full URL distribution by depth...\n",
    "\n",
    "Calculating total URLs for depth <= 5 from received data...\n",
    "\n",
    "Total URLs at depth 5 or less: 1,500,000\n",
    "\n",
    "**Rationale**: Before doing a download of a CSV it is often worth checking if the number of rows returned will be under the 1-million row API limit. By using a depth filter, we now have the foundation for reducing depth until we get a downloadable number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Get Aggregates: How To Get Map of Click-Depths Aggregates Given Analysis Slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pprint # Keep pprint for the final output as in the original\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Load necessary details from files (adjust paths if needed)\n",
    "try:\n",
    "    config = json.load(open(\"config.json\"))\n",
    "    api_key = open('botify_token.txt').read().strip()\n",
    "    # Get specific config values needed\n",
    "    ORG_SLUG = config['org']\n",
    "    PROJECT_SLUG = config['project']\n",
    "    ANALYSIS_SLUG = config['analysis']\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.json' or 'botify_token.txt' not found.\")\n",
    "    exit(1)\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Key '{e}' not found in 'config.json'.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 2. Function to Get URL Counts Per Depth ---\n",
    "# (Kept original function name)\n",
    "def get_urls_by_depth(org, project, analysis, key):\n",
    "    \"\"\"\n",
    "    Fetches URL counts aggregated by depth from the Botify API.\n",
    "    Returns a dictionary {depth: count} or an empty {} on error.\n",
    "    (Matches original functionality return type on error)\n",
    "    \"\"\"\n",
    "    # Use clearer variable name for URL\n",
    "    api_url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\"Authorization\": f\"Token {key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Use clearer variable name for payload\n",
    "    payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [f\"crawl.{analysis}.depth\"],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis}.count_urls_crawl\"}],\n",
    "            \"sort\": [{\"field\": f\"crawl.{analysis}.depth\", \"order\": \"asc\"}]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Requesting URL counts per depth from Botify API...\")\n",
    "        # Use a longer timeout (e.g., 120 seconds)\n",
    "        response = httpx.post(api_url, headers=headers, json=payload, timeout=120.0)\n",
    "        response.raise_for_status() # Check for HTTP errors (like 4xx, 5xx)\n",
    "        print(\"Data received successfully.\")\n",
    "\n",
    "        # Convert the response list into a more usable {depth: count} dictionary\n",
    "        # Add validation during processing\n",
    "        response_data = response.json()\n",
    "        results = response_data.get(\"results\", [])\n",
    "\n",
    "        depth_distribution = {}\n",
    "        print(\"Processing API response...\")\n",
    "        for row in results:\n",
    "             # Validate structure before accessing keys/indices\n",
    "             if \"dimensions\" in row and len(row[\"dimensions\"]) == 1 and \\\n",
    "                \"metrics\" in row and len(row[\"metrics\"]) == 1:\n",
    "                 depth = row[\"dimensions\"][0]\n",
    "                 count = row[\"metrics\"][0]\n",
    "                 # Ensure depth is an integer\n",
    "                 if isinstance(depth, int):\n",
    "                     depth_distribution[depth] = count\n",
    "                 else:\n",
    "                     print(f\"Warning: Skipping row with non-integer depth: {row}\")\n",
    "             else:\n",
    "                 print(f\"Warning: Skipping row with unexpected structure: {row}\")\n",
    "\n",
    "        print(\"Processing complete.\")\n",
    "        return depth_distribution # Return the dictionary with results\n",
    "\n",
    "    # Adopt more specific error handling from the target style\n",
    "    except httpx.ReadTimeout:\n",
    "        print(\"Request timed out after 120 seconds.\")\n",
    "        return {} # Return empty dict per original functionality\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"API Error: {e.response.status_code} - {e.response.reason_phrase}\")\n",
    "        try:\n",
    "            # Attempt to show detailed API error message\n",
    "            error_details = e.response.json()\n",
    "            print(f\"Error details: {error_details}\")\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback if response is not JSON\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return {} # Return empty dict per original functionality\n",
    "    except httpx.RequestError as e:\n",
    "        # Handles other request errors like connection issues\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "        return {} # Return empty dict per original functionality\n",
    "    except (KeyError, IndexError, TypeError, json.JSONDecodeError) as e:\n",
    "         # Catch issues during JSON processing or accessing expected keys/indices\n",
    "         print(f\"Error processing API response: {e}\")\n",
    "         if 'response' in locals(): # Log raw response if available\n",
    "              print(f\"Response Text: {response.text}\")\n",
    "         return {} # Return empty dict per original functionality\n",
    "    except Exception as e:\n",
    "        # Catch-all for any other unexpected errors in this function\n",
    "        print(f\"An unexpected error occurred in get_urls_by_depth: {e}\")\n",
    "        return {} # Return empty dict per original functionality\n",
    "\n",
    "\n",
    "# --- 3. Main Execution ---\n",
    "print(\"\\nStarting script execution to get URL distribution by depth...\")\n",
    "try:\n",
    "    # Call the function using loaded config variables\n",
    "    depth_distribution_result = get_urls_by_depth(\n",
    "        ORG_SLUG, PROJECT_SLUG, ANALYSIS_SLUG, api_key\n",
    "    )\n",
    "\n",
    "    # Check if the result is not an empty dictionary\n",
    "    # An empty dict signifies an error occurred in the function OR no data found\n",
    "    if depth_distribution_result: # A non-empty dict evaluates to True\n",
    "        print(\"\\n--- URL Distribution by Depth ---\")\n",
    "        # Use pprint for formatted output as in the original script\n",
    "        pprint.pprint(depth_distribution_result, width=1)\n",
    "        print(\"---------------------------------\")\n",
    "    else:\n",
    "        # This message prints if the function returned {}\n",
    "        print(\"\\nRetrieved empty distribution. This could be due to an error (check logs above) or no URLs found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch any unexpected errors during the main execution sequence\n",
    "    print(f\"An unexpected error occurred during the main execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```plaintext\n",
    "Configuration loaded successfully.\n",
    "\n",
    "Starting script execution to get URL distribution by depth...\n",
    "Requesting URL counts per depth from Botify API...\n",
    "Data received successfully.\n",
    "Processing API response...\n",
    "Processing complete.\n",
    "\n",
    "--- URL Distribution by Depth (Scaled Generic Example) ---\n",
    "{ 0: 10,        # Start pages (kept small)\n",
    "  1: 550,\n",
    "  2: 28000,     # Rounded 5k * 5.5\n",
    "  3: 275000,    # 50k * 5.5\n",
    "  4: 825000,    # Peak (150k * 5.5)\n",
    "  5: 660000,    # Starting decline (120k * 5.5)\n",
    "  6: 440000,    # 80k * 5.5\n",
    "  7: 275000,    # 50k * 5.5\n",
    "  8: 165000,    # 30k * 5.5\n",
    "  9: 110000,    # 20k * 5.5\n",
    " 10: 55000,     # 10k * 5.5\n",
    " 11: 44000,     # 8k * 5.5\n",
    " 12: 33000,     # 6k * 5.5\n",
    " 13: 28000,     # Rounded 5k * 5.5\n",
    " 14: 22000,     # 4k * 5.5\n",
    " 15: 17000,     # Rounded 3k * 5.5\n",
    " 16: 11000,     # 2k * 5.5\n",
    " 17: 6000,      # Rounded 1k * 5.5\n",
    " 18: 3000,      # Rounded 500 * 5.5\n",
    " 19: 1100,      # 200 * 5.5\n",
    " 20: 550 }      # 100 * 5.5\n",
    "--------------------------------------------------------\n",
    "```\n",
    "\n",
    "**Rationale**: This depth distribution shows how many URLs exist at each click depth level from the homepage (hompage = depth 0). A healthy site typically has most content within 3 or 4 clicks of the homepage. Much more, and it may as well not exist. Such reports help identify potential deep crawl issues, spider-traps, and why (in addition to the infinite spam-cannon of generative AI content), brute-force crawls that *\"make a copy of the Internet\"* are all but dead. And did I mention that excessively crawl-able faceted search makes your site's link-graph look like the Death Star? Yeah, I think I did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "# Get Aggregates: How To Get Map of CUMULATIVE Click-Depths Aggregates Given Analysis Slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pprint # Keep pprint for the final output\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Load necessary details from files (adjust paths if needed)\n",
    "try:\n",
    "    config = json.load(open(\"config.json\"))\n",
    "    api_key = open('botify_token.txt').read().strip()\n",
    "    # Get specific config values needed\n",
    "    ORG_SLUG = config['org']\n",
    "    PROJECT_SLUG = config['project']\n",
    "    ANALYSIS_SLUG = config['analysis']\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.json' or 'botify_token.txt' not found.\")\n",
    "    exit(1)\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Key '{e}' not found in 'config.json'.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 2. Function to Get URL Counts Per Depth ---\n",
    "# (This function remains unchanged as it fetches the base data needed)\n",
    "def get_urls_by_depth(org, project, analysis, key):\n",
    "    \"\"\"\n",
    "    Fetches URL counts aggregated by depth from the Botify API.\n",
    "    Returns a dictionary {depth: count} or an empty {} on error.\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\"Authorization\": f\"Token {key}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [f\"crawl.{analysis}.depth\"],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis}.count_urls_crawl\"}],\n",
    "            \"sort\": [{\"field\": f\"crawl.{analysis}.depth\", \"order\": \"asc\"}]\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        print(\"Requesting URL counts per depth from Botify API...\")\n",
    "        response = httpx.post(api_url, headers=headers, json=payload, timeout=120.0)\n",
    "        response.raise_for_status()\n",
    "        print(\"Data received successfully.\")\n",
    "        response_data = response.json()\n",
    "        results = response_data.get(\"results\", [])\n",
    "        depth_distribution = {}\n",
    "        print(\"Processing API response...\")\n",
    "        for row in results:\n",
    "             if \"dimensions\" in row and len(row[\"dimensions\"]) == 1 and \\\n",
    "                \"metrics\" in row and len(row[\"metrics\"]) == 1:\n",
    "                 depth = row[\"dimensions\"][0]\n",
    "                 count = row[\"metrics\"][0]\n",
    "                 if isinstance(depth, int):\n",
    "                     depth_distribution[depth] = count\n",
    "                 else:\n",
    "                     print(f\"Warning: Skipping row with non-integer depth: {row}\")\n",
    "             else:\n",
    "                 print(f\"Warning: Skipping row with unexpected structure: {row}\")\n",
    "        print(\"Processing complete.\")\n",
    "        return depth_distribution\n",
    "    except httpx.ReadTimeout:\n",
    "        print(\"Request timed out after 120 seconds.\")\n",
    "        return {}\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"API Error: {e.response.status_code} - {e.response.reason_phrase}\")\n",
    "        try:\n",
    "            error_details = e.response.json()\n",
    "            print(f\"Error details: {error_details}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return {}\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "        return {}\n",
    "    except (KeyError, IndexError, TypeError, json.JSONDecodeError) as e:\n",
    "         print(f\"Error processing API response: {e}\")\n",
    "         if 'response' in locals():\n",
    "              print(f\"Response Text: {response.text}\")\n",
    "         return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in get_urls_by_depth: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# --- 3. Main Execution ---\n",
    "print(\"\\nStarting script execution...\")\n",
    "try:\n",
    "    # Call the function to get the dictionary {depth: count_at_depth}\n",
    "    depth_distribution_result = get_urls_by_depth(\n",
    "        ORG_SLUG, PROJECT_SLUG, ANALYSIS_SLUG, api_key\n",
    "    )\n",
    "\n",
    "    # Check if the result is not an empty dictionary (which indicates an error)\n",
    "    if depth_distribution_result: # A non-empty dict evaluates to True\n",
    "        print(\"\\nCalculating cumulative URL counts by depth...\")\n",
    "\n",
    "        # --- Calculate Cumulative Distribution ---\n",
    "        cumulative_depth_distribution = {}\n",
    "        current_cumulative_sum = 0\n",
    "        # Get the depths present and sort them to process in order (0, 1, 2...)\n",
    "        # Handle case where result might be empty just in case, though checked above\n",
    "        sorted_depths = sorted(depth_distribution_result.keys())\n",
    "        max_depth_found = sorted_depths[-1] if sorted_depths else -1\n",
    "\n",
    "        # Iterate from depth 0 up to the maximum depth found in the results\n",
    "        for depth_level in range(max_depth_found + 1):\n",
    "            # Get the count for this specific depth_level from the original results.\n",
    "            # Use .get(depth, 0) in case a depth level has no URLs (e.g., depth 0 might be missing if start page redirected)\n",
    "            count_at_this_level = depth_distribution_result.get(depth_level, 0)\n",
    "\n",
    "            # Add this level's count to the running cumulative sum\n",
    "            current_cumulative_sum += count_at_this_level\n",
    "\n",
    "            # Store the *cumulative* sum in the new dictionary for this depth level\n",
    "            cumulative_depth_distribution[depth_level] = current_cumulative_sum\n",
    "        # --- End Calculation ---\n",
    "\n",
    "        print(\"\\n--- Cumulative URL Distribution by Depth (URLs <= Depth) ---\")\n",
    "        # Use pprint for formatted output of the NEW cumulative dictionary\n",
    "        pprint.pprint(cumulative_depth_distribution, width=1)\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "    else:\n",
    "        # This message prints if the function returned {}\n",
    "        print(\"\\nRetrieved empty distribution. Cannot calculate cumulative counts.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch any unexpected errors during the main execution sequence\n",
    "    print(f\"An unexpected error occurred during the main execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```plaintext\n",
    "Configuration loaded successfully.\n",
    "\n",
    "Starting script execution...\n",
    "Requesting URL counts per depth from Botify API...\n",
    "Data received successfully.\n",
    "Processing API response...\n",
    "Processing complete.\n",
    "\n",
    "Calculating cumulative URL counts by depth...\n",
    "\n",
    "--- Cumulative URL Distribution by Depth (Scaled Generic Example) ---\n",
    "{ 0: 10,\n",
    "  1: 560,          # (10 + 550)\n",
    "  2: 28560,        # (560 + 28000)\n",
    "  3: 303560,       # (28560 + 275000)\n",
    "  4: 1128560,      # (303560 + 825000) <-- Reaches 1M+\n",
    "  5: 1788560,      # (1128560 + 660000)\n",
    "  6: 2228560,      # (1788560 + 440000) <-- Reaches 2M+\n",
    "  7: 2503560,      # (2228560 + 275000)\n",
    "  8: 2668560,      # (2503560 + 165000)\n",
    "  9: 2778560,      # (2668560 + 110000)\n",
    " 10: 2833560,      # (2778560 + 55000) <-- Growth significantly slower now\n",
    " 11: 2877560,      # (2833560 + 44000)\n",
    " 12: 2910560,      # (2877560 + 33000)\n",
    " 13: 2938560,      # (2910560 + 28000)\n",
    " 14: 2960560,      # (2938560 + 22000)\n",
    " 15: 2977560,      # (2960560 + 17000)\n",
    " 16: 2988560,      # (2977560 + 11000)\n",
    " 17: 2994560,      # (2988560 + 6000)\n",
    " 18: 2997560,      # (2994560 + 3000)\n",
    " 19: 2998660,      # (2997560 + 1100)\n",
    " 20: 2999210 }      # (2998660 + 550) <-- Final cumulative total ~3M\n",
    "------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "**Rationale**: This depth distribution shows how many URLs exist at each click depth level from the homepage (hompage = depth 0). A healthy site typically has most content within 3 or 4 clicks of the homepage. Much more, and it may as well not exist. Such reports help identify potential deep crawl issues, spider-traps, and why (in addition to the infinite spam-cannon of generative AI content), brute-force crawls that *\"make a copy of the Internet\"* are all but dead. And did I mention that excessively crawl-able faceted search makes your site's link-graph look like the Death Star? Yeah, I think I did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# Download Link Graph: How to Download a Link Graph for a Specified Organization, Project, and Analysis For Website Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "# Define API headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Determine optimal click depth for link graph export\n",
    "def find_optimal_depth(org, project, analysis, max_edges=1000000):\n",
    "    \"\"\"\n",
    "    Determine the highest depth for which the number of edges does not exceed max_edges.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    # httpx doesn't have Session, use Client instead\n",
    "    client = httpx.Client()\n",
    "    previous_edges = 0\n",
    "\n",
    "    for depth in range(1, 10):\n",
    "        data_payload = {\n",
    "            \"collections\": [f\"crawl.{analysis}\"],\n",
    "            \"query\": {\n",
    "                \"dimensions\": [],\n",
    "                \"metrics\": [{\"function\": \"sum\", \"args\": [f\"crawl.{analysis}.outlinks_internal.nb.total\"]}],\n",
    "                \"filters\": {\"field\": f\"crawl.{analysis}.depth\", \"predicate\": \"lte\", \"value\": depth},\n",
    "            },\n",
    "        }\n",
    "\n",
    "        response = client.post(url, headers=headers, json=data_payload)\n",
    "        data = response.json()\n",
    "        edges = data[\"results\"][0][\"metrics\"][0]\n",
    "\n",
    "        print(f\"Depth {depth}: {edges:,} edges\")\n",
    "\n",
    "        if edges > max_edges or edges == previous_edges:\n",
    "            return depth - 1 if depth > 1 else depth, previous_edges\n",
    "\n",
    "        previous_edges = edges\n",
    "\n",
    "    return depth, previous_edges\n",
    "\n",
    "# Export link graph to CSV\n",
    "def export_link_graph(org, project, analysis, chosen_depth, save_path=\"downloads\"):\n",
    "    \"\"\"\n",
    "    Export link graph up to the chosen depth level and save as a CSV.\n",
    "    \"\"\"\n",
    "    url = \"https://api.botify.com/v1/jobs\"\n",
    "    data_payload = {\n",
    "        \"job_type\": \"export\",\n",
    "        \"payload\": {\n",
    "            \"username\": org,\n",
    "            \"project\": project,\n",
    "            \"connector\": \"direct_download\",\n",
    "            \"formatter\": \"csv\",\n",
    "            \"export_size\": 1000000,\n",
    "            \"query\": {\n",
    "                \"collections\": [f\"crawl.{analysis}\"],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [\n",
    "                        \"url\",\n",
    "                        f\"crawl.{analysis}.outlinks_internal.graph.url\",\n",
    "                    ],\n",
    "                    \"metrics\": [],\n",
    "                    \"filters\": {\"field\": f\"crawl.{analysis}.depth\", \"predicate\": \"lte\", \"value\": chosen_depth},\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    export_job_details = response.json()\n",
    "    job_url = f\"https://api.botify.com{export_job_details.get('job_url')}\"\n",
    "\n",
    "    # Polling for job completion\n",
    "    attempts = 300\n",
    "    delay = 3\n",
    "    while attempts > 0:\n",
    "        time.sleep(delay)\n",
    "        response_poll = httpx.get(job_url, headers=headers)\n",
    "        job_status_details = response_poll.json()\n",
    "        if job_status_details[\"job_status\"] == \"DONE\":\n",
    "            download_url = job_status_details[\"results\"][\"download_url\"]\n",
    "            save_as_filename = Path(save_path) / f\"{org}_{project}_{analysis}_linkgraph_depth-{chosen_depth}.csv\"\n",
    "            download_file(download_url, save_as_filename)\n",
    "            return save_as_filename\n",
    "        elif job_status_details[\"job_status\"] == \"FAILED\":\n",
    "            print(\"Export job failed.\")\n",
    "            return None\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        attempts -= 1\n",
    "\n",
    "    print(\"Unable to complete download attempts successfully.\")\n",
    "    return None\n",
    "\n",
    "# Download file function\n",
    "def download_file(url, save_path):\n",
    "    \"\"\"\n",
    "    Download a file from a URL to a specified local path.\n",
    "    \"\"\"\n",
    "    # Fix: httpx.get() doesn't support 'stream' parameter directly\n",
    "    with httpx.Client() as client:\n",
    "        with client.stream(\"GET\", url) as response:\n",
    "            with open(save_path, \"wb\") as file:\n",
    "                for chunk in response.iter_bytes(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "    print(f\"\\nFile downloaded as '{save_path}'\")\n",
    "\n",
    "# Main execution\n",
    "print(\"Determining optimal depth for link graph export...\")\n",
    "chosen_depth, final_edges = find_optimal_depth(org, project, analysis)\n",
    "print(f\"Using depth {chosen_depth} with {final_edges:,} edges for export.\")\n",
    "\n",
    "# Make sure the downloads folder exists\n",
    "downloads_folder = Path(\"downloads\")\n",
    "downloads_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Starting link graph export...\")\n",
    "link_graph_path = export_link_graph(org, project, analysis, chosen_depth, save_path=\"downloads\")\n",
    "\n",
    "if link_graph_path:\n",
    "    print(f\"Link graph saved to: {link_graph_path}\")\n",
    "else:\n",
    "    print(\"Link graph export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "```\n",
    "Determining optimal depth for link graph export...\n",
    "Depth 1: 50,000 edges\n",
    "Depth 2: 120,000 edges\n",
    "Depth 3: 500,000 edges\n",
    "Depth 4: 1,200,000 edges\n",
    "Using depth 3 with 500,000 edges for export.\n",
    "Starting link graph export...\n",
    "Polling for job completion: ...\n",
    "Download URL: https://botify-export-url.com/file.csv\n",
    "File downloaded as 'downloads/org_project_analysis_linkgraph_depth-3.csv'\n",
    "Link graph saved to: downloads/org_project_analysis_linkgraph_depth-3.csv\n",
    "```\n",
    "\n",
    "**Rationale**: And now, the moment you’ve all been waiting for—the elusive, hard-to-visualize link-graph of your website. Think Admiral Ackbar scrutinizing a hologram of the Death Star, examining every strength and vulnerability, now superimposed with Google Search Console Clicks and Impressions. The Rebels lean in, studying surprise hot spots and patches of dead wood. Every faceted search site ends up looking like the Death Star. But if you’ve done it right, with solid topical clustering, you’ll have something that resembles broccoli or cauliflower... are those called nodules? Florets? Either way, it’s a good look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "# Check Link-Graph Enhancements: How To Check What Data is Available to Enhance Link-Graph Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "# Load configuration and API key\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {open('botify_token.txt').read().strip()}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def preview_data(org, project, analysis, depth=1):\n",
    "    \"\"\"Preview data availability before committing to full download\"\"\"\n",
    "    # Get analysis date from the slug (assuming YYYYMMDD format)\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    # Calculate period start (7 days before analysis date)\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"collections\": [\n",
    "            f\"crawl.{analysis}\",\n",
    "            \"search_console\"\n",
    "        ],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\"\n",
    "            ],\n",
    "            \"metrics\": [\n",
    "                \"search_console.period_0.count_impressions\",\n",
    "                \"search_console.period_0.count_clicks\"\n",
    "            ],\n",
    "            \"filters\": {\n",
    "                \"field\": f\"crawl.{analysis}.depth\",\n",
    "                \"predicate\": \"lte\",\n",
    "                \"value\": depth\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": \"search_console.period_0.count_impressions\",\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"periods\": [\n",
    "            [\n",
    "                period_start,\n",
    "                period_end\n",
    "            ]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(f\"\\n🔍 Sampling data for {org}/{project}/{analysis}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ Preview failed:\", response.status_code)\n",
    "        return False\n",
    "        \n",
    "    data = response.json()\n",
    "    if not data.get('results'):\n",
    "        print(\"⚠️  No preview data available\")\n",
    "        return False\n",
    "        \n",
    "    print(\"\\n📊 Data Sample Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    metrics_found = 0\n",
    "    for result in data['results'][:3]:  # Show just top 3 for cleaner output\n",
    "        url = result['dimensions'][0]\n",
    "        impressions = result['metrics'][0]\n",
    "        clicks = result['metrics'][1]\n",
    "        metrics_found += bool(impressions or clicks)\n",
    "        print(f\"• URL: {url[:60]}...\")\n",
    "        print(f\"  └─ Performance: {impressions:,} impressions, {clicks:,} clicks\")\n",
    "    \n",
    "    print(\"\\n🎯 Data Quality Check\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"✓ URLs found: {len(data['results'])}\")\n",
    "    print(f\"✓ Search metrics: {'Available' if metrics_found else 'Not found'}\")\n",
    "    print(f\"✓ Depth limit: {depth}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis):\n",
    "    \"\"\"Fetch data based on BQLv2 query for a specific Botify analysis.\"\"\"\n",
    "    collection = f\"crawl.{analysis}\"\n",
    "    \n",
    "    # Calculate dates\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # Base dimensions that should always be available in crawl\n",
    "    base_dimensions = [\n",
    "        f\"{collection}.url\",\n",
    "        f\"{collection}.depth\",\n",
    "    ]\n",
    "    \n",
    "    # Optional dimensions that might not be available\n",
    "    optional_dimensions = [\n",
    "        f\"{collection}.segments.pagetype.value\",\n",
    "        f\"{collection}.compliant.is_compliant\",\n",
    "        f\"{collection}.compliant.main_reason\",\n",
    "        f\"{collection}.canonical.to.equal\",\n",
    "        f\"{collection}.sitemaps.present\",\n",
    "        f\"{collection}.js.rendering.exec\",\n",
    "        f\"{collection}.js.rendering.ok\"\n",
    "    ]\n",
    "    \n",
    "    # Optional metrics from other collections\n",
    "    optional_metrics = [\n",
    "        \"search_console.period_0.count_impressions\",\n",
    "        \"search_console.period_0.count_clicks\"\n",
    "    ]\n",
    "    \n",
    "    # First, let's check which collections are available\n",
    "    collections = [collection]  # Using full collection name\n",
    "    try:\n",
    "        # We could add an API call here to check available collections\n",
    "        # For now, let's assume search_console might be available\n",
    "        collections.append(\"search_console\")\n",
    "    except Exception as e:\n",
    "        print(f\"Search Console data not available: {e}\")\n",
    "    \n",
    "    data_payload = {\n",
    "        \"collections\": collections,\n",
    "        \"query\": {\n",
    "            \"dimensions\": base_dimensions + optional_dimensions,\n",
    "            \"metrics\": optional_metrics if \"search_console\" in collections else [],\n",
    "            \"filters\": {\n",
    "                \"field\": f\"{collection}.depth\",\n",
    "                \"predicate\": \"lte\",\n",
    "                \"value\": 2\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": \"search_console.period_0.count_impressions\" if \"search_console\" in collections else f\"{collection}.depth\",\n",
    "                    \"order\": \"desc\" if \"search_console\" in collections else \"asc\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"periods\": [[period_start, period_end]] if \"search_console\" in collections else None\n",
    "    }\n",
    "\n",
    "    print(f\"Query payload: {json.dumps(data_payload, indent=2)}\")\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error response: {response.text}\")\n",
    "        response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Define all possible columns\n",
    "    all_columns = ['url', 'depth', 'pagetype', 'compliant', 'reason', 'canonical', \n",
    "                  'sitemap', 'js_exec', 'js_ok', 'impressions', 'clicks']\n",
    "    \n",
    "    # Create DataFrame with available data\n",
    "    results = []\n",
    "    for item in data['results']:\n",
    "        # Fill missing dimensions/metrics with None\n",
    "        row = item['dimensions']\n",
    "        if 'metrics' in item:\n",
    "            row.extend(item['metrics'])\n",
    "        while len(row) < len(all_columns):\n",
    "            row.append(None)\n",
    "        results.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=all_columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fetch_fields(org: str, project: str, collection: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch available fields for a given collection from the Botify API.\n",
    "    \n",
    "    Args:\n",
    "        org: Organization slug\n",
    "        project: Project slug  \n",
    "        collection: Collection name (e.g. 'crawl.20241108')\n",
    "        \n",
    "    Returns:\n",
    "        List of field IDs available in the collection\n",
    "    \"\"\"\n",
    "    fields_url = f\"https://api.botify.com/v1/projects/{org}/{project}/collections/{collection}\"\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(fields_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        fields_data = response.json()\n",
    "        return [\n",
    "            field['id'] \n",
    "            for dataset in fields_data.get('datasets', [])\n",
    "            for field in dataset.get('fields', [])\n",
    "        ]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching fields for collection '{collection}': {e}\")\n",
    "        return []\n",
    "\n",
    "def check_compliance_fields(org, project, analysis):\n",
    "    \"\"\"Check available compliance fields in a more structured way.\"\"\"\n",
    "    collection = f\"crawl.{analysis}\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # Group compliance fields by category\n",
    "    compliance_categories = {\n",
    "        'Basic Compliance': [\n",
    "            'compliant.is_compliant',\n",
    "            'compliant.main_reason',\n",
    "            'compliant.reason.http_code',\n",
    "            'compliant.reason.content_type',\n",
    "            'compliant.reason.canonical',\n",
    "            'compliant.reason.noindex',\n",
    "            'compliant.detailed_reason'\n",
    "        ],\n",
    "        'Performance': [\n",
    "            'scoring.issues.slow_first_to_last_byte_compliant',\n",
    "            'scoring.issues.slow_render_time_compliant',\n",
    "            'scoring.issues.slow_server_time_compliant',\n",
    "            'scoring.issues.slow_load_time_compliant'\n",
    "        ],\n",
    "        'SEO': [\n",
    "            'scoring.issues.duplicate_query_kvs_compliant'\n",
    "        ],\n",
    "        'Outlinks': [\n",
    "            'outlinks_errors.non_compliant.nb.follow.unique',\n",
    "            'outlinks_errors.non_compliant.nb.follow.total',\n",
    "            'outlinks_errors.non_compliant.urls'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n🔍 Field Availability Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    available_count = 0\n",
    "    total_count = sum(len(fields) for fields in compliance_categories.values())\n",
    "    \n",
    "    available_fields = []\n",
    "    for category, fields in compliance_categories.items():\n",
    "        available_in_category = 0\n",
    "        print(f\"\\n📑 {category}\")\n",
    "        print(\"-\" * 30)\n",
    "        for field in fields:\n",
    "            full_field = f\"{collection}.{field}\"\n",
    "            # Test field availability with a minimal query\n",
    "            test_query = {\n",
    "                \"collections\": [collection],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [full_field],\n",
    "                    \"filters\": {\"field\": f\"{collection}.depth\", \"predicate\": \"eq\", \"value\": 0}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = httpx.post(url, headers=headers, json=test_query, timeout=60)\n",
    "                if response.status_code == 200:\n",
    "                    available_in_category += 1\n",
    "                    available_count += 1\n",
    "                    print(f\"✓ {field.split('.')[-1]}\")\n",
    "                    available_fields.append(field)\n",
    "                else:\n",
    "                    print(f\"× {field.split('.')[-1]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"? {field.split('.')[-1]} (error checking)\")\n",
    "    \n",
    "    coverage = (available_count / total_count) * 100\n",
    "    print(f\"\\n📊 Field Coverage: {coverage:.1f}%\")\n",
    "    return available_fields\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution logic\"\"\"\n",
    "    try:\n",
    "        with open('config.json') as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: config.json file not found\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: config.json is not valid JSON\")\n",
    "        return\n",
    "    \n",
    "    org = config.get('org')\n",
    "    project = config.get('project')\n",
    "    analysis = config.get('analysis')\n",
    "    \n",
    "    if not all([org, project, analysis]):\n",
    "        print(\"Error: Missing required fields in config.json (org, project, analysis)\")\n",
    "        return\n",
    "    \n",
    "    print(\"Previewing data availability...\")\n",
    "    if preview_data(org, project, analysis, depth=2):\n",
    "        print(\"Data preview successful. Proceeding with full export...\")\n",
    "        print(\"Fetching BQLv2 data...\")\n",
    "        df = get_bqlv2_data(org, project, analysis)\n",
    "        print(\"\\nData Preview:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to CSV\n",
    "        Path(\"downloads\").mkdir(parents=True, exist_ok=True)\n",
    "        output_file = f\"downloads/{org}_{project}_{analysis}_metadata.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nData saved to {output_file}\")\n",
    "        \n",
    "        # Use check_compliance_fields\n",
    "        check_compliance_fields(org, project, analysis)\n",
    "    else:\n",
    "        print(\"Data preview failed. Please check configuration and try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Previewing data availability...\n",
    "    \n",
    "    🔍 Sampling data for example/retail-division/20241108\n",
    "    ==================================================\n",
    "    \n",
    "    📊 Data Sample Analysis\n",
    "    ------------------------------\n",
    "    • URL: https://www.example.com/...\n",
    "      └─ Performance: 123,456 impressions, 12,345 clicks\n",
    "    • URL: https://www.example.com/site/retail/seasonal-sale/pcmcat...\n",
    "      └─ Performance: 98,765 impressions, 8,765 clicks\n",
    "    • URL: https://www.example.com/site/misc/daily-deals/pcmcat2480...\n",
    "      └─ Performance: 54,321 impressions, 4,321 clicks\n",
    "    \n",
    "    🎯 Data Quality Check\n",
    "    ------------------------------\n",
    "    ✓ URLs found: 404\n",
    "    ✓ Search metrics: Available\n",
    "    ✓ Depth limit: 2\n",
    "    Data preview successful. Proceeding with full export...\n",
    "    Fetching BQLv2 data...\n",
    "    Query payload: {\n",
    "      \"collections\": [\n",
    "        \"crawl.20241108\",\n",
    "        \"search_console\"\n",
    "      ],\n",
    "      \"query\": {\n",
    "        \"dimensions\": [\n",
    "          \"crawl.20241108.url\",\n",
    "          \"crawl.20241108.depth\",\n",
    "          \"crawl.20241108.segments.pagetype.value\",\n",
    "          \"crawl.20241108.compliant.is_compliant\",\n",
    "          \"crawl.20241108.compliant.main_reason\",\n",
    "          \"crawl.20241108.canonical.to.equal\",\n",
    "          \"crawl.20241108.sitemaps.present\",\n",
    "          \"crawl.20241108.js.rendering.exec\",\n",
    "          \"crawl.20241108.js.rendering.ok\"\n",
    "        ],\n",
    "        \"metrics\": [\n",
    "          \"search_console.period_0.count_impressions\",\n",
    "          \"search_console.period_0.count_clicks\"\n",
    "        ],\n",
    "        \"filters\": {\n",
    "          \"field\": \"crawl.20241108.depth\",\n",
    "          \"predicate\": \"lte\",\n",
    "          \"value\": 2\n",
    "        },\n",
    "        \"sort\": [\n",
    "          {\n",
    "            \"field\": \"search_console.period_0.count_impressions\",\n",
    "            \"order\": \"desc\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"periods\": [\n",
    "        [\n",
    "          \"2024-11-01\",\n",
    "          \"2024-11-08\"\n",
    "        ]\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "    Data Preview:\n",
    "                                                                                                                            url  \\\n",
    "    0  https://www.example.com/realm/shops/merchants-quarter/enchanted-items/\n",
    "    1  https://www.example.com/realm/elven-moonlight-potion-azure/\n",
    "    2  https://www.example.com/realm/dwarven-decorative-runes-sapphire/   \n",
    "    3  https://www.example.com/realm/legendary-artifacts/master-crafted-items/   \n",
    "    4  ttps://www.example.com/realm/orcish-war-drums-obsidian/    \n",
    "    \n",
    "       depth       pagetype  compliant     reason canonical  sitemap  js_exec  \\\n",
    "    0      0           home       True  Indexable      True    False     True   \n",
    "    1      2            pdp       True  Indexable      True    False     True   \n",
    "    2      1            plp       True  Indexable      True    False     True   \n",
    "    3      1       category       True  Indexable      True    False     True   \n",
    "    4      1           main       True  Indexable      True    False     True   \n",
    "    \n",
    "       js_ok  impressions  clicks  \n",
    "    0  False       123456   12345  \n",
    "    1  False        98765    8765  \n",
    "    2  False        54321    4321  \n",
    "    3  False        11111    1111  \n",
    "    4  False        12345    1234  \n",
    "    \n",
    "    Data saved to downloads/example_retail-division_20241108_metadata.csv\n",
    "\n",
    "**Rationale**: Just because you happen to work at an enterprise SEO company and possess this peculiar intersection of skills—like crafting prompts that give LLMs instant deep-knowledge (think Neo suddenly knowing kung fu)—doesn't mean you actually understand BQL. In fact, needing to write this prompt rather proves the opposite... wait, did I just create a paradox? Anyway, there's a very subtle chicken-and-egg problem that this file in general and this example in particular helps address: ***validation of collection fields*** so you can template automations without them being too fragile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "# Color-Code Link-Graphs: How To Download Data to Enhance Website Link-Graph Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Load configuration and API key\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {open('botify_token.txt').read().strip()}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def preview_data(org, project, analysis, depth=1):\n",
    "    \"\"\"Preview data availability before committing to full download\"\"\"\n",
    "    # Get analysis date from the slug (assuming YYYYMMDD format)\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    # Calculate period start (7 days before analysis date)\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"collections\": [\n",
    "            f\"crawl.{analysis}\",\n",
    "            \"search_console\"\n",
    "        ],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\"\n",
    "            ],\n",
    "            \"metrics\": [\n",
    "                \"search_console.period_0.count_impressions\",\n",
    "                \"search_console.period_0.count_clicks\"\n",
    "            ],\n",
    "            \"filters\": {\n",
    "                \"field\": f\"crawl.{analysis}.depth\",\n",
    "                \"predicate\": \"lte\",\n",
    "                \"value\": depth\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": \"search_console.period_0.count_impressions\",\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"periods\": [\n",
    "            [\n",
    "                period_start,\n",
    "                period_end\n",
    "            ]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(f\"\\n🔍 Sampling data for {org}/{project}/{analysis}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ Preview failed:\", response.status_code)\n",
    "        return False\n",
    "        \n",
    "    data = response.json()\n",
    "    if not data.get('results'):\n",
    "        print(\"⚠️  No preview data available\")\n",
    "        return False\n",
    "        \n",
    "    print(\"\\n📊 Data Sample Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    metrics_found = 0\n",
    "    for result in data['results'][:3]:  # Show just top 3 for cleaner output\n",
    "        url = result['dimensions'][0]\n",
    "        impressions = result['metrics'][0]\n",
    "        clicks = result['metrics'][1]\n",
    "        metrics_found += bool(impressions or clicks)\n",
    "        print(f\"• URL: {url[:60]}...\")\n",
    "        print(f\"  └─ Performance: {impressions:,} impressions, {clicks:,} clicks\")\n",
    "    \n",
    "    print(\"\\n🎯 Data Quality Check\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"✓ URLs found: {len(data['results'])}\")\n",
    "    print(f\"✓ Search metrics: {'Available' if metrics_found else 'Not found'}\")\n",
    "    print(f\"✓ Depth limit: {depth}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis):\n",
    "    \"\"\"Fetch BQLv2 data using jobs endpoint\"\"\"\n",
    "    # Calculate periods\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = \"https://api.botify.com/v1/jobs\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"job_type\": \"export\",\n",
    "        \"payload\": {\n",
    "            \"username\": org,\n",
    "            \"project\": project,\n",
    "            \"connector\": \"direct_download\",\n",
    "            \"formatter\": \"csv\",\n",
    "            \"export_size\": 1000000,\n",
    "            \"query\": {\n",
    "                \"collections\": [\n",
    "                    f\"crawl.{analysis}\",\n",
    "                    \"search_console\"\n",
    "                ],\n",
    "                \"periods\": [[period_start, period_end]],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [\n",
    "                        f\"crawl.{analysis}.url\", \n",
    "                        f\"crawl.{analysis}.depth\",\n",
    "                        f\"crawl.{analysis}.segments.pagetype.value\",\n",
    "                        f\"crawl.{analysis}.compliant.is_compliant\",\n",
    "                        f\"crawl.{analysis}.compliant.main_reason\",\n",
    "                        f\"crawl.{analysis}.canonical.to.equal\",\n",
    "                        f\"crawl.{analysis}.sitemaps.present\",\n",
    "                        f\"crawl.{analysis}.js.rendering.exec\",\n",
    "                        f\"crawl.{analysis}.js.rendering.ok\"\n",
    "                    ],\n",
    "                    \"metrics\": [\n",
    "                        \"search_console.period_0.count_impressions\",\n",
    "                        \"search_console.period_0.count_clicks\"\n",
    "                    ],\n",
    "                    \"filters\": {\n",
    "                        \"field\": f\"crawl.{analysis}.depth\",\n",
    "                        \"predicate\": \"lte\",\n",
    "                        \"value\": 2\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\\nStarting export job...\")\n",
    "    response = httpx.post(url, json=data_payload, headers=headers)\n",
    "    job_data = response.json()\n",
    "    job_url = f\"https://api.botify.com{job_data['job_url']}\"\n",
    "    print(f\"Job created successfully (ID: {job_data['job_id']})\")\n",
    "    \n",
    "    print(\"\\nPolling for job completion: \", end=\"\", flush=True)\n",
    "    while True:\n",
    "        time.sleep(5)  # Poll every 5 seconds\n",
    "        status = httpx.get(job_url, headers=headers).json()\n",
    "        print(f\"\\nCurrent status: {status['job_status']}\")\n",
    "        \n",
    "        if status['job_status'] in ['COMPLETE', 'DONE']:\n",
    "            download_url = status['results']['download_url']\n",
    "            print(f\"\\nDownload URL: {download_url}\")\n",
    "            \n",
    "            # Download and process the file\n",
    "            gz_filename = \"export.csv.gz\"\n",
    "            csv_filename = \"export.csv\"\n",
    "            \n",
    "            # Download gzipped file\n",
    "            response = httpx.get(download_url)\n",
    "            with open(gz_filename, \"wb\") as gz_file:\n",
    "                gz_file.write(response.content)\n",
    "            print(f\"File downloaded as '{gz_filename}'\")\n",
    "            \n",
    "            # Decompress and read into DataFrame\n",
    "            with gzip.open(gz_filename, \"rb\") as f_in:\n",
    "                with open(csv_filename, \"wb\") as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            print(f\"File decompressed as '{csv_filename}'\")\n",
    "            \n",
    "            # Read CSV into DataFrame\n",
    "            df = pd.read_csv(csv_filename, names=[\n",
    "                'url', 'depth', 'pagetype', 'compliant', 'reason', \n",
    "                'canonical', 'sitemap', 'js_exec', 'js_ok',\n",
    "                'impressions', 'clicks'\n",
    "            ])\n",
    "            \n",
    "            # Cleanup temporary files\n",
    "            os.remove(gz_filename)\n",
    "            os.remove(csv_filename)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        elif status['job_status'] == 'FAILED':\n",
    "            print(f\"\\nJob failed. Error details: {status}\")\n",
    "            raise Exception(\"Export job failed\")\n",
    "            \n",
    "        elif status['job_status'] in ['CREATED', 'PROCESSING']:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nUnexpected status: {status}\")\n",
    "            raise Exception(f\"Unexpected job status: {status['job_status']}\")\n",
    "\n",
    "def fetch_fields(org: str, project: str, collection: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch available fields for a given collection from the Botify API.\n",
    "    \n",
    "    Args:\n",
    "        org: Organization slug\n",
    "        project: Project slug  \n",
    "        collection: Collection name (e.g. 'crawl.20241108')\n",
    "        \n",
    "    Returns:\n",
    "        List of field IDs available in the collection\n",
    "    \"\"\"\n",
    "    fields_url = f\"https://api.botify.com/v1/projects/{org}/{project}/collections/{collection}\"\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(fields_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        fields_data = response.json()\n",
    "        return [\n",
    "            field['id'] \n",
    "            for dataset in fields_data.get('datasets', [])\n",
    "            for field in dataset.get('fields', [])\n",
    "        ]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching fields for collection '{collection}': {e}\")\n",
    "        return []\n",
    "\n",
    "def check_compliance_fields(org, project, analysis):\n",
    "    \"\"\"Check available compliance fields in a more structured way.\"\"\"\n",
    "    collection = f\"crawl.{analysis}\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # Group compliance fields by category\n",
    "    compliance_categories = {\n",
    "        'Basic Compliance': [\n",
    "            'compliant.is_compliant',\n",
    "            'compliant.main_reason',\n",
    "            'compliant.reason.http_code',\n",
    "            'compliant.reason.content_type',\n",
    "            'compliant.reason.canonical',\n",
    "            'compliant.reason.noindex',\n",
    "            'compliant.detailed_reason'\n",
    "        ],\n",
    "        'Performance': [\n",
    "            'scoring.issues.slow_first_to_last_byte_compliant',\n",
    "            'scoring.issues.slow_render_time_compliant',\n",
    "            'scoring.issues.slow_server_time_compliant',\n",
    "            'scoring.issues.slow_load_time_compliant'\n",
    "        ],\n",
    "        'SEO': [\n",
    "            'scoring.issues.duplicate_query_kvs_compliant'\n",
    "        ],\n",
    "        'Outlinks': [\n",
    "            'outlinks_errors.non_compliant.nb.follow.unique',\n",
    "            'outlinks_errors.non_compliant.nb.follow.total',\n",
    "            'outlinks_errors.non_compliant.urls'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n🔍 Field Availability Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    available_count = 0\n",
    "    total_count = sum(len(fields) for fields in compliance_categories.values())\n",
    "    \n",
    "    available_fields = []\n",
    "    for category, fields in compliance_categories.items():\n",
    "        available_in_category = 0\n",
    "        print(f\"\\n📑 {category}\")\n",
    "        print(\"-\" * 30)\n",
    "        for field in fields:\n",
    "            full_field = f\"{collection}.{field}\"\n",
    "            # Test field availability with a minimal query\n",
    "            test_query = {\n",
    "                \"collections\": [collection],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [full_field],\n",
    "                    \"filters\": {\"field\": f\"{collection}.depth\", \"predicate\": \"eq\", \"value\": 0}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = httpx.post(url, headers=headers, json=test_query)\n",
    "                if response.status_code == 200:\n",
    "                    available_in_category += 1\n",
    "                    available_count += 1\n",
    "                    print(f\"✓ {field.split('.')[-1]}\")\n",
    "                    available_fields.append(field)\n",
    "                else:\n",
    "                    print(f\"× {field.split('.')[-1]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"? {field.split('.')[-1]} (error checking)\")\n",
    "    \n",
    "    coverage = (available_count / total_count) * 100\n",
    "    print(f\"\\n📊 Field Coverage: {coverage:.1f}%\")\n",
    "    return available_fields\n",
    "\n",
    "def download_and_process_csv(download_url, output_filename):\n",
    "    \"\"\"Download and decompress CSV from Botify API.\"\"\"\n",
    "    gz_filename = f\"{output_filename}.gz\"\n",
    "    \n",
    "    # Download gzipped file\n",
    "    response = httpx.get(download_url)\n",
    "    with open(gz_filename, \"wb\") as gz_file:\n",
    "        gz_file.write(response.content)\n",
    "    print(f\"Downloaded: {gz_filename}\")\n",
    "    \n",
    "    # Decompress to CSV\n",
    "    with gzip.open(gz_filename, \"rb\") as f_in:\n",
    "        with open(output_filename, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(f\"Decompressed to: {output_filename}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(gz_filename)\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution logic\"\"\"\n",
    "    try:\n",
    "        with open('config.json') as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: config.json file not found\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: config.json is not valid JSON\")\n",
    "        return\n",
    "    \n",
    "    org = config.get('org')\n",
    "    project = config.get('project')\n",
    "    analysis = config.get('analysis')\n",
    "    \n",
    "    if not all([org, project, analysis]):\n",
    "        print(\"Error: Missing required fields in config.json (org, project, analysis)\")\n",
    "        return\n",
    "    \n",
    "    print(\"Previewing data availability...\")\n",
    "    if preview_data(org, project, analysis, depth=2):\n",
    "        print(\"Data preview successful. Proceeding with full export...\")\n",
    "        print(\"Fetching BQLv2 data...\")\n",
    "        df = get_bqlv2_data(org, project, analysis)\n",
    "        print(\"\\nData Preview:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to CSV\n",
    "        Path(\"downloads\").mkdir(parents=True, exist_ok=True)\n",
    "        output_file = f\"downloads/{org}_{project}_{analysis}_metadata.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nData saved to {output_file}\")\n",
    "        \n",
    "        # Use check_compliance_fields\n",
    "        check_compliance_fields(org, project, analysis)\n",
    "    else:\n",
    "        print(\"Data preview failed. Please check configuration and try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Previewing data availability...\n",
    "    \n",
    "    🔍 Sampling data for example/retail-division/20241108\n",
    "    ==================================================\n",
    "    \n",
    "    📊 Data Sample Analysis\n",
    "    ------------------------------\n",
    "    • URL: https://www.example.com/...\n",
    "      └─ Performance: 123,456 impressions, 12,345 clicks\n",
    "    • URL: https://www.example.com/site/retail/seasonal-sale/pcmcat...\n",
    "      └─ Performance: 98,765 impressions, 8,765 clicks\n",
    "    • URL: https://www.example.com/site/misc/daily-deals/pcmcat2480...\n",
    "      └─ Performance: 54,321 impressions, 4,321 clicks\n",
    "    \n",
    "    🎯 Data Quality Check\n",
    "    ------------------------------\n",
    "    ✓ URLs found: 404\n",
    "    ✓ Search metrics: Available\n",
    "    ✓ Depth limit: 2\n",
    "    Data preview successful. Proceeding with full export...\n",
    "    Fetching BQLv2 data...\n",
    "    \n",
    "    Starting export job...\n",
    "    Job created successfully (ID: 12345)\n",
    "    \n",
    "    Polling for job completion: \n",
    "    Current status: CREATED\n",
    "    .\n",
    "    Current status: PROCESSING\n",
    "    .\n",
    "    Current status: DONE\n",
    "    \n",
    "    Download URL: https://example.cloudfront.net/exports/a/b/c/abc123def456/example-2024-11-10.csv.gz\n",
    "    File downloaded as 'export.csv.gz'\n",
    "    File decompressed as 'export.csv'\n",
    "    \n",
    "    Data Preview:\n",
    "                                                                                                                            url  \\\n",
    "    0  https://www.example.com/realm/shops/merchants-quarter/enchanted-items/\n",
    "    1  https://www.example.com/realm/elven-moonlight-potion-azure/\n",
    "    2  https://www.example.com/realm/dwarven-decorative-runes-sapphire/   \n",
    "    3  https://www.example.com/realm/legendary-artifacts/master-crafted-items/   \n",
    "    4  ttps://www.example.com/realm/orcish-war-drums-obsidian/  \n",
    "    \n",
    "       depth             pagetype  compliant     reason canonical  sitemap  \\\n",
    "    0      2             category       True  Indexable      True    False   \n",
    "    1      2                  pdp       True  Indexable      True     True   \n",
    "    2      2                  plp       True  Indexable      True     True   \n",
    "    3      2               review       True  Indexable      True    False   \n",
    "    4      2                 main       True  Indexable      True     True   \n",
    "    \n",
    "       js_exec  js_ok  impressions  clicks  \n",
    "    0     True  False       12345     123   \n",
    "    1     True  False        9876      98   \n",
    "    2     True  False        5432      54   \n",
    "    3     True  False        1111      11   \n",
    "    4     True  False         987       9   \n",
    "    \n",
    "    Data saved to downloads/example_retail-division_20241108_metadata.csv\n",
    "\n",
    "**Rationale**: BQL and enterprise SEO diagnostics provide powerful tools for comprehensive website analysis. This code transforms link structures into detailed visualizations of your site's SEO health, presenting search signals and performance metrics in a multi-dimensional view. Similar to how diagnostic imaging reveals underlying conditions, these analyses expose strengths, vulnerabilities, and technical issues within your site architecture. Prepare your site for the increasing number of AI crawlers by optimizing your content and structure. Ensure your schema.org structured data is properly implemented to support real-time crawls that evaluate product availability and other critical information.\n",
    " \n",
    "Next-generation SEO requires adapting to these AI-driven changes. Now, let's examine the process of converting from BQLv1 to the collection-based BQLv2 format..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "# Web Logs: How To Check If A Project Has a Web Logs Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import os\n",
    "# No typing imports needed\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG_FILE = \"config.json\"\n",
    "TOKEN_FILE = \"botify_token.txt\"\n",
    "# The specific collection ID we are looking for\n",
    "TARGET_LOG_COLLECTION_ID = \"logs\"\n",
    "\n",
    "# --- !!! EASY OVERRIDE SECTION !!! ---\n",
    "# Set these variables to directly specify org/project, bypassing config.json\n",
    "# Leave as None to use config.json or prompts.\n",
    "# https://app.botify.com/michaellevin-org/mikelev.in\n",
    "# ORG_OVERRIDE = None\n",
    "# PROJECT_OVERRIDE = None\n",
    "ORG_OVERRIDE = \"michaellevin-org\"\n",
    "PROJECT_OVERRIDE = \"mikelev.in\"\n",
    "# Example:\n",
    "# ORG_OVERRIDE = \"my-direct-org\"\n",
    "# PROJECT_OVERRIDE = \"my-direct-project.com\"\n",
    "# --- END OVERRIDE SECTION ---\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_api_key():\n",
    "    \"\"\"Loads the API key from the token file. Returns None on error.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(TOKEN_FILE):\n",
    "            print(f\"Error: Token file '{TOKEN_FILE}' not found.\")\n",
    "            return None\n",
    "        with open(TOKEN_FILE) as f:\n",
    "            api_key = f.read().strip()\n",
    "            if not api_key:\n",
    "                print(f\"Error: Token file '{TOKEN_FILE}' is empty.\")\n",
    "                return None\n",
    "        return api_key\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading API key: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_org_project_from_config():\n",
    "    \"\"\"Loads org and project from config file. Returns (None, None) on error or if missing.\"\"\"\n",
    "    org_config = None\n",
    "    project_config = None\n",
    "    try:\n",
    "        if os.path.exists(CONFIG_FILE):\n",
    "            with open(CONFIG_FILE) as f:\n",
    "                config_data = json.load(f)\n",
    "                org_config = config_data.get('org')\n",
    "                project_config = config_data.get('project')\n",
    "        # No error if file doesn't exist, just return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: '{CONFIG_FILE}' contains invalid JSON.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load org/project from {CONFIG_FILE}: {e}\")\n",
    "    return org_config, project_config\n",
    "\n",
    "def get_api_headers(api_key):\n",
    "    \"\"\"Returns standard API headers.\"\"\"\n",
    "    return {\n",
    "        \"Authorization\": f\"Token {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "def check_if_log_collection_exists(org_slug, project_slug, api_key):\n",
    "    \"\"\"\n",
    "    Checks if a collection with ID 'logs' exists for the given org and project.\n",
    "    Returns True if found, False otherwise or on error.\n",
    "    \"\"\"\n",
    "    if not org_slug or not project_slug or not api_key:\n",
    "        print(\"Error: Org slug, project slug, and API key are required for check.\")\n",
    "        return False\n",
    "\n",
    "    collections_url = f\"https://api.botify.com/v1/projects/{org_slug}/{project_slug}/collections\"\n",
    "    headers = get_api_headers(api_key)\n",
    "\n",
    "    print(f\"\\nChecking for collection '{TARGET_LOG_COLLECTION_ID}' in {org_slug}/{project_slug}...\")\n",
    "\n",
    "    try:\n",
    "        response = httpx.get(collections_url, headers=headers, timeout=60.0)\n",
    "\n",
    "        if response.status_code == 401:\n",
    "            print(\"Error: Authentication failed (401). Check your API token.\")\n",
    "            return False\n",
    "        elif response.status_code == 403:\n",
    "             print(\"Error: Forbidden (403). You may not have access to this project or endpoint.\")\n",
    "             return False\n",
    "        elif response.status_code == 404:\n",
    "             print(\"Error: Project not found (404). Check org/project slugs.\")\n",
    "             return False\n",
    "\n",
    "        response.raise_for_status() # Raise errors for other bad statuses (like 5xx)\n",
    "        collections_data = response.json()\n",
    "\n",
    "        if not isinstance(collections_data, list):\n",
    "             print(f\"Error: Unexpected API response format. Expected a list.\")\n",
    "             return False\n",
    "\n",
    "        for collection in collections_data:\n",
    "            if isinstance(collection, dict) and collection.get('id') == TARGET_LOG_COLLECTION_ID:\n",
    "                print(f\"Success: Found collection with ID '{TARGET_LOG_COLLECTION_ID}'.\")\n",
    "                return True\n",
    "\n",
    "        print(f\"Result: Collection with ID '{TARGET_LOG_COLLECTION_ID}' was not found in the list.\")\n",
    "        return False\n",
    "\n",
    "    except httpx.HTTPStatusError as e:\n",
    "         print(f\"API Error checking collections: {e.response.status_code}\")\n",
    "         return False\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Network error checking collections: {e}\")\n",
    "        return False\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Could not decode the API response as JSON.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during check: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Main Function ---\n",
    "\n",
    "def run_check(org_override=None, project_override=None):\n",
    "    \"\"\"\n",
    "    Orchestrates the check for the 'logs' collection.\n",
    "    Uses override values if provided, otherwise falls back to config file, then prompts.\n",
    "    \"\"\"\n",
    "    print(\"Starting Botify Log Collection Check...\")\n",
    "\n",
    "    # 1. Load API Key (Essential)\n",
    "    api_key = load_api_key()\n",
    "    if not api_key:\n",
    "        print(\"Cannot proceed without a valid API key.\")\n",
    "        return # Exit the function\n",
    "\n",
    "    # 2. Determine Org and Project to use\n",
    "    org_config, project_config = load_org_project_from_config()\n",
    "\n",
    "    # Apply overrides if they exist\n",
    "    org_to_use = org_override if org_override is not None else org_config\n",
    "    project_to_use = project_override if project_override is not None else project_config\n",
    "\n",
    "    # If still missing after config and overrides, prompt the user\n",
    "    if not org_to_use:\n",
    "        print(f\"Organization slug not found in config or override.\")\n",
    "        org_to_use = input(\"Enter the organization slug: \").strip()\n",
    "\n",
    "    if not project_to_use:\n",
    "        print(f\"Project slug not found in config or override.\")\n",
    "        project_to_use = input(\"Enter the project slug: \").strip()\n",
    "\n",
    "    # Final check before running API call\n",
    "    if not org_to_use or not project_to_use:\n",
    "        print(\"Organization and Project slugs are required to run the check. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 3. Run the core check function\n",
    "    has_logs = check_if_log_collection_exists(org_to_use, project_to_use, api_key)\n",
    "\n",
    "    # 4. Report Final Result\n",
    "    print(\"\\n--- Check Complete ---\")\n",
    "    if has_logs:\n",
    "        print(f\"The project '{org_to_use}/{project_to_use}' appears to HAVE a '{TARGET_LOG_COLLECTION_ID}' collection available.\")\n",
    "    else:\n",
    "        print(f\"The project '{org_to_use}/{project_to_use}' does NOT appear to have a '{TARGET_LOG_COLLECTION_ID}' collection available (or an error occurred).\")\n",
    "\n",
    "\n",
    "# --- Script Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the main function, passing the override values defined at the top\n",
    "    run_check(org_override=ORG_OVERRIDE, project_override=PROJECT_OVERRIDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Starting Botify Log Collection Check...\n",
    "    \n",
    "    Checking for collection 'logs' in example-org/example.com...\n",
    "    Success: Found collection with ID 'logs'.\n",
    "    \n",
    "    --- Check Complete ---\n",
    "    The project 'example-org/example.com' appears to HAVE a 'logs' collection available.\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "    Starting Botify Log Collection Check...\n",
    "    \n",
    "    Checking for collection 'logs' in example-org/example.com...\n",
    "    Result: Collection with ID 'logs' was not found in the list.\n",
    "    \n",
    "    --- Check Complete ---\n",
    "    The project 'example-org/example.com' does NOT appear to have a 'logs' collection available (or an error occurred)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "# Migrating from BQLv1 to BQLv2\n",
    "\n",
    "This guide explains how to convert BQLv1 queries to BQLv2 format, with practical examples and validation helpers.\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### API Endpoint Changes\n",
    "\n",
    "- **BQLv1**: `/v1/analyses/{username}/{website}/{analysis}/urls`\n",
    "- **BQLv2**: `/v1/projects/{username}/{website}/query`\n",
    "\n",
    "### Key Structural Changes\n",
    "\n",
    "1. Collections replace URL parameters\n",
    "2. Fields become dimensions\n",
    "3. All fields require collection prefixes\n",
    "4. Areas are replaced with explicit filters\n",
    "\n",
    "## Query Conversion Examples\n",
    "\n",
    "### 1. Basic URL Query\n",
    "\n",
    "```json\n",
    "// BQLv1 (/v1/analyses/user/site/20210801/urls?area=current&previous_crawl=20210715)\n",
    "{\n",
    "  \"fields\": [\n",
    "    \"url\",\n",
    "    \"http_code\",\n",
    "    \"previous.http_code\"\n",
    "  ],\n",
    "  \"filters\": {\n",
    "    \"field\": \"indexable.is_indexable\",\n",
    "    \"predicate\": \"eq\",\n",
    "    \"value\": true\n",
    "  }\n",
    "}\n",
    "\n",
    "// BQLv2 (/v1/projects/user/site/query)\n",
    "{\n",
    "  \"collections\": [\n",
    "    \"crawl.20210801\",\n",
    "    \"crawl.20210715\"\n",
    "  ],\n",
    "  \"query\": {\n",
    "    \"dimensions\": [\n",
    "      \"crawl.20210801.url\",\n",
    "      \"crawl.20210801.http_code\",\n",
    "      \"crawl.20210715.http_code\"\n",
    "    ],\n",
    "    \"metrics\": [],\n",
    "    \"filters\": {\n",
    "      \"field\": \"crawl.20210801.indexable.is_indexable\",\n",
    "      \"predicate\": \"eq\",\n",
    "      \"value\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. Aggregation Query\n",
    "\n",
    "```json\n",
    "// BQLv1 (/v1/analyses/user/site/20210801/urls/aggs)\n",
    "[\n",
    "  {\n",
    "    \"aggs\": [\n",
    "      {\n",
    "        \"metrics\": [\"count\"],\n",
    "        \"group_by\": [\n",
    "          {\n",
    "            \"distinct\": {\n",
    "              \"field\": \"segments.pagetype.depth_1\",\n",
    "              \"order\": {\"value\": \"asc\"},\n",
    "              \"size\": 300\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "// BQLv2\n",
    "{\n",
    "  \"collections\": [\"crawl.20210801\"],\n",
    "  \"query\": {\n",
    "    \"dimensions\": [\n",
    "      \"crawl.20210801.segments.pagetype.depth_1\"\n",
    "    ],\n",
    "    \"metrics\": [\n",
    "      \"crawl.20210801.count_urls_crawl\"\n",
    "    ],\n",
    "    \"sort\": [0]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Area Filters\n",
    "\n",
    "BQLv1's area parameter is replaced with explicit filters in BQLv2:\n",
    "\n",
    "#### New URLs Filter\n",
    "```json\n",
    "{\n",
    "  \"and\": [\n",
    "    {\n",
    "      \"field\": \"crawl.20210801.url_exists_crawl\",\n",
    "      \"value\": true\n",
    "    },\n",
    "    {\n",
    "      \"field\": \"crawl.20210715.url_exists_crawl\",\n",
    "      \"value\": false\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Disappeared URLs Filter\n",
    "```json\n",
    "{\n",
    "  \"and\": [\n",
    "    {\n",
    "      \"field\": \"crawl.20210801.url_exists_crawl\",\n",
    "      \"value\": false\n",
    "    },\n",
    "    {\n",
    "      \"field\": \"crawl.20210715.url_exists_crawl\",\n",
    "      \"value\": true\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Conversion Helper Functions\n",
    "\n",
    "```python\n",
    "def validate_bql_v2(query):\n",
    "    \"\"\"Validate BQLv2 query structure\"\"\"\n",
    "    required_keys = {'collections', 'query'}\n",
    "    query_keys = {'dimensions', 'metrics', 'filters'}\n",
    "    \n",
    "    if not all(key in query for key in required_keys):\n",
    "        raise ValueError(f\"Missing required keys: {required_keys}\")\n",
    "    if not any(key in query['query'] for key in query_keys):\n",
    "        raise ValueError(f\"Query must contain one of: {query_keys}\")\n",
    "    for collection in query['collections']:\n",
    "        if not collection.startswith('crawl.'):\n",
    "            raise ValueError(f\"Invalid collection format: {collection}\")\n",
    "    return True\n",
    "\n",
    "def convert_url_query(query_v1, current_analysis, previous_analysis=None):\n",
    "    \"\"\"Convert BQLv1 URL query to BQLv2\"\"\"\n",
    "    collections = [f\"crawl.{current_analysis}\"]\n",
    "    if previous_analysis:\n",
    "        collections.append(f\"crawl.{previous_analysis}\")\n",
    "    \n",
    "    # Convert fields to dimensions\n",
    "    dimensions = []\n",
    "    for field in query_v1.get('fields', []):\n",
    "        if field.startswith('previous.'):\n",
    "            if not previous_analysis:\n",
    "                raise ValueError(\"Previous analysis required for previous fields\")\n",
    "            field = field.replace('previous.', '')\n",
    "            dimensions.append(f\"crawl.{previous_analysis}.{field}\")\n",
    "        else:\n",
    "            dimensions.append(f\"crawl.{current_analysis}.{field}\")\n",
    "    \n",
    "    # Convert filters\n",
    "    filters = None\n",
    "    if 'filters' in query_v1:\n",
    "        filters = {\n",
    "            \"field\": f\"crawl.{current_analysis}.{query_v1['filters']['field']}\",\n",
    "            \"predicate\": query_v1['filters']['predicate'],\n",
    "            \"value\": query_v1['filters']['value']\n",
    "        }\n",
    "    \n",
    "    query_v2 = {\n",
    "        \"collections\": collections,\n",
    "        \"query\": {\n",
    "            \"dimensions\": dimensions,\n",
    "            \"metrics\": [],\n",
    "            \"filters\": filters\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    validate_bql_v2(query_v2)\n",
    "    return query_v2\n",
    "```\n",
    "\n",
    "## Key Conversion Rules\n",
    "\n",
    "1. **Collections**\n",
    "   - Add `collections` array with `crawl.{analysis}` format\n",
    "   - Include both analyses for comparison queries\n",
    "\n",
    "2. **Fields to Dimensions**\n",
    "   - Prefix fields with `crawl.{analysis}.`\n",
    "   - Replace `previous.` prefix with `crawl.{previous_analysis}.`\n",
    "\n",
    "3. **Metrics**\n",
    "   - Convert aggregation metrics to appropriate BQLv2 metric fields\n",
    "   - Use empty array when no metrics needed\n",
    "\n",
    "4. **Filters**\n",
    "   - Prefix filter fields with collection name\n",
    "   - Replace area parameters with explicit URL existence filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "# All Botify API Endpoints: How Do You Generate a Python Code Example for Every Botify Endpoint Given Their OpenAPI Swagger\n",
    "\n",
    "Botify OpenAPI Swagger File: [https://api.botify.com/v1/swagger.json](https://api.botify.com/v1/swagger.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything From Botify OpenAPI Swagger File: https://api.botify.com/v1/swagger.json\n",
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "def generate_python_example(method: str, path: str, params: Dict, config: Dict, description: str = \"\", show_config: bool = False) -> str:\n",
    "    \"\"\"Craft a Python invocation example for a given API endpoint\"\"\"\n",
    "    docstring = f'    \"\"\"{description}\"\"\"\\n' if description else \"\"\n",
    "    \n",
    "    lines = [\n",
    "        \"```python\",\n",
    "        \"# Summon the necessary artifacts\",\n",
    "        \"import httpx\",\n",
    "        \"import json\",\n",
    "        \"\",\n",
    "        \"def example_request():\",\n",
    "        \"\\n\",\n",
    "        docstring\n",
    "    ]\n",
    "    \n",
    "    if show_config:\n",
    "        lines.extend([\n",
    "            \"# Your configuration sigil should contain:\",\n",
    "            \"#   - token: Your API token\",\n",
    "            \"#   - org: Your organization ID\",\n",
    "            \"#   - project: Your project ID\",\n",
    "            \"#   - analysis: Your analysis ID\",\n",
    "            \"#   - collection: Your collection ID\",\n",
    "            \"\",\n",
    "            \"# Load token from secure storage\",\n",
    "            'with open(\"botify_token.txt\") as f:',\n",
    "            '    token = f.read().strip()',\n",
    "            ''\n",
    "        ])\n",
    "    \n",
    "    # Format the URL and parameters\n",
    "    url = f\"url = f'https://api.botify.com/v1{path}'\"\n",
    "    lines.extend([\n",
    "        \"# Craft the invocation URL\",\n",
    "        url,\n",
    "        \"\",\n",
    "        \"# Prepare the headers for your spell\",\n",
    "        'headers = {',\n",
    "        '    \"Authorization\": f\"Token {token}\",',\n",
    "        '    \"Content-Type\": \"application/json\"',\n",
    "        '}',\n",
    "        ''\n",
    "    ])\n",
    "    \n",
    "    # Add method-specific code\n",
    "    if method.lower() in ['post', 'put', 'patch']:\n",
    "        lines.extend([\n",
    "            \"# Define the payload for your invocation\",\n",
    "            \"data = {\",\n",
    "            '    # Add your request parameters here',\n",
    "            \"}\",\n",
    "            \"\",\n",
    "            \"# Cast the spell\",\n",
    "            f\"response = httpx.{method.lower()}(url, headers=headers, json=data)\",\n",
    "            \"\"\n",
    "        ])\n",
    "    else:\n",
    "        lines.extend([\n",
    "            \"# Cast the spell\",\n",
    "            f\"response = httpx.{method.lower()}(url, headers=headers)\",\n",
    "            \"\"\n",
    "        ])\n",
    "    \n",
    "    lines.extend([\n",
    "        \"# Interpret the response\",\n",
    "        \"if response.status_code == 200:\",\n",
    "        \"    result = response.json()\",\n",
    "        \"    print(json.dumps(result, indent=2))\",\n",
    "        \"else:\",\n",
    "        \"    print(f'Error: {response.status_code}')\",\n",
    "        \"    print(response.text)\"\n",
    "    ])\n",
    "    \n",
    "    lines.append(\"```\")\n",
    "    return \"\\n\".join(line for line in lines if line)\n",
    "\n",
    "def generate_markdown(spec: Dict[str, Any], config: Dict[str, str]) -> str:\n",
    "    md_lines = [\n",
    "        \"## Everything From Botify OpenAPI Swagger File: https://api.botify.com/v1/swagger.json\",\n",
    "        \"\",\n",
    "        \"Having mastered the arts of BQL, we now document the full spectrum of API invocations.\",\n",
    "        \"Each endpoint is presented with its purpose, capabilities, and Python implementation.\",\n",
    "        \"\",\n",
    "        \"### Endpoint Categories\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    endpoints_by_tag = {}\n",
    "    for path, methods in spec['paths'].items():\n",
    "        for method, details in methods.items():\n",
    "            if method == 'parameters':\n",
    "                continue\n",
    "            tag = details.get('tags', ['Untagged'])[0]\n",
    "            if tag not in endpoints_by_tag:\n",
    "                endpoints_by_tag[tag] = []\n",
    "            endpoints_by_tag[tag].append((method, path, details))\n",
    "    \n",
    "    first_example = True\n",
    "    for tag in sorted(endpoints_by_tag.keys()):\n",
    "        md_lines.extend([\n",
    "            f\"#### {tag} Invocations\",\n",
    "            \"\",\n",
    "            f\"These endpoints allow you to manipulate {tag.lower()} aspects of your digital realm.\",\n",
    "            \"\"\n",
    "        ])\n",
    "        \n",
    "        for method, path, details in sorted(endpoints_by_tag[tag]):\n",
    "            description = details.get('description', '')\n",
    "            summary = details.get('summary', '')\n",
    "            parameters = details.get('parameters', [])\n",
    "            responses = details.get('responses', {})\n",
    "            \n",
    "            # Create a semantic block for LLMs\n",
    "            md_lines.extend([\n",
    "                f\"##### {method.upper()} {path}\",\n",
    "                \"\",\n",
    "                \"**Purpose:**\",\n",
    "                summary or \"No summary provided.\",\n",
    "                \"\",\n",
    "                \"**Detailed Description:**\",\n",
    "                description or \"No detailed description available.\",\n",
    "                \"\",\n",
    "                \"**Parameters Required:**\",\n",
    "                \"```\",\n",
    "                \"\\n\".join(f\"- {p.get('name')}: {p.get('description', 'No description')}\" \n",
    "                         for p in parameters) if parameters else \"No parameters required\",\n",
    "                \"```\",\n",
    "                \"\",\n",
    "                \"**Expected Responses:**\",\n",
    "                \"```\",\n",
    "                \"\\n\".join(f\"- {code}: {details.get('description', 'No description')}\" \n",
    "                         for code, details in responses.items()),\n",
    "                \"```\",\n",
    "                \"\",\n",
    "                \"**Example Implementation:**\",\n",
    "                generate_python_example(method, path, details, config, \n",
    "                                     description=description, show_config=first_example),\n",
    "                \"\",\n",
    "                \"---\",\n",
    "                \"\"\n",
    "            ])\n",
    "            first_example = False\n",
    "    \n",
    "    # Add a section specifically for LLM understanding\n",
    "    md_lines.extend([\n",
    "        \"### LLM Guidance\",\n",
    "        \"\",\n",
    "        \"When deciding which endpoint to use:\",\n",
    "        \"1. Consider the category (tag) that matches your task\",\n",
    "        \"2. Review the Purpose and Description to ensure alignment\",\n",
    "        \"3. Check the required parameters match your available data\",\n",
    "        \"4. Verify the expected responses meet your needs\",\n",
    "        \"\",\n",
    "        \"Example prompt format:\",\n",
    "        '```',\n",
    "        'I need to [task description]. I have access to [available data].',\n",
    "        'Which endpoint would be most appropriate?',\n",
    "        '```',\n",
    "        \"\"\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(md_lines)\n",
    "\n",
    "# First, ensure we have our token\n",
    "if not Path(\"botify_token.txt\").exists():\n",
    "    print(\"Please run the authentication cell first to create your token file.\")\n",
    "else:\n",
    "    # Load token\n",
    "    with open(\"botify_token.txt\") as f:\n",
    "        token = f.read().strip()\n",
    "    \n",
    "    # Use existing configuration from earlier cells\n",
    "    config = {\n",
    "        \"token\": token,\n",
    "        # These will be used as placeholders in examples\n",
    "        \"org\": \"{org_id}\",\n",
    "        \"project\": \"{project_id}\",\n",
    "        \"analysis\": \"{analysis_id}\",\n",
    "        \"collection\": \"{collection_id}\"\n",
    "    }\n",
    "    \n",
    "    # Fetch the API specification\n",
    "    try:\n",
    "        response = httpx.get(\"https://api.botify.com/v1/swagger.json\", \n",
    "                              headers={\"Authorization\": f\"Token {token}\"})\n",
    "        spec = response.json()\n",
    "        \n",
    "        # Generate and display the markdown\n",
    "        markdown_content = generate_markdown(spec, config)\n",
    "        print(\"API documentation generated successfully!\")\n",
    "        \n",
    "        # The markdown content will be rendered in the next cell\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching API specification: {e}\")\n",
    "print(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "API documentation generated successfully!\n",
    "## Everything From Botify OpenAPI Swagger File: https://api.botify.com/v1/swagger.json\n",
    "\n",
    "Having mastered the arts of BQL, we now document the full spectrum of API invocations.\n",
    "Each endpoint is presented with its purpose, capabilities, and Python implementation.\n",
    "\n",
    "### Endpoint Categories\n",
    "\n",
    "#### Analysis Invocations\n",
    "\n",
    "These endpoints allow you to manipulate analysis aspects of your digital realm.\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Get an Analysis detail\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- previous_crawl: Previous analysis identifier\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Get an Analysis detail\"\"\"\n",
    "\n",
    "# Your configuration sigil should contain:\n",
    "#   - token: Your API token\n",
    "#   - org: Your organization ID\n",
    "#   - project: Your project ID\n",
    "#   - analysis: Your analysis ID\n",
    "#   - collection: Your collection ID\n",
    "# Load token from secure storage\n",
    "with open(\"botify_token.txt\") as f:\n",
    "    token = f.read().strip()\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/crawl_statistics\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Return global statistics for an analysis\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Return global statistics for an analysis\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/crawl_statistics'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/crawl_statistics/time\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Return crawl statistics grouped by time frequency (1 min, 5 mins or 60 min) for an analysis\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- limit: max number of elements to retrieve\n",
    "- frequency: Aggregation frequency\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Return crawl statistics grouped by time frequency (1 min, 5 mins or 60 min) for an analysis\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/crawl_statistics/time'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/crawl_statistics/urls/{list_type}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Return a list of 1000 latest URLs crawled (all crawled URLs or only URLS with HTTP errors)\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Return a list of 1000 latest URLs crawled (all crawled URLs or only URLS with HTTP errors)\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/crawl_statistics/urls/{list_type}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/ganalytics/orphan_urls/{medium}/{source}\n",
    "\n",
    "**Purpose:**\n",
    "Legacy\n",
    "\n",
    "**Detailed Description:**\n",
    "Legacy    List of Orphan URLs. URLs which generated visits from the selected source according to Google Analytics data, but were not crawled with by the Botify crawler (either because no links to them were found on the website, or because the crawler was not allowed to follow these links according to the project settings).   For a search engine (medium: origanic; sources: all, aol, ask, baidu, bing, google, naver, yahoo, yandex) or a social network (medium: social; sources: all, facebook, google+, linkedin, pinterest, reddit, tumblr, twitter)\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Legacy    List of Orphan URLs. URLs which generated visits from the selected source according to Google Analytics data, but were not crawled with by the Botify crawler (either because no links to them were found on the website, or because the crawler was not allowed to follow these links according to the project settings).   For a search engine (medium: origanic; sources: all, aol, ask, baidu, bing, google, naver, yahoo, yandex) or a social network (medium: social; sources: all, facebook, google+, linkedin, pinterest, reddit, tumblr, twitter)\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/ganalytics/orphan_urls/{medium}/{source}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/links/percentiles\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Get inlinks percentiles\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Get inlinks percentiles\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/links/percentiles'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/pagerank/lost\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Lost pagerank\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Lost pagerank\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/pagerank/lost'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/scoring/summary\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Scoring summary\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Scoring summary\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/scoring/summary'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/search_console/stats\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List clicks and impressions per day\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List clicks and impressions per day\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/search_console/stats'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/sitemaps/report\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Get global information of the sitemaps found (sitemaps indexes, invalid sitemaps urls, etc.)\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Get global information of the sitemaps found (sitemaps indexes, invalid sitemaps urls, etc.)\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/sitemaps/report'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/sitemaps/samples/out_of_config\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Sample list of URLs which were found in your sitemaps but outside of the crawl perimeter defined for the project, for instance domain/subdomain or protocol (HTTP/HTTPS) not allowed in the crawl settings.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Sample list of URLs which were found in your sitemaps but outside of the crawl perimeter defined for the project, for instance domain/subdomain or protocol (HTTP/HTTPS) not allowed in the crawl settings.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/sitemaps/samples/out_of_config'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/sitemaps/samples/sitemap_only\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Sample list of URLs which were found in your sitemaps, within the project allowed scope (allowed domains/subdomains/protocols), but not found by the Botify crawler.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Sample list of URLs which were found in your sitemaps, within the project allowed scope (allowed domains/subdomains/protocols), but not found by the Botify crawler.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/sitemaps/samples/sitemap_only'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/top_domains/domains\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Top domains\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Top domains\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/top_domains/domains'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/top_domains/subdomains\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Top subddomains\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Top subddomains\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/top_domains/subdomains'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/features/visits/orphan_urls/{medium}/{source}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List of Orphan URLs. URLs which generated visits from the selected source according to Google Analytics data, but were not crawled with by the Botify crawler (either because no links to them were found on the website, or because the crawler was not allowed to follow these links according to the project settings).   For a search engine (medium: origanic; sources: all, aol, ask, baidu, bing, google, naver, yahoo, yandex) or a social network (medium: social; sources: all, facebook, google+, linkedin, pinterest, reddit, tumblr, twitter)\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List of Orphan URLs. URLs which generated visits from the selected source according to Google Analytics data, but were not crawled with by the Botify crawler (either because no links to them were found on the website, or because the crawler was not allowed to follow these links according to the project settings).   For a search engine (medium: origanic; sources: all, aol, ask, baidu, bing, google, naver, yahoo, yandex) or a social network (medium: social; sources: all, facebook, google+, linkedin, pinterest, reddit, tumblr, twitter)\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/features/visits/orphan_urls/{medium}/{source}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/segments\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Get the segments feature public metadata of an analysis.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Get the segments feature public metadata of an analysis.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/segments'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/staticfiles/robots-txt-indexes\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Return an object containing all robots.txt files found on the project's domains. The object is null for virtual robots.txt.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Return an object containing all robots.txt files found on the project's domains. The object is null for virtual robots.txt.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/staticfiles/robots-txt-indexes'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/staticfiles/robots-txt-indexes/{robots_txt}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Return content of a robots.txt file.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Return content of a robots.txt file.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/staticfiles/robots-txt-indexes/{robots_txt}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/urls/ai/{url}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Gets AI suggestions of an URL for an analysis\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Gets AI suggestions of an URL for an analysis\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls/ai/{url}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/urls/datamodel\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Gets an Analysis datamodel\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- area: Analysis context\n",
    "- previous_crawl: Previous analysis identifier\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Gets an Analysis datamodel\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls/datamodel'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/urls/datasets\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Gets Analysis Datasets\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- area: Analysis context\n",
    "- previous_crawl: Previous analysis identifier\n",
    "- deprecated_fields: Include deprecated fields\n",
    "- sequenced: Whether to use sequenced groups\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Gets Analysis Datasets\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls/datasets'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/urls/export\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "A list of the CSV Exports requests and their current status\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"A list of the CSV Exports requests and their current status\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls/export'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/urls/export/{url_export_id}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Checks the status of an CSVUrlExportJob object. Returns json object with the status.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Checks the status of an CSVUrlExportJob object. Returns json object with the status.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls/export/{url_export_id}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/urls/html/{url}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Gets the HTML of an URL for an analysis\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Gets the HTML of an URL for an analysis\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls/html/{url}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/{analysis_slug}/urls/{url}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Gets the detail of an URL for an analysis\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- fields: comma separated list of fields to return (c.f. URLs Datamodel)\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Gets the detail of an URL for an analysis\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls/{url}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /analyses/{username}/{project_slug}/create/launch\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Create and launch an analysis for a project\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 201: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Create and launch an analysis for a project\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/create/launch'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /analyses/{username}/{project_slug}/{analysis_slug}/pause\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Pause an analysis for a project\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 201: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Pause an analysis for a project\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/pause'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /analyses/{username}/{project_slug}/{analysis_slug}/resume\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Resume an analysis for a project\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 201: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Resume an analysis for a project\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/resume'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /analyses/{username}/{project_slug}/{analysis_slug}/urls\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Executes a query and returns a paginated response\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- Query: Query\n",
    "- area: Analysis context\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "- previous_crawl: Previous analysis identifier\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Executes a query and returns a paginated response\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /analyses/{username}/{project_slug}/{analysis_slug}/urls/aggs\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Query aggregator. It accepts multiple queries and dispatches them.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- AggsQueries: AggsQueries\n",
    "- area: Analysis context\n",
    "- previous_crawl: Previous analysis identifier\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Query aggregator. It accepts multiple queries and dispatches them.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls/aggs'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /analyses/{username}/{project_slug}/{analysis_slug}/urls/export\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Creates a new UrlExport object and starts a task that will export the results into a csv. Returns the model id that manages the task\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- Query: Query\n",
    "- area: Analysis context\n",
    "- previous_crawl: Previous analysis identifier\n",
    "- export_size: Maximum size of the export (deprecated => size instead)\n",
    "- size: Maximum size of the export\n",
    "- compression: Compressed file format\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 201: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Creates a new UrlExport object and starts a task that will export the results into a csv. Returns the model id that manages the task\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/{analysis_slug}/urls/export'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Collections Invocations\n",
    "\n",
    "These endpoints allow you to manipulate collections aspects of your digital realm.\n",
    "\n",
    "##### GET /projects/{username}/{project_slug}/collections\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List all collections for a project\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List all collections for a project\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}/{project_slug}/collections'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /projects/{username}/{project_slug}/collections/{collection}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Get the detail of a collection\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- sequenced: Whether to use sequenced groups\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Get the detail of a collection\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}/{project_slug}/collections/{collection}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Datasource Invocations\n",
    "\n",
    "These endpoints allow you to manipulate datasource aspects of your digital realm.\n",
    "\n",
    "##### GET /users/{username}/datasources_summary_by_projects\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Get the datasources details for all projects of a user\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Get the datasources details for all projects of a user\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/users/{username}/datasources_summary_by_projects'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Job Invocations\n",
    "\n",
    "These endpoints allow you to manipulate job aspects of your digital realm.\n",
    "\n",
    "##### GET /jobs\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List All jobs\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "- job_type: The job type\n",
    "- project: The project slug\n",
    "- analysis: The analysis slug\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List All jobs\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/jobs'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /jobs/{job_id}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Retrieve one job\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Retrieve one job\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/jobs/{job_id}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /jobs\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Creates a job instance of a class which depends on the \"job_type\" parameter you sent. Returns the model id that manages the task.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- JobCreate: Job create\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 201: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Creates a job instance of a class which depends on the \"job_type\" parameter you sent. Returns the model id that manages the task.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/jobs'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Project Invocations\n",
    "\n",
    "These endpoints allow you to manipulate project aspects of your digital realm.\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List all analyses for a project\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "- only_success: Return only successfully finished analyses\n",
    "- fields: Which fields to return (comma-separated)\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List all analyses for a project\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /analyses/{username}/{project_slug}/light\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List all analyses for a project (light)\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "- only_success: Return only successfully finished analyses\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List all analyses for a project (light)\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/analyses/{username}/{project_slug}/light'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /projects/{username}/{project_slug}/filters\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List all the project's saved filters (each filter's name, ID and filter value)\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "- search: Search query on the filter name\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List all the project's saved filters (each filter's name, ID and filter value)\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}/{project_slug}/filters'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /projects/{username}/{project_slug}/filters/{identifier}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Retrieve a specific project saved, account or recommended filter's name, ID and filter value\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Retrieve a specific project saved, account or recommended filter's name, ID and filter value\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}/{project_slug}/filters/{identifier}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /projects/{username}/{project_slug}/saved_explorers\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List all the project's Saved Explorers.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List all the project's Saved Explorers.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}/{project_slug}/saved_explorers'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GET /users/{username}/projects\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List all active projects for the user\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "No parameters required\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List all active projects for the user\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/users/{username}/projects'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /projects/{username}/{project_slug}/query\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Query collections at a project level.\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- ProjectQuery: Project Query\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Query collections at a project level.\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}/{project_slug}/query'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /projects/{username}/{project_slug}/urls/aggs\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Project Query aggregator. It accepts multiple queries that will be executed on all completed analyses in the project\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- AggsQueries: AggsQueries\n",
    "- area: Analyses context\n",
    "- last_analysis_slug: Last analysis on the trend\n",
    "- nb_analyses: Max number of analysis to return\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 201: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Project Query aggregator. It accepts multiple queries that will be executed on all completed analyses in the project\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}/{project_slug}/urls/aggs'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### POST /projects/{username}/{project_slug}/values_list/clone\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "Clone all keyword groups of the current project to another one.  This endpoint is a little more general (for further use). That's why you will see a 'type' field (with a default value of: 'keywords').\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- ValuesListCloneRequestBody: JSON body to use as request body for ValuesList clone operation\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 201: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"Clone all keyword groups of the current project to another one.  This endpoint is a little more general (for further use). That's why you will see a 'type' field (with a default value of: 'keywords').\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}/{project_slug}/values_list/clone'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Define the payload for your invocation\n",
    "data = {\n",
    "    # Add your request parameters here\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.post(url, headers=headers, json=data)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ProjectQuery Invocations\n",
    "\n",
    "These endpoints allow you to manipulate projectquery aspects of your digital realm.\n",
    "\n",
    "##### GET /projects/{username}/{project_slug}/account_filters\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List all the account saved filters\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "- search: Search query on the filter name\n",
    "- disable_project: Flag to return or not filters of this project\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List all the account saved filters\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}/{project_slug}/account_filters'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### User Invocations\n",
    "\n",
    "These endpoints allow you to manipulate user aspects of your digital realm.\n",
    "\n",
    "##### GET /projects/{username}\n",
    "\n",
    "**Purpose:**\n",
    "No summary provided.\n",
    "\n",
    "**Detailed Description:**\n",
    "List all active projects for the user\n",
    "\n",
    "**Parameters Required:**\n",
    "```\n",
    "- name: Project's name\n",
    "- page: Page Number\n",
    "- size: Page Size\n",
    "```\n",
    "\n",
    "**Expected Responses:**\n",
    "```\n",
    "- default: error payload\n",
    "- 200: Successful operation\n",
    "```\n",
    "\n",
    "**Example Implementation:**\n",
    "```python\n",
    "# Summon the necessary artifacts\n",
    "import httpx\n",
    "import json\n",
    "def example_request():\n",
    "\n",
    "\n",
    "    \"\"\"List all active projects for the user\"\"\"\n",
    "\n",
    "# Craft the invocation URL\n",
    "url = f'https://api.botify.com/v1/projects/{username}'\n",
    "# Prepare the headers for your spell\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Cast the spell\n",
    "response = httpx.get(url, headers=headers)\n",
    "# Interpret the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### LLM Guidance\n",
    "\n",
    "When deciding which endpoint to use:\n",
    "1. Consider the category (tag) that matches your task\n",
    "2. Review the Purpose and Description to ensure alignment\n",
    "3. Check the required parameters match your available data\n",
    "4. Verify the expected responses meet your needs\n",
    "\n",
    "Example prompt format:\n",
    "```\n",
    "I need to [task description]. I have access to [available data].\n",
    "Which endpoint would be most appropriate?\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
