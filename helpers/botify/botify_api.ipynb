{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DON'T PANIC!\n",
    "\n",
    "## Welcome to 2 Tabs\n",
    "\n",
    "If you don't see the 2nd tab appear in the next few seconds, shut out of that Terminal, open another and `nix develop` again.\n",
    "\n",
    "Welcome to both **JupyterLab** and **Pipulate** (aka Botifython). Here are the most critical things you need to know.\n",
    "\n",
    "1. If you see this, ***great job!*** The install worked. You're almost there.\n",
    "2. ***Two tabs will open***: this one plus one more for Pipulate (aka Botifython).\n",
    "3. This tab, where *full notebooks run* (you can see the code) is extra.\n",
    "\n",
    "Pipulate is to run SEO Workflows as Web Apps that would otherwise require Notebooks but without the Notebooks ‚Äî mostly so you don't have to look at the Python code.\n",
    "\n",
    "### If Pipulate is for Web Apps instead of Notebooks, why JupyterLab?\n",
    "\n",
    "1. **Debugging**: Pipulate shows Python code to help workflow development. This gives a built-in place to copy/paste and test-run it.\n",
    "2. **Education**: Just because Pipulate *fishes for you* doesn't mean you shouldn't learn how to fish. Learn Python to better speak with AI.\n",
    "3. **Integration**: While it's not implemented yet, a certain type of widget we plan on using (AnyWidget) works best with JupyterLab here.\n",
    "\n",
    "## How to Get Started?\n",
    "\n",
    "1. Don't. Go over and use Pipulate instead.\n",
    "2. Oh, okay. If you're here for Python, keep reading.\n",
    "\n",
    "Let's start with the ***Hello World*** program. In a Notebook, The Hello World program can be this simple (run the cell below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "How's that for simple? This works because the *Notebook environment* automatically prints the last line of a cell. You can also explicitly use the `print()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Hello World Program\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Easy, right? Welcome to the hook that's makes Python ***so enduringly popular*** even in the face of JavaScript taking over the world. \n",
    "\n",
    "There are small differences between using `print()` and relying on the automatic output of being the last line of a cell. Specifically, `print()` leaves off the quotes for more user-friendly output while the other way includes the quotes to help you identify the value as a '`String`' datatype. \n",
    "\n",
    "## Data Piping\n",
    "\n",
    "Oh there's so much more we should say before jumping into the concept of ***data pipelines*** ‚Äî but they are so fundamental to everything to follow that we find it impossible to not jump right in and show you that if you set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Hello\"\n",
    "a  # Prints a because it's on a line by itself at the end of the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "...then `a` is still hanging around in memory. The value of a *persists!* And that means you can start piping the ouput of `a` into the input of `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a + \" World!\"  # Appends a to the beginning of a String\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The entire world of technology hinges on this [data pipelines](https://en.wikipedia.org/wiki/Pipeline_(Unix)) concept. Details will vary from tech system to system, particularly how persistence is achieved. People rarely frame it this way but Notebooks are popular in part because of how well they *pipe* the output of one cell so intuitively and naturally into the input of the next cell ‚Äî chaining them up into a data pipeline driven workflow uniquely suited for the fields of [science](https://en.wikipedia.org/wiki/Replication_crisis) and [finance](https://www.youtube.com/watch?v=3ygPF-Qshkc). This piping of data concept is so important that we have made the Hello World of data piping into a [core feature of Pipulate](http://localhost:5001/hello_workflow).\n",
    "\n",
    "## Terminology \n",
    "\n",
    "Now there is a lot of terminology here and presumed prerequisite knowledge. We will get as much of it out of the way immediately as possible. There's tons more. It is a lifetime of learning, but this Notebook is only trying to do so much. we'll give ***further reading*** links where possible.\n",
    "\n",
    "1. **This is a Notebook**: Notebooks, aka *Jupyter Notebook*, deliberately mix documentation (like this) and code. That's a large part of the point. The concept is called [Literate Programming](https://en.wikipedia.org/wiki/Literate_programming), coined by Donald Knuth but brought to you in modern times by [Fernando P√©rez](https://en.wikipedia.org/wiki/Fernando_P%C3%A9rez_(software_developer))\n",
    "2. **Cell-by-cell run**: The way code executes in a Notebook is unique. Each cell can be run individually instead of the more typical **all at once** way (with more traditional `.py` files). The concept is [REPL](https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop), coined by L. Peter Deutsch and Edmund Berkeley.\n",
    "3. **Platform**: Many things run Notebooks, like Google Colab, but they do not run them the exact same way. There is still an underlying platform / environment / context:\n",
    "   - Cloud vs. Local\n",
    "   - If local, host system vs. virtual environment/subsystem\n",
    "4. **Nix**: The way Notebooks are running here is local with a Linux [Nix](https://nixos.org/) subsystem.\n",
    "5. **Leading edge**: You are on the edge here. Most people are going to do this kind of work out on the cloud to make it easy-as-possible ‚Äî but that's full of limits, privacy concerns and hidden costs. Pipulate makes the increased price of free & local (complexity) as gentle as possible.\n",
    "\n",
    "## Free Software\n",
    "\n",
    "If you're not paying for the product, then you are the product ‚Äî **this is not true.** *Genuine altruism exists in this world starting with GNU and the FSF* (Free Software Foundation) behind Linux and many of the other components of the Pipulate system. That expression has had the unfortunate effect over the years of steering people away from the underlying technology stack that everyone providing the paid-for or advertising-subsidized service builds on in the first place. The price of true freedom (as in liberty) is complexity, but it is becoming easier thanks to tools like Nix.\n",
    "\n",
    "## Copy/Paste\n",
    "\n",
    "Blah, blah, blah. But you're here to run code! Okay, go ahead and paste some Python code in the empty code-cell provided below. Everyting past that is Botify Boot Camp for Botify employees and customers learning the API. Sorry folks, but I know what pays the bills so Botify always comes first. If you want a new Notebook:\n",
    "\n",
    "1. Go to File / New Launcher (`Ctrl`+`Shift`+`L` or the big blue button all the way to the left)\n",
    "2. Click the Notebook icon (Python 3 ipykernel)\n",
    "\n",
    "## More Tutorials\n",
    "\n",
    "Tutorials for Jupyter Notebooks are all over the Web, but one of the more interesting ones is this experimental one from Project Jupyter themselves:\n",
    "\n",
    "- [Introduction to the JupyterLab and Jupyter Notebooks](https://jupyter.org/try-jupyter/notebooks/?path=notebooks/Intro.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should really start a new Notebook if you want to paste and run some Python code\n",
    "# but understanding a lot of folks will struggle with this, we provide this cell for that.\n",
    "# Alternatively, press Ctrl+Shift+L or click the big blue plus in the upper-left.\n",
    "\n",
    "print(\"Replace me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Everything below this point is for Botify employees and perhaps the brave and the bold customers!\n",
    "\n",
    "# Botify API Bootcamp\n",
    "\n",
    "> This Notebook is for Botify Employees who have a Botify API Key and for the LLMs being trained to help humans with the API. Botify Customers may also find it valuable. This Notebook runs from the JupyterLab on your local machine that gets installed side-by-side with Botifython. Running the last cell generates [Botify API Bootcamp](http://localhost:5001/docs/botify_api/page/1) documentation in the Pipulate app.\n",
    "\n",
    "## Run Two Scripts\n",
    "\n",
    "The first 2 scripts in this series (immediately below) set you up. They may look large and scary, but they each do 1 very simple thing: \n",
    "\n",
    "- Cell 2 makes sure you have your `botify_token.txt` for API-access.\n",
    "- Cell 3 makes sure you have an example `config.json` to use in the examples.\n",
    "\n",
    "> When you run the scripts in cells 2 & 3, they will appear to *\"lock up\"*. It's not. Input fields are waiting for you. Type/paste into the field and *Hit Enter*.  \n",
    "> If you get confused, hit `Esc` + `0` + `0` + `Enter` and that will *restart the kernel.*  \n",
    "> Yes, that's **Esc, Zero, Zero, Enter** ‚Äî *it's a Notebook thing.*  \n",
    "\n",
    "Good luck!\n",
    "\n",
    "## Script #1: Get Your Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Botify Token Setup\n",
    "==================\n",
    "\n",
    "A standalone Jupyter notebook program that:\n",
    "1. Guides users to get their Botify API token\n",
    "2. Validates the token with the Botify API\n",
    "3. Saves the token to botify_token.txt for use in other programs\n",
    "\n",
    "This replicates the functionality of the \"Connect With Botify\" workflow\n",
    "but as a simple standalone script for Jupyter notebooks.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import getpass\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import httpx\n",
    "\n",
    "\n",
    "# Configuration\n",
    "TOKEN_FILE = 'botify_token.txt'\n",
    "ACCOUNT_URL = \"https://app.botify.com/account\"\n",
    "\n",
    "\n",
    "def display_instructions():\n",
    "    \"\"\"Display instructions for getting the Botify API token.\"\"\"\n",
    "    print(\"üöÄ Botify API Token Setup\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"To use Botify API tutorials and tools, you need to set up your API token.\")\n",
    "    print()\n",
    "    print(\"üìã Steps to get your token:\")\n",
    "    print(\"1. Visit your Botify account page:\")\n",
    "    print(f\"   üîó {ACCOUNT_URL}\")\n",
    "    print(\"2. Look for the 'API' or 'Tokens' section\")\n",
    "    print(\"3. Copy your 'Main Token' (not a project-specific token)\")\n",
    "    print(\"4. Paste it in the input box below\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def check_existing_token() -> Optional[str]:\n",
    "    \"\"\"Check if a token file already exists and return its content.\"\"\"\n",
    "    # Search for token file in multiple locations (same logic as config generator)\n",
    "    search_paths = [\n",
    "        TOKEN_FILE,  # Current directory\n",
    "        os.path.join(os.getcwd(), TOKEN_FILE),  # Explicit current directory\n",
    "        os.path.join(os.path.expanduser('~'), TOKEN_FILE),  # Home directory\n",
    "        os.path.join('/home/mike/repos/pipulate', TOKEN_FILE),  # Pipulate root\n",
    "        os.path.join('/home/mike/repos', TOKEN_FILE),  # Repos root\n",
    "    ]\n",
    "    \n",
    "    # Add script directory if __file__ is available (not in Jupyter)\n",
    "    try:\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        search_paths.insert(3, os.path.join(script_dir, TOKEN_FILE))\n",
    "    except NameError:\n",
    "        # __file__ not available (running in Jupyter), skip script directory\n",
    "        pass\n",
    "    \n",
    "    # Also check if we're in a subdirectory and look up the tree\n",
    "    current_dir = os.getcwd()\n",
    "    while current_dir != os.path.dirname(current_dir):  # Not at filesystem root\n",
    "        search_paths.append(os.path.join(current_dir, TOKEN_FILE))\n",
    "        current_dir = os.path.dirname(current_dir)\n",
    "    \n",
    "    for path in search_paths:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    content = f.read().strip()\n",
    "                    token = content.split('\\n')[0].strip()\n",
    "                    if token:\n",
    "                        print(f\"üîç Found existing token file: {path}\")\n",
    "                        return token\n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "async def validate_botify_token(token: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Validate the Botify API token and return the username if successful.\n",
    "    \n",
    "    Args:\n",
    "        token: The Botify API token to validate\n",
    "        \n",
    "    Returns:\n",
    "        str or None: The username if token is valid, None otherwise\n",
    "    \"\"\"\n",
    "    url = \"https://api.botify.com/v1/authentication/profile\"\n",
    "    headers = {\"Authorization\": f\"Token {token}\"}\n",
    "    \n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(url, headers=headers, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Extract username if the token is valid\n",
    "            user_data = response.json()\n",
    "            username = user_data[\"data\"][\"username\"]\n",
    "            return username\n",
    "            \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        if e.response.status_code == 401:\n",
    "            print(f\"‚ùå Authentication failed: Invalid token\")\n",
    "        elif e.response.status_code == 403:\n",
    "            print(f\"‚ùå Access forbidden: Token may not have required permissions\")\n",
    "        else:\n",
    "            print(f\"‚ùå HTTP error {e.response.status_code}: {e.response.text}\")\n",
    "        return None\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"‚ùå Network error: {str(e)}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Invalid response format: {str(e)}\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Unexpected response structure: Missing {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_token(token: str, username: str) -> bool:\n",
    "    \"\"\"\n",
    "    Save the validated token to the token file in the current working directory.\n",
    "    Always saves locally to ensure relative paths work for sample scripts.\n",
    "    \n",
    "    Args:\n",
    "        token: The validated Botify API token\n",
    "        username: The username associated with the token\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if saved successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Always save to current working directory for notebook compatibility\n",
    "        local_token_path = os.path.join(os.getcwd(), TOKEN_FILE)\n",
    "        \n",
    "        # Save token with a comment containing the username\n",
    "        with open(local_token_path, 'w') as f:\n",
    "            f.write(f\"{token}\\n# username: {username}\")\n",
    "        \n",
    "        print(f\"‚úÖ Token saved locally to: {local_token_path}\")\n",
    "        print(f\"üìÅ This ensures relative paths work for sample scripts and notebooks.\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving token: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    display_instructions()\n",
    "    \n",
    "    # Check for existing token\n",
    "    existing_token = check_existing_token()\n",
    "    if existing_token:\n",
    "        print(\"‚ö†Ô∏è  You already have a Botify API token configured.\")\n",
    "        print()\n",
    "        \n",
    "        # Validate existing token\n",
    "        print(\"üîç Validating existing token...\")\n",
    "        username = await validate_botify_token(existing_token)\n",
    "        \n",
    "        if username:\n",
    "            print(f\"‚úÖ Existing token is valid for user: {username}\")\n",
    "            print()\n",
    "            \n",
    "            # Check if token is already local to current directory\n",
    "            local_token_path = os.path.join(os.getcwd(), TOKEN_FILE)\n",
    "            existing_token_path = None\n",
    "            \n",
    "            # Find where the existing token is located\n",
    "            search_paths = [\n",
    "                TOKEN_FILE,  # Current directory\n",
    "                os.path.join(os.getcwd(), TOKEN_FILE),  # Explicit current directory\n",
    "                os.path.join(os.path.expanduser('~'), TOKEN_FILE),  # Home directory\n",
    "                os.path.join('/home/mike/repos/pipulate', TOKEN_FILE),  # Pipulate root\n",
    "                os.path.join('/home/mike/repos', TOKEN_FILE),  # Repos root\n",
    "            ]\n",
    "            \n",
    "            for path in search_paths:\n",
    "                if os.path.exists(path):\n",
    "                    existing_token_path = path\n",
    "                    break\n",
    "            \n",
    "            # If token is not in current directory, copy it locally\n",
    "            if existing_token_path and os.path.abspath(existing_token_path) != os.path.abspath(local_token_path):\n",
    "                print(f\"üìã Copying token from {existing_token_path} to current directory...\")\n",
    "                if save_token(existing_token, username):\n",
    "                    print(\"‚úÖ Token copied locally for notebook compatibility.\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Failed to copy token locally, but existing token is still valid.\")\n",
    "            \n",
    "            # Ask if they want to update it\n",
    "            update_choice = getpass.getpass(\"Do you want to update your token? (y/N): \").strip().lower()\n",
    "            if update_choice not in ['y', 'yes']:\n",
    "                print(\"üéâ Setup complete! Your token is ready to use.\")\n",
    "                return True  # Return success status, not the token\n",
    "        else:\n",
    "            print(\"‚ùå Existing token is invalid. You'll need to enter a new one.\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Get token from user\n",
    "    print(\"üìù Please paste your Botify API 'Main Token' below:\")\n",
    "    print(\"(The input will be hidden for security)\")\n",
    "    print()\n",
    "    \n",
    "    # Diagnostic information\n",
    "    print(f\"üîß Running in: {sys.executable}\")\n",
    "    print(f\"üîß sys.stdin.isatty(): {sys.stdin.isatty()}\")\n",
    "    print()\n",
    "    \n",
    "    # Use getpass for hidden input - no fallback to visible input\n",
    "    try:\n",
    "        token = getpass.getpass(\"Botify API Token: \").strip()\n",
    "        if token:\n",
    "            print(f\"‚úÖ Token received (length: {len(token)})\")\n",
    "        else:\n",
    "            print(\"‚ùå No token provided. Setup cancelled.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå getpass.getpass() failed: {e}\")\n",
    "        print(\"‚ùå Secure input not available. Setup cancelled.\")\n",
    "        print(\"üí° Try running this in a proper terminal or Jupyter environment.\")\n",
    "        return False\n",
    "    \n",
    "    # Validate the token\n",
    "    print()\n",
    "    print(\"üîç Validating your token...\")\n",
    "    username = await validate_botify_token(token)\n",
    "    \n",
    "    if not username:\n",
    "        print(\"‚ùå Token validation failed. Please check your token and try again.\")\n",
    "        print()\n",
    "        print(\"üí° Make sure you're using the 'Main Token' from:\")\n",
    "        print(f\"   {ACCOUNT_URL}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"‚úÖ Token validated successfully for user: {username}\")\n",
    "    \n",
    "    # Save the token\n",
    "    print()\n",
    "    print(\"üíæ Saving token...\")\n",
    "    if save_token(token, username):\n",
    "        print()\n",
    "        print(\"üéâ Botify API setup complete!\")\n",
    "        print()\n",
    "        print(\"üìÑ You can now use:\")\n",
    "        print(\"   ‚Ä¢ Botify API tutorials and examples\")\n",
    "        print(\"   ‚Ä¢ The Botify configuration generator\")\n",
    "        print(\"   ‚Ä¢ Any Botify-related workflows\")\n",
    "        print()\n",
    "        print(\"üîß Next steps:\")\n",
    "        print(\"   1. Run the Botify configuration generator to set up project config\")\n",
    "        print(\"   2. Explore the Botify API tutorials in the notebooks\")\n",
    "        \n",
    "        return True  # Return success status, not the token\n",
    "    else:\n",
    "        print(\"‚ùå Failed to save token. Setup incomplete.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# For Jupyter Notebook execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if we're in Jupyter (has get_ipython function)\n",
    "    try:\n",
    "        get_ipython()\n",
    "        print(\"üî¨ Running in Jupyter environment\")\n",
    "        # In Jupyter, use await directly\n",
    "        success = await main()\n",
    "    except NameError:\n",
    "        # Not in Jupyter, use asyncio.run()\n",
    "        print(\"üñ•Ô∏è  Running in standard Python environment\")\n",
    "        success = asyncio.run(main()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Script #2: Setup config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Botify Configuration Generator\n",
    "=============================\n",
    "\n",
    "A standalone Jupyter notebook program that:\n",
    "1. Asks for a Botify project URL via input()\n",
    "2. Validates the URL and extracts org/project info\n",
    "3. Fetches the most recent analysis from Botify API\n",
    "4. Generates a configuration JSON file\n",
    "\n",
    "Based on the Botify Trifecta workflow patterns.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from typing import Tuple, Optional, Dict, Any\n",
    "\n",
    "import httpx\n",
    "\n",
    "\n",
    "# Configuration\n",
    "TOKEN_FILE = 'botify_token.txt'\n",
    "\n",
    "\n",
    "def load_api_token() -> str:\n",
    "    \"\"\"Load the Botify API token from the token file.\"\"\"\n",
    "    # Search for token file in multiple locations\n",
    "    search_paths = [\n",
    "        TOKEN_FILE,  # Current directory\n",
    "        os.path.join(os.getcwd(), TOKEN_FILE),  # Explicit current directory\n",
    "        os.path.join(os.path.expanduser('~'), TOKEN_FILE),  # Home directory\n",
    "        os.path.join('/home/mike/repos/pipulate', TOKEN_FILE),  # Pipulate root\n",
    "        os.path.join('/home/mike/repos', TOKEN_FILE),  # Repos root\n",
    "    ]\n",
    "    \n",
    "    # Add script directory if __file__ is available (not in Jupyter)\n",
    "    try:\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        search_paths.insert(3, os.path.join(script_dir, TOKEN_FILE))\n",
    "    except NameError:\n",
    "        # __file__ not available (running in Jupyter), skip script directory\n",
    "        pass\n",
    "    \n",
    "    # Also check if we're in a subdirectory and look up the tree\n",
    "    current_dir = os.getcwd()\n",
    "    while current_dir != os.path.dirname(current_dir):  # Not at filesystem root\n",
    "        search_paths.append(os.path.join(current_dir, TOKEN_FILE))\n",
    "        current_dir = os.path.dirname(current_dir)\n",
    "    \n",
    "    token_path = None\n",
    "    for path in search_paths:\n",
    "        if os.path.exists(path):\n",
    "            token_path = path\n",
    "            break\n",
    "    \n",
    "    if not token_path:\n",
    "        searched_locations = '\\n  - '.join(search_paths[:6])  # Show first 6 locations\n",
    "        raise ValueError(f\"Token file '{TOKEN_FILE}' not found in any of these locations:\\n  - {searched_locations}\\n\\nPlease ensure the file exists in one of these locations.\")\n",
    "    \n",
    "    try:\n",
    "        with open(token_path) as f:\n",
    "            content = f.read().strip()\n",
    "            api_key = content.split('\\n')[0].strip()\n",
    "            if not api_key:\n",
    "                raise ValueError(f\"Token file '{token_path}' is empty.\")\n",
    "            print(f\"üîë Using token from: {token_path}\")\n",
    "            return api_key\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading token file '{token_path}': {str(e)}\")\n",
    "\n",
    "\n",
    "def validate_botify_url(url: str) -> Tuple[bool, str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Validate a Botify project URL and extract project information.\n",
    "    \n",
    "    Args:\n",
    "        url: The Botify project URL to validate\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, message, project_data)\n",
    "    \"\"\"\n",
    "    url = url.strip()\n",
    "    if not url:\n",
    "        return (False, 'URL is required', {})\n",
    "    \n",
    "    try:\n",
    "        if not url.startswith(('https://app.botify.com/')):\n",
    "            return (False, 'URL must be a Botify project URL (starting with https://app.botify.com/)', {})\n",
    "        \n",
    "        parsed_url = urlparse(url)\n",
    "        path_parts = [p for p in parsed_url.path.strip('/').split('/') if p]\n",
    "        \n",
    "        if len(path_parts) < 2:\n",
    "            return (False, 'Invalid Botify URL: must contain at least organization and project', {})\n",
    "        \n",
    "        org_slug = path_parts[0]\n",
    "        project_slug = path_parts[1]\n",
    "        \n",
    "        canonical_url = f'https://{parsed_url.netloc}/{org_slug}/{project_slug}/'\n",
    "        \n",
    "        project_data = {\n",
    "            'url': canonical_url,\n",
    "            'username': org_slug,\n",
    "            'project_name': project_slug,\n",
    "            'project_id': f'{org_slug}/{project_slug}'\n",
    "        }\n",
    "        \n",
    "        return (True, f'Valid Botify project: {project_slug}', project_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return (False, f'Error parsing URL: {str(e)}', {})\n",
    "\n",
    "\n",
    "async def get_username(api_token: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch the username associated with the API key.\n",
    "    \n",
    "    Args:\n",
    "        api_token: Botify API token\n",
    "        \n",
    "    Returns:\n",
    "        Username string or None if error\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Token {api_token}\"}\n",
    "    \n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(\"https://api.botify.com/v1/authentication/profile\", headers=headers, timeout=60.0)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"data\"][\"username\"]\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error fetching username: {str(e)}')\n",
    "        return None\n",
    "\n",
    "\n",
    "async def fetch_projects(username: str, api_token: str) -> Optional[list]:\n",
    "    \"\"\"\n",
    "    Fetch all projects for a given username from Botify API.\n",
    "    \n",
    "    Args:\n",
    "        username: Username to fetch projects for\n",
    "        api_token: Botify API token\n",
    "        \n",
    "    Returns:\n",
    "        List of project tuples (name, slug, user_or_org) or None if error\n",
    "    \"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{username}\"\n",
    "    headers = {\"Authorization\": f\"Token {api_token}\"}\n",
    "    projects = []\n",
    "    \n",
    "    try:\n",
    "        while url:\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, headers=headers, timeout=60.0)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                # Extract project info as tuples (name, slug, user_or_org)\n",
    "                for p in data.get('results', []):\n",
    "                    projects.append((\n",
    "                        p['name'], \n",
    "                        p['slug'], \n",
    "                        p['user']['login']\n",
    "                    ))\n",
    "                \n",
    "                url = data.get('next')\n",
    "        \n",
    "        return sorted(projects) if projects else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error fetching projects for {username}: {str(e)}')\n",
    "        return None\n",
    "\n",
    "\n",
    "async def fetch_most_recent_analysis(org: str, project: str, api_token: str, quiet: bool = False) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch the most recent analysis slug for a Botify project.\n",
    "    \n",
    "    Args:\n",
    "        org: Organization slug\n",
    "        project: Project slug\n",
    "        api_token: Botify API token\n",
    "        \n",
    "    Returns:\n",
    "        Most recent analysis slug or None if error/no analyses found\n",
    "    \"\"\"\n",
    "    if not org or not project or not api_token:\n",
    "        print(f'‚ùå Missing required parameters: org={org}, project={project}')\n",
    "        return None\n",
    "    \n",
    "    # Fetch first page of analyses (they should be sorted by date, most recent first)\n",
    "    url = f'https://api.botify.com/v1/analyses/{org}/{project}/light'\n",
    "    headers = {\n",
    "        'Authorization': f'Token {api_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(url, headers=headers, timeout=60.0)\n",
    "            \n",
    "        if response.status_code != 200:\n",
    "            print(f'‚ùå API error: Status {response.status_code} for {url}')\n",
    "            print(f'Response: {response.text}')\n",
    "            return None\n",
    "        \n",
    "        data = response.json()\n",
    "        if 'results' not in data:\n",
    "            print(f\"‚ùå No 'results' key in response: {data}\")\n",
    "            return None\n",
    "        \n",
    "        analyses = data['results']\n",
    "        if not analyses:\n",
    "            if not quiet:\n",
    "                print('‚ùå No analyses found for this project')\n",
    "            return None\n",
    "        \n",
    "        # Get the first (most recent) analysis\n",
    "        most_recent = analyses[0]\n",
    "        analysis_slug = most_recent.get('slug')\n",
    "        \n",
    "        if not analysis_slug:\n",
    "            if not quiet:\n",
    "                print('‚ùå No slug found in most recent analysis')\n",
    "            return None\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'‚úÖ Found most recent analysis: {analysis_slug}')\n",
    "            print(f'üìä Total analyses available: {len(analyses)}')\n",
    "            \n",
    "            # Show some details about the most recent analysis\n",
    "            if 'date_created' in most_recent:\n",
    "                print(f'üìÖ Created: {most_recent[\"date_created\"]}')\n",
    "            if 'date_last_modified' in most_recent:\n",
    "                print(f'üîÑ Last modified: {most_recent[\"date_last_modified\"]}')\n",
    "        \n",
    "        return analysis_slug\n",
    "        \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f'‚ùå HTTP error: {e.response.status_code} - {e.response.text}')\n",
    "        return None\n",
    "    except httpx.RequestError as e:\n",
    "        print(f'‚ùå Network error: {str(e)}')\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f'‚ùå JSON decode error: {str(e)}')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Unexpected error: {str(e)}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_config(org: str, project: str, analysis_slug: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate the configuration dictionary.\n",
    "    \n",
    "    Args:\n",
    "        org: Organization slug\n",
    "        project: Project slug  \n",
    "        analysis_slug: Analysis slug\n",
    "        \n",
    "    Returns:\n",
    "        Configuration dictionary\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"org\": org,\n",
    "        \"project\": project,\n",
    "        \"analysis\": analysis_slug,\n",
    "        \"collection\": f\"crawl.{analysis_slug}\"\n",
    "    }\n",
    "\n",
    "\n",
    "async def get_default_configuration(api_token: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Get default configuration using first available project with analyses.\n",
    "    \n",
    "    Args:\n",
    "        api_token: Botify API token\n",
    "        \n",
    "    Returns:\n",
    "        Default configuration dictionary or None if unable to generate\n",
    "    \"\"\"\n",
    "    print(\"üîç Fetching your default configuration...\")\n",
    "    \n",
    "    # Step 1: Get username\n",
    "    print(\"üë§ Fetching username...\")\n",
    "    username = await get_username(api_token)\n",
    "    if not username:\n",
    "        print(\"‚ùå Could not fetch username\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üë§ Using username: {username}\")\n",
    "    \n",
    "    # Step 2: Get projects for username\n",
    "    print(f\"üìÅ Fetching projects for {username}...\")\n",
    "    projects = await fetch_projects(username, api_token)\n",
    "    if not projects:\n",
    "        print(\"‚ùå No projects found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìä Found {len(projects)} projects. Searching for one with analyses...\")\n",
    "    \n",
    "    # Step 3: Try each project until we find one with analyses\n",
    "    for i, project_tuple in enumerate(projects):\n",
    "        project_name = project_tuple[0]  # name\n",
    "        project_slug = project_tuple[1]  # slug\n",
    "        org_slug = project_tuple[2]      # user_or_org\n",
    "        \n",
    "        print(f\"üîç Trying project {i+1}/{len(projects)}: {org_slug}/{project_slug}\")\n",
    "        \n",
    "        # Try to get latest analysis for this project (quiet mode during search)\n",
    "        analysis_slug = await fetch_most_recent_analysis(org_slug, project_slug, api_token, quiet=True)\n",
    "        \n",
    "        if analysis_slug:\n",
    "            print(f\"‚úÖ Found project with analyses!\")\n",
    "            print(f\"üè¢ Using organization: {org_slug}\")\n",
    "            print(f\"üìÅ Using project: {project_slug}\")\n",
    "            \n",
    "            # Now get the analysis details with full output\n",
    "            print(f\"üîç Fetching analysis details...\")\n",
    "            await fetch_most_recent_analysis(org_slug, project_slug, api_token, quiet=False)\n",
    "            \n",
    "            # Step 4: Generate configuration\n",
    "            config = generate_config(org_slug, project_slug, analysis_slug)\n",
    "            return config\n",
    "        else:\n",
    "            print(f\"‚è≠Ô∏è  No analyses found for {org_slug}/{project_slug}, trying next project...\")\n",
    "    \n",
    "    # If we get here, no projects had analyses\n",
    "    print(\"‚ùå No projects found with analyses\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_config(config: Dict[str, str], filename: str = \"botify_config.json\") -> None:\n",
    "    \"\"\"\n",
    "    Save the configuration to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = Path(filename)\n",
    "        \n",
    "        if file_path.exists():\n",
    "            print(f'üìÑ Configuration file already exists: {filename}')\n",
    "            print(f'üí° Skipping save to avoid overwriting existing file.')\n",
    "            print(f'üîç Current configuration that would have been saved:')\n",
    "            print(json.dumps(config, indent=4))\n",
    "            return\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "        print(f'‚úÖ Configuration saved to: {filename}')\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error saving configuration: {str(e)}')\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"üöÄ Botify Configuration Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Load API token\n",
    "    try:\n",
    "        api_token = load_api_token()\n",
    "        print(f\"‚úÖ API token loaded from {TOKEN_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Get Botify project URL from user (with default option)\n",
    "    while True:\n",
    "        print(\"\\nüìù Please enter your Botify project URL:\")\n",
    "        print(\"Example: https://app.botify.com/your-org/your-project/\")\n",
    "        print(\"üí° Or just hit Enter to use your default (first org, first project, latest analysis)\")\n",
    "        \n",
    "        botify_url = input(\"Botify Project URL (or Enter for default): \").strip()\n",
    "        \n",
    "        # Check if user wants default configuration\n",
    "        if not botify_url:\n",
    "            print(\"\\nüéØ Using default configuration...\")\n",
    "            config = await get_default_configuration(api_token)\n",
    "            \n",
    "            if config:\n",
    "                print(\"‚úÖ Default configuration generated successfully!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"‚ùå Could not generate default configuration.\")\n",
    "                print(\"üí° Please enter a specific Botify project URL instead.\")\n",
    "                continue\n",
    "        \n",
    "        # Step 3: Validate URL\n",
    "        is_valid, message, project_data = validate_botify_url(botify_url)\n",
    "        \n",
    "        if not is_valid:\n",
    "            print(f\"‚ùå {message}\")\n",
    "            print(\"üí° Please try again or hit Enter for default configuration.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"‚úÖ {message}\")\n",
    "        \n",
    "        org = project_data['username']\n",
    "        project = project_data['project_name']\n",
    "        \n",
    "        print(f\"üè¢ Organization: {org}\")\n",
    "        print(f\"üìÅ Project: {project}\")\n",
    "        \n",
    "        # Step 4: Fetch most recent analysis\n",
    "        print(f\"\\nüîç Fetching most recent analysis for {org}/{project}...\")\n",
    "        \n",
    "        analysis_slug = await fetch_most_recent_analysis(org, project, api_token)\n",
    "        \n",
    "        if not analysis_slug:\n",
    "            print(\"‚ùå Could not fetch analysis information\")\n",
    "            print(\"üí° Please try a different URL or hit Enter for default configuration.\")\n",
    "            continue\n",
    "        \n",
    "        # Step 5: Generate configuration\n",
    "        config = generate_config(org, project, analysis_slug)\n",
    "        break\n",
    "    \n",
    "    # Display and save configuration\n",
    "    if config:\n",
    "        print(f\"\\nüìã Generated Configuration:\")\n",
    "        print(\"=\" * 30)\n",
    "        print(json.dumps(config, indent=4))\n",
    "        \n",
    "        # Step 6: Save configuration\n",
    "        print(f\"\\nüíæ Saving configuration...\")\n",
    "        save_config(config, \"config.json\")\n",
    "        \n",
    "        print(f\"\\nüéâ Configuration generation complete!\")\n",
    "        print(f\"üìÑ You can now use the generated config.json file in your projects.\")\n",
    "        \n",
    "        return config\n",
    "    else:\n",
    "        print(\"‚ùå Configuration generation failed.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# For Jupyter Notebook execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if we're in Jupyter (has get_ipython function)\n",
    "    try:\n",
    "        get_ipython()\n",
    "        print(\"üî¨ Running in Jupyter environment\")\n",
    "        # In Jupyter, use await directly\n",
    "        config = await main()\n",
    "    except NameError:\n",
    "        # Not in Jupyter, use asyncio.run()\n",
    "        print(\"üñ•Ô∏è  Running in standard Python environment\")\n",
    "        config = asyncio.run(main()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Introduction to BQL (Botify Query Language)\n",
    "\n",
    "Botify API interactions come in many shapes and forms. The example shown below is the most popular: BQLv2 (Botify Query Language V2), but there are others ‚Äî not just BQLv1 but also a vast array of *specialized endpoints* for custom reports and analysis. Of all the variations you will find, two \"endpoints\" (URLs that you make requests to) rise above all the others in their utility and frequency you'll encounter them. And they are:\n",
    "\n",
    "1. The `/query` endpoint\n",
    "2. The `/jobs` endpoint\n",
    "\n",
    "## `/query` Endpoint Example\n",
    "\n",
    "```python\n",
    "import httpx\n",
    "\n",
    "# This is set by the botify_token.txt file you just created above.\n",
    "api_key = \"your_api_key_here\"\n",
    "\n",
    "# These part are read from the config.json file you just created above.\n",
    "org = \"your_organization_slug\"\n",
    "project = \"your_project_slug\"\n",
    "analysis = \"your_analysis_slug\"\n",
    "collection = \"your_collection_name\"\n",
    "\n",
    "# This is an \"endpoint\". Endpoints are URLs that you can \"submit\" requests to.\n",
    "url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"  # <-- SEE!!! This is the `/query` endpoint!\n",
    "\n",
    "# This sets values that need to be sent WITH the request that you submit to the endpoint.\n",
    "headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# BQL query payload\n",
    "payload = {\n",
    "    \"collections\": [collection],\n",
    "    \"query\": {\n",
    "        \"dimensions\": [{\"field\": \"segments.pagetype.value\"}],\n",
    "        \"metrics\": [{\"field\": f\"{collection}.count_urls_crawl\"}],\n",
    "        \"sort\": [\n",
    "            {\"type\": \"metrics\", \"index\": 0, \"order\": \"desc\"},\n",
    "            {\"type\": \"dimensions\", \"index\": 0, \"order\": \"asc\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send POST request\n",
    "response = httpx.post(url, headers=headers, json=payload)\n",
    "response.raise_for_status()\n",
    "print(response.json())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Use API: Simplest Example\n",
    "\n",
    "Remember when I told you there are lots of specialized endpoints? Well, this is the first and easiest to have an immediate API success. If you did the above API step correctly, you should be able to get your username with this bare minimum Botify API script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "# Read only the first line (the token) from botify_token.txt\n",
    "# This handles the multi-line format: token on line 1, comment on line 2\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "\n",
    "headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "user_data = httpx.get(\"https://api.botify.com/v1/authentication/profile\", headers=headers).json()\n",
    "\n",
    "username = user_data[\"data\"][\"username\"]\n",
    "print(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**Sample Output**: \n",
    "\n",
    "    first.last\n",
    "\n",
    "**Note**: these \"sample output\" cells are here in part for the LLMs who need to know enough about the expected Botify API output in order to help you. Each code-cell is accompanies by a markdown cell that embeds sample output so the system doesn't have to rely on the actual output generated by the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# List Orgs: How To Get the List of Projects And Their Orgs Given Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "# Load API key\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "\n",
    "def get_username():\n",
    "    \"\"\"Fetch the username associated with the API key.\"\"\"\n",
    "    try:\n",
    "        response = httpx.get(\"https://api.botify.com/v1/authentication/profile\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"data\"][\"username\"]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching username: {e}\")\n",
    "\n",
    "def fetch_projects(username):\n",
    "    \"\"\"Fetch all projects for a given username from Botify API.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{username}\"\n",
    "    projects = []\n",
    "    try:\n",
    "        while url:\n",
    "            response = httpx.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            projects.extend(\n",
    "                (p['name'], p['slug'], p['user']['login']) for p in data.get('results', [])\n",
    "            )\n",
    "            url = data.get('next')\n",
    "        return sorted(projects)\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching projects for {username}: {e}\")\n",
    "        return []\n",
    "\n",
    "username = get_username()\n",
    "if username:\n",
    "    projects = fetch_projects(username)\n",
    "    print(f\"{'Project Name':<30} {'Project Slug':<35} {'User or Org':<15}\")\n",
    "    print(\"=\" * 80)\n",
    "    for name, slug, user in projects:\n",
    "        print(f\"{name:<30} {slug:<35} {user:<15}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve username or projects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "Username: first.last\n",
    "Project Name                   Project Slug                        User or Org    \n",
    "================================================================================\n",
    "Foo Test                       foo.com                             first.last       \n",
    "Bar Test                       bar.com                             bar-org       \n",
    "Baz Test                       baz.com                             baz-org       \n",
    "```\n",
    "\n",
    "**Rationale**: You need an Organization slug (**org**) for these exercises. It goes in your **config.json** to get started. Your personal login username will usually be used for one Project, but then an offical ***org slug*** (aka group) will usually appear on the others. By convention, these values often end with `-org`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# List Projects: How To Get the List of Projects Given an Organization\n",
    "\n",
    "Note: \n",
    "- If you're running this in VSCode or Cursor IDE, the `config.json` file should be in the root directory of your project.\n",
    "- If you're running this in Jupyter Notebook, the `config.json` file should be in the same directory as this notebook.\n",
    "\n",
    "Your config.json should look like:\n",
    "```json\n",
    "{\n",
    "    \"org\": \"org_name\",\n",
    "    \"project\": \"project_name\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "\n",
    "def fetch_projects(org):\n",
    "    \"\"\"Fetch all projects for a given organization from Botify API.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    projects = []\n",
    "    \n",
    "    try:\n",
    "        while url:\n",
    "            response = httpx.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            projects.extend((p['name'], p['slug'], p['user']['login']) for p in data.get('results', []))\n",
    "            url = data.get('next')\n",
    "        return sorted(projects)\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching projects: {e}\")\n",
    "        return []\n",
    "\n",
    "projects = fetch_projects(org)\n",
    "\n",
    "print(f\"{'Project Name':<30} {'Project Slug':<35} {'Login':<15}\")\n",
    "print(\"=\" * 80)\n",
    "for name, slug, user in projects:\n",
    "    print(f\"{name:<30} {slug:<35} {user:<15}\")\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "Organization: foo-bar\n",
    "Project Name                   Project Slug                        Login     \n",
    "================================================================================\n",
    "Legendary Product Vault        legendary-product-vault             foo-org       \n",
    "Hidden Content Cove            hidden-content-cove                 foo-org       \n",
    "Fabled Catalog of Curiosities  fabled-catalog-of-curiosities       foo-org       \n",
    "```\n",
    "\n",
    "**Rationale**: Next, you need Project slugs for these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# List Analyses: How To Get the List of Analysis Slugs Given a Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org, project = config['org'], config['project']\n",
    "\n",
    "def fetch_analyses(org, project):\n",
    "    \"\"\"Fetch analysis slugs for a given project from Botify API.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/analyses/{org}/{project}/light\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    slugs = []\n",
    "    \n",
    "    try:\n",
    "        while url:\n",
    "            response = httpx.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            slugs.extend(a['slug'] for a in data.get('results', []))\n",
    "            url = data.get('next')\n",
    "        return slugs\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching analyses: {e}\")\n",
    "        return []\n",
    "\n",
    "# Output analysis slugs\n",
    "for slug in fetch_analyses(org, project):\n",
    "    print(slug)\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "20240301\n",
    "20240201\n",
    "20240101-2\n",
    "20240101\n",
    "20231201\n",
    "20231101\n",
    "```\n",
    "\n",
    "**Rationale**: Analysis slugs are dates in YYYYMMDD format but sometimes get incremeted with `-n` extensions starting with `-2`. They're the third thing you typically need in **config.json** for these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# List URLs: How To Get a List of the First 500 URLs\n",
    "\n",
    "**Important**: For this step to work, you need to have an `analysis` value set in your `config.json` file. \n",
    "\n",
    "Your `config.json` should include:\n",
    "```json\n",
    "{\n",
    "    \"org\": \"your-organization\",\n",
    "    \"project\": \"your-project-slug\",\n",
    "    \"analysis\": \"your-analysis-slug\"\n",
    "}\n",
    "```\n",
    "\n",
    "The analysis slug is typically a date in YYYYMMDD format (like \"20240301\") as shown in the sample output below. Without this value in your config file, you'll encounter a KeyError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis, api_key):\n",
    "    \"\"\"Fetch data based on BQLv2 query for a specific Botify analysis.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # BQLv2 query payload\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\"\n",
    "            ],\n",
    "            \"metrics\": []  # Don't come crying to me when you delete this and it stops working.\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Send the request\n",
    "    response = httpx.post(url, headers=headers, json=data_payload, timeout=60.0)\n",
    "    response.raise_for_status()  # Check for errors\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Run the query and load results\n",
    "data = get_bqlv2_data(org, project, analysis, api_key)\n",
    "\n",
    "list_of_urls = [url['dimensions'][0] for url in data['results']]\n",
    "\n",
    "for i, url in enumerate(list_of_urls):\n",
    "    print(i + 1, url)\n",
    "    if i >= 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "1 https://example.com/page1\n",
    "2 https://example.com/page2\n",
    "3 https://example.com/page3\n",
    "4 https://example.com/page4\n",
    "5 https://example.com/page5\n",
    "6 https://example.com/page6\n",
    "7 https://example.com/page7\n",
    "8 https://example.com/page8\n",
    "9 https://example.com/page9\n",
    "10 https://example.com/page10\n",
    "```\n",
    "\n",
    "**Rationale**: To explicitly tell you that you have to leave the `metrics\": []` field in this example even though it's empty. Don't believe me? Try it. Ugh! Also, I'm not here to teach you Python, but it's worth noting:\n",
    "\n",
    "- `enumerate()` exposes the internal counter index.\n",
    "- Python uses zero-based indexes, thus the `+1` for humans and `>= 9` to cut off at 10.\n",
    "- The `print()` function takes multiple (un-labeled) inputs‚Äîcounter & url in this case.\n",
    "- The other way to use the counter & url together is ***f-strings***: `f\"{i+1} {url}\"`, which would also work.\n",
    "\n",
    "You're welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# List SEO Fields: How To Get a List of the First 500 URLs, Titles, Meta Descriptions and H1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis, api_key):\n",
    "    \"\"\"Fetch data for URLs with title, meta description, and H1 fields.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # BQLv2 query payload\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\",\n",
    "                f\"crawl.{analysis}.metadata.title.content\",\n",
    "                f\"crawl.{analysis}.metadata.description.content\",\n",
    "                f\"crawl.{analysis}.metadata.h1.contents\"\n",
    "            ],\n",
    "            \"metrics\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Send the request with a timeout of 60 seconds\n",
    "    response = httpx.post(url, headers=headers, json=data_payload, timeout=60)\n",
    "    response.raise_for_status()  # Check for errors\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Run the query and load results\n",
    "data = get_bqlv2_data(org, project, analysis, api_key)\n",
    "\n",
    "# Flatten the data into a DataFrame\n",
    "columns = [\"url\", \"title\", \"meta_description\", \"h1\"]\n",
    "df = pd.DataFrame([item['dimensions'] for item in data['results']], columns=columns)\n",
    "\n",
    "# Display the first 500 URLs\n",
    "df.head(500).to_csv(\"first_500_urls.csv\", index=False)\n",
    "print(\"Data saved to first_500_urls.csv\")\n",
    "\n",
    "# Show a preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "\n",
    "| url                              | title               | meta_description                           | h1                 |\n",
    "|----------------------------------|---------------------|--------------------------------------------|---------------------|\n",
    "| https://example.com/foo          | Foo Title          | This is a description of Foo.              | Foo Heading        |\n",
    "| https://example.com/bar          | Bar Overview       | Bar is a collection of great resources.    | Bar Insights       |\n",
    "| https://example.com/baz          | Baz Guide          | Learn all about Baz and its applications.  | Baz Essentials     |\n",
    "\n",
    "...\n",
    "\n",
    "*Data saved to `first_500_urls.csv`*\n",
    "\n",
    "**Rationale**: To show you the main endpoint for listing 500 lines at a time, paging and quick aggregate queries. To show you how `org` and `project` are in the url (so you notice them disappearing later when we export csv downloads). To introduce the infinitely popular and useful `pandas` data library for manipulating ***row & column*** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "def format_analysis_date(analysis_slug):\n",
    "    \"\"\"Convert analysis slug (e.g. '20241108' or '20241108-2') to YYYY-MM-DD format.\"\"\"\n",
    "    # Strip any suffix after hyphen\n",
    "    base_date = analysis_slug.split('-')[0]\n",
    "    \n",
    "    # Insert hyphens for YYYY-MM-DD format\n",
    "    return f\"{base_date[:4]}-{base_date[4:6]}-{base_date[6:8]}\"\n",
    "\n",
    "def get_previous_analysis(org, project, current_analysis, api_key):\n",
    "    \"\"\"Get the analysis slug immediately prior to the given one.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/analyses/{org}/{project}/light\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        analyses = response.json().get('results', [])\n",
    "        \n",
    "        # Get base date without suffix\n",
    "        current_base = current_analysis.split('-')[0]\n",
    "        \n",
    "        # Find the first analysis that's before our current one\n",
    "        for analysis in analyses:\n",
    "            slug = analysis['slug']\n",
    "            base_slug = slug.split('-')[0]\n",
    "            if base_slug < current_base:\n",
    "                return slug\n",
    "                \n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching analyses: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis, api_key):\n",
    "    \"\"\"Fetch data for URLs with titles and search console metrics, sorted by impressions.\"\"\"\n",
    "    # Get date range for search console data\n",
    "    end_date = format_analysis_date(analysis)\n",
    "    prev_analysis = get_previous_analysis(org, project, analysis, api_key)\n",
    "    if prev_analysis:\n",
    "        start_date = format_analysis_date(prev_analysis)\n",
    "    else:\n",
    "        # Fallback to 7 days before if no previous analysis found\n",
    "        start_date = end_date  # You may want to subtract 7 days here\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"collections\": [\n",
    "            f\"crawl.{analysis}\",\n",
    "            \"search_console\"\n",
    "        ],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\",\n",
    "                f\"crawl.{analysis}.metadata.title.content\"\n",
    "            ],\n",
    "            \"metrics\": [\n",
    "                \"search_console.period_0.count_impressions\",\n",
    "                \"search_console.period_0.count_clicks\"\n",
    "            ],\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": \"search_console.period_0.count_impressions\",\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"periods\": [\n",
    "            [start_date, end_date]\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    response = httpx.post(url, headers=headers, json=data_payload, timeout=60.0)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Run the query and load results\n",
    "data = get_bqlv2_data(org, project, analysis, api_key)\n",
    "\n",
    "# Flatten the data into a DataFrame with URL, title, and search console metrics\n",
    "columns = [\"url\", \"title\", \"impressions\", \"clicks\"]\n",
    "df = pd.DataFrame([\n",
    "    item['dimensions'] + item['metrics'] \n",
    "    for item in data['results']\n",
    "], columns=columns)\n",
    "\n",
    "# Display the first 500 URLs and titles\n",
    "df.head(500).to_csv(\"first_500_urls_titles.csv\", index=False)\n",
    "print(\"Data saved to first_500_urls_titles.csv\")\n",
    "\n",
    "# Show a preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "| | url                               | title              | impressions | clicks |\n",
    "|-|-----------------------------------|--------------------|-------------|--------|\n",
    "|0| https://example.com/foo           | Foo Page Title    | 1200        | 35     |\n",
    "|1| https://example.com/bar           | Bar Page Title    | 1150        | 40     |\n",
    "|2| https://example.com/baz           | Baz Page Title    | 980         | 25     |\n",
    "\n",
    "\n",
    "**Rationale**: So that I can jump up and down screaming that BQL is not SQL and tell the LLMs to stop showing me SQL examples for BQL. Surely SQL is down there somewhere, but it's ***API-wrapped***. Though this does not spare us from some SQL methodology. For example, table-joins across Collections are a thing‚Äîdemonstrated here as `search_console` joined with `crawl.YYMMDD`, left-outer if I'm reading it correctly (I may have to amend that). If you really wanna know, Collections are table aliases that help with the API-wrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# # Query Segments: How to Get Pagetype Segment Data for a Project With URL Counts\n",
    "\n",
    "This query requires the \"collection\" field in your config.json file, in addition to \"org\", \"project\", and \"analysis\".\n",
    "Example config.json:\n",
    "```json\n",
    "{\n",
    "    \"org\": \"org_name\",\n",
    "    \"project\": \"project_name\",\n",
    "    \"analysis\": \"20241230\",\n",
    "    \"collection\": \"crawl.20241230\"\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "\n",
    "# Load configuration values from config.json\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Extract configuration details\n",
    "org = config[\"org\"]\n",
    "project = config[\"project\"]\n",
    "analysis = config[\"analysis\"]\n",
    "collection = config[\"collection\"]\n",
    "\n",
    "# Load the API key from botify_token.txt\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "\n",
    "# Define the URL for the API request\n",
    "url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "\n",
    "# Set headers for authorization and content type\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the payload for the API query\n",
    "payload = {\n",
    "    \"collections\": [collection],\n",
    "    \"query\": {\n",
    "        \"dimensions\": [{\"field\": \"segments.pagetype.value\"}],\n",
    "        \"metrics\": [{\"field\": f\"{collection}.count_urls_crawl\"}],\n",
    "        \"sort\": [\n",
    "            {\"type\": \"metrics\", \"index\": 0, \"order\": \"desc\"},\n",
    "            {\"type\": \"dimensions\", \"index\": 0, \"order\": \"asc\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the POST request to the API\n",
    "response = httpx.post(url, headers=headers, json=payload, timeout=60)\n",
    "response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "# Get the results from the response JSON\n",
    "results = response.json()\n",
    "\n",
    "# Use json.dumps with separators and indent for compact pretty printing\n",
    "print(json.dumps(results, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"results\": [\n",
    "        {\n",
    "            \"dimensions\": [\"pdp\"],\n",
    "            \"metrics\": [82150]\n",
    "        },\n",
    "        {\n",
    "            \"dimensions\": [\"plp\"],\n",
    "            \"metrics\": [53400]\n",
    "        },\n",
    "        {\n",
    "            \"dimensions\": [\"category\"],\n",
    "            \"metrics\": [44420]\n",
    "        },\n",
    "        [...]\n",
    "    ],\n",
    "    \"previous\": null,\n",
    "    \"next\": \"https://api.botify.com/v1/org/project/query?page=1\",\n",
    "    \"page\": 1,\n",
    "    \"size\": 10\n",
    "}\n",
    "```\n",
    "\n",
    "**Rationale**: To give you an example that uses dimensions, metrics and sorting all at once. Also to show you the `page` parameter on the querystring making you think it's the **GET method**, `org` & `project` arguments posing as folders, and finally a JSON `payload` showing you it's actually using the **POST method**. Ahhh, *gotta love the Botify API*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "# List Collections: How To Get the List of Collections Given a Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of Collections Given a Project\n",
    "\n",
    "import json\n",
    "import httpx\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "\n",
    "def fetch_collections(org, project, api_key):\n",
    "    \"\"\"Fetch collection IDs for a given project from the Botify API.\"\"\"\n",
    "    collections_url = f\"https://api.botify.com/v1/projects/{org}/{project}/collections\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(collections_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        collections_data = response.json()\n",
    "        return [\n",
    "            (collection['id'], collection['name']) for collection in collections_data\n",
    "        ]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching collections for project '{project}': {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Fetch collections\n",
    "collections = fetch_collections(org, project, api_key)\n",
    "for collection_id, collection_name in collections:\n",
    "    print(f\"ID: {collection_id}, Name: {collection_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "ID: crawl.20240917, Name: 2024 Sept. 17th\n",
    "ID: actionboard_ml.20240917, Name: ActionBoard ML\n",
    "ID: crawl.20240715, Name: 2024 July 15th\n",
    "ID: search_engines_orphans.20240715, Name: Search Engines Orphans\n",
    "```\n",
    "\n",
    "**Rationale**: To let you know how tough Collections are once you start digging in. The first challenge is simply knowing what collections you have and what you can do with them‚Äîthough 9 out of 10 times it's `crawl.YYYYMMDD` and `search_console`. If not, come talk to me, I wanna pick your brain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "# List Fields: How To Get The List of Fields Given a Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of Fields Given a Collection\n",
    "\n",
    "import json\n",
    "import httpx\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "collection = config['collection']\n",
    "\n",
    "def fetch_fields(org, project, collection, api_key):\n",
    "    \"\"\"Fetch available fields for a given collection from the Botify API.\"\"\"\n",
    "    fields_url = f\"https://api.botify.com/v1/projects/{org}/{project}/collections/{collection}\"\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(fields_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        fields_data = response.json()\n",
    "        return [\n",
    "            (field['id'], field['name'])\n",
    "            for dataset in fields_data.get('datasets', [])\n",
    "            for field in dataset.get('fields', [])\n",
    "        ]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching fields for collection '{collection}' in project '{project}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch and print fields\n",
    "fields = fetch_fields(org, project, collection, api_key)\n",
    "print(f\"Fields for collection '{collection}':\")\n",
    "for field_id, field_name in fields:\n",
    "    print(f\"ID: {field_id}, Name: {field_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "Fields for collection 'crawl.20241101':\n",
    "ID: field_of_vision, Name: Survey the Landscape\n",
    "ID: field_of_dreams, Name: The Mind's Eye\n",
    "ID: straying_far_afield, Name: Go Home Spiderman\n",
    "ID: afield_a_complaint, Name: Red Swingline\n",
    "```\n",
    "\n",
    "**Rationale**: So you've got a collection and have no idea what to do with it? Well, you can always start by listing its fields. Yeah, let's list the fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# Get Pagetypes: How To Get the Unfiltered URL Counts by Pagetype for a Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "website = config['project']  # Assuming 'project' here refers to the website\n",
    "analysis_date = config['analysis']  # Date of analysis, formatted as 'YYYYMMDD'\n",
    "\n",
    "def get_first_page_pagetype_url_counts(org, website, analysis_date, api_key, size=5000):\n",
    "    \"\"\"Fetch pagetype segmentation counts for the first page only, sorted by URL count in descending order.\"\"\"\n",
    "    url = f\"https://app.botify.com/api/v1/projects/{org}/{website}/query?size={size}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Token {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-botify-client\": \"spa\",\n",
    "    }\n",
    "    \n",
    "    # Payload to retrieve pagetype segmentation counts, ordered by URL count\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis_date}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [{\"field\": \"segments.pagetype.value\"}],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis_date}.count_urls_crawl\"}],\n",
    "            \"sort\": [\n",
    "                {\"type\": \"metrics\", \"index\": 0, \"order\": \"desc\"},\n",
    "                {\"type\": \"dimensions\", \"index\": 0, \"order\": \"asc\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Make a single request for the first page of results\n",
    "        response = httpx.post(url, headers=headers, json=data_payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Process the first page of results\n",
    "        results = []\n",
    "        for item in data.get(\"results\", []):\n",
    "            pagetype = item[\"dimensions\"][0] if item[\"dimensions\"] else \"Unknown\"\n",
    "            count = item[\"metrics\"][0] if item[\"metrics\"] else 0\n",
    "            results.append({\"Pagetype\": pagetype, \"URL Count\": count})\n",
    "    \n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching pagetype URL counts: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Fetch pagetype URL counts for the first page only\n",
    "results = get_first_page_pagetype_url_counts(org, website, analysis_date, api_key)\n",
    "\n",
    "# Convert results to a DataFrame and save\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"pagetype_url_counts.csv\", index=False)\n",
    "print(\"Data saved to pagetype_url_counts.csv\")\n",
    "\n",
    "# Display a preview\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```\n",
    "Data saved to pagetype_url_counts.csv\n",
    "                 Pagetype  URL Count\n",
    "0                    pdp     250000\n",
    "1                    plp      50000\n",
    "2                    hub       5000\n",
    "3                   blog       2500\n",
    "4                    faq        500\n",
    "```\n",
    "\n",
    "**Rationale**: Do you ever get the feeling a website's folder-structure can tell you something about how it's organized? Yeah, me too. Thankfully, we here at Botify do the ***Regular Expressions*** so you don't have to. And it makes really great great color-coding in the link-graph visualizations. Psst! Wanna see the Death Star?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Get Short Titles: How To Get the First 500 URLs With Short Titles Given Pagetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the First 500 URLs With Short Titles Given Pagetype\n",
    "\n",
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis, api_key):\n",
    "    \"\"\"Fetch data based on BQLv2 query for a specific Botify analysis.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # BQLv2 query payload\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\",\n",
    "                f\"crawl.{analysis}.metadata.title.len\",\n",
    "                f\"crawl.{analysis}.metadata.title.content\",\n",
    "                f\"crawl.{analysis}.metadata.title.quality\",\n",
    "                f\"crawl.{analysis}.metadata.description.content\",\n",
    "                f\"crawl.{analysis}.metadata.structured.breadcrumb.tree\",\n",
    "                f\"crawl.{analysis}.metadata.h1.contents\",\n",
    "                f\"crawl.{analysis}.metadata.h2.contents\"\n",
    "            ],\n",
    "            \"metrics\": [],\n",
    "            \"filters\": {\n",
    "                \"and\": [\n",
    "                    {\n",
    "                        \"field\": f\"crawl.{analysis}.scoring.issues.title_len\",\n",
    "                        \"predicate\": \"eq\",\n",
    "                        \"value\": True\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": f\"crawl.{analysis}.segments.pagetype.depth_1\",\n",
    "                        \"value\": \"pdp\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": f\"crawl.{analysis}.metadata.title.len\",\n",
    "                    \"order\": \"asc\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Token {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Send the request\n",
    "    # Allow up to a minute for the API to respond\n",
    "    response = httpx.post(url, headers=headers, json=data_payload, timeout=60.0)\n",
    "    response.raise_for_status()  # Check for errors\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Run the query and load results\n",
    "data = get_bqlv2_data(org, project, analysis, api_key)\n",
    "\n",
    "# Flatten the data into a DataFrame\n",
    "# Define column names for each dimension in the data\n",
    "columns = [\n",
    "    \"url\", \"title_len\", \"title_content\", \"title_quality\",\n",
    "    \"description_content\", \"breadcrumb_tree\", \"h1_contents\", \"h2_contents\"\n",
    "]\n",
    "df = pd.DataFrame([item['dimensions'] for item in data['results']], columns=columns)\n",
    "\n",
    "print(\"Data saved to titles_too_short.csv\")\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display all rows \n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Increase column width to avoid truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "**Data saved to titles_too_short.csv**\n",
    "\n",
    "| url                                      | title_len | title_content     | title_quality | description_content                                              | breadcrumb_tree                                       | h1_contents                     | h2_contents                             |\n",
    "|------------------------------------------|-----------|-------------------|---------------|------------------------------------------------------------------|--------------------------------------------------------|----------------------------------|-----------------------------------------|\n",
    "| https://www.example.com/site/socks/12345 | 8         | Sock Store        | unique        | Best socks for every season, unbeatable comfort and style.      | Example Store/Footwear/Accessories/Socks               | [Our Socks]                     | [Features, Sizes, Related Items]        |\n",
    "| https://www.example.com/site/hats/98765  | 9         | Top Hats Here!    | duplicate     | Stylish hats available year-round with exclusive discounts.     | Example Store/Apparel/Accessories/Hats                 | [Hat Selection]                 | [Details, Reviews, Top Picks]           |\n",
    "| https://www.example.com/site/shirts/54321| 10        | - Shirt Emporium  | duplicate     | Discover comfortable and stylish shirts at great prices.        | Example Store/Apparel/Topwear/Shirts                   | [Shirt Central]                 | [Sizing, Similar Styles, Reviews]       |\n",
    "\n",
    "...\n",
    "\n",
    "**Rationale**: Ahh, ***title tags***. They show in browser bookmarks, tabs and SERPs‚Äîthe only relevancy factor that will remain standing after SEO Armageddon. You could ditch every other factor but ***anchor text***, set your uber-crawler go off-site, use a click-depth of 4‚Äîand harvest yourself a pretty good link-graph of the entire Internet... were it not for spammers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Count Short Titles: How To Count Number of URLs Having Short Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "def count_short_titles(org, project, analysis, api_key):\n",
    "    \"\"\"Count URLs with short titles for a specific Botify analysis.\"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Token {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Data payload for the count of URLs with short titles\n",
    "    data_payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [],  # No dimensions needed, as we only need a count\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"function\": \"count\",\n",
    "                    \"args\": [f\"crawl.{analysis}.url\"]\n",
    "                }\n",
    "            ],\n",
    "            \"filters\": {\n",
    "                \"field\": f\"crawl.{analysis}.scoring.issues.title_len\",\n",
    "                \"predicate\": \"eq\",\n",
    "                \"value\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send the request\n",
    "        response = httpx.post(url, headers=headers, json=data_payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract the count of URLs with short titles\n",
    "        short_title_count = data[\"results\"][0][\"metrics\"][0]\n",
    "        return short_title_count\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching short title count for analysis '{analysis}' in project '{project}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the count query and display the result\n",
    "short_title_count = count_short_titles(org, project, analysis, api_key)\n",
    "\n",
    "if short_title_count is not None:\n",
    "    print(f\"Number of URLs with short titles: {short_title_count:,}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the count of URLs with short titles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Number of URLs with short titles: 675,080\n",
    "\n",
    "**Rationale**: Sometimes ya gotta count what you're trying to get before you go try and download it. Plus, learn ***filtering*** in the Botify API! But I think really I just wanted to show you how easy it is to format `f\"{big_numbers:,}\"` with commas using ***f-strings*** (I'm talking to you humans‚Äîbecause the LLMs *already know*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "# Download CSV: How To Download Up to 10K URLs Having Short Titles As a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import time\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "\n",
    "def start_export_job_for_short_titles(org, project, analysis, api_key):\n",
    "    \"\"\"Start an export job for URLs with short titles, downloading key metadata fields.\"\"\"\n",
    "    url = \"https://api.botify.com/v1/jobs\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Token {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Data payload for the export job with necessary fields\n",
    "    data_payload = {\n",
    "        \"job_type\": \"export\",\n",
    "        \"payload\": {\n",
    "            \"username\": org,\n",
    "            \"project\": project,\n",
    "            \"connector\": \"direct_download\",\n",
    "            \"formatter\": \"csv\",\n",
    "            \"export_size\": 10000,  # Adjust as needed\n",
    "            \"query\": {\n",
    "                \"collections\": [f\"crawl.{analysis}\"],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [\n",
    "                        f\"crawl.{analysis}.url\",\n",
    "                        f\"crawl.{analysis}.metadata.title.len\",\n",
    "                        f\"crawl.{analysis}.metadata.title.content\",\n",
    "                        f\"crawl.{analysis}.metadata.title.quality\",\n",
    "                        f\"crawl.{analysis}.metadata.description.content\",\n",
    "                        f\"crawl.{analysis}.metadata.h1.contents\"\n",
    "                    ],\n",
    "                    \"metrics\": [],\n",
    "                    \"filters\": {\n",
    "                        \"field\": f\"crawl.{analysis}.scoring.issues.title_len\",\n",
    "                        \"predicate\": \"eq\",\n",
    "                        \"value\": True\n",
    "                    },\n",
    "                    \"sort\": [\n",
    "                        {\n",
    "                            \"field\": f\"crawl.{analysis}.metadata.title.len\",\n",
    "                            \"order\": \"asc\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Starting export job for short titles...\")\n",
    "        # Use a longer timeout to prevent ReadTimeout errors\n",
    "        response = httpx.post(url, headers=headers, json=data_payload, timeout=300.0)\n",
    "        response.raise_for_status()\n",
    "        export_job_details = response.json()\n",
    "        \n",
    "        # Extract job URL for polling\n",
    "        job_url = f\"https://api.botify.com{export_job_details.get('job_url')}\"\n",
    "        \n",
    "        # Polling for job completion\n",
    "        print(\"Polling for job completion:\", end=\" \")\n",
    "        while True:\n",
    "            time.sleep(5)\n",
    "            # Use a longer timeout for polling requests as well\n",
    "            poll_response = httpx.get(job_url, headers=headers, timeout=120.0)\n",
    "            poll_response.raise_for_status()\n",
    "            job_status_details = poll_response.json()\n",
    "            \n",
    "            if job_status_details[\"job_status\"] == \"DONE\":\n",
    "                download_url = job_status_details[\"results\"][\"download_url\"]\n",
    "                print(\"\\nDownload URL:\", download_url)\n",
    "                return download_url\n",
    "            elif job_status_details[\"job_status\"] == \"FAILED\":\n",
    "                print(\"\\nJob failed. Error details:\", job_status_details)\n",
    "                return None\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "    except httpx.ReadTimeout as e:\n",
    "        print(f\"\\nTimeout error during export job: {e}\")\n",
    "        print(\"The API request timed out. Consider increasing the timeout value or try again later.\")\n",
    "        return None\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"\\nError starting or polling export job: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Start export job and get download URL\n",
    "download_url = start_export_job_for_short_titles(org, project, analysis, api_key)\n",
    "\n",
    "# Download and decompress the file if the download URL is available\n",
    "if download_url:\n",
    "    gz_filename = \"short_titles_export.csv.gz\"\n",
    "    csv_filename = \"short_titles_export.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Download the gzipped CSV file with increased timeout\n",
    "        response = httpx.get(download_url, timeout=300.0)\n",
    "        with open(gz_filename, \"wb\") as gz_file:\n",
    "            gz_file.write(response.content)\n",
    "        print(f\"File downloaded as '{gz_filename}'\")\n",
    "        \n",
    "        # Step 2: Decompress the .gz file to .csv\n",
    "        with gzip.open(gz_filename, \"rb\") as f_in:\n",
    "            with open(csv_filename, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"File decompressed and saved as '{csv_filename}'\")\n",
    "    except httpx.ReadTimeout as e:\n",
    "        print(f\"Timeout error during file download: {e}\")\n",
    "        print(\"The download request timed out. Try again later or download manually from the URL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during file download or decompression: {e}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the download URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Starting export job for short titles...  \n",
    "    Polling for job completion: .  \n",
    "    Download URL: https://cdn.example.com/export_data/abc/def/ghi/xyz1234567890/funfetti-2024-11-08.csv.gz  \n",
    "    File downloaded as 'short_titles_export.csv.gz'  \n",
    "    File decompressed and saved as 'short_titles_export.csv'  \n",
    "\n",
    "**Rationale**: Is it pulling or pooling? I could never remember. In either case, exporting and downloading csv-files is not as straightforward as you think. First, you make the request. Then you look for ***where*** to check progress, then keep re-checking until done. Then you sacrifice a chicken to help you debug useless errors. Lastly, you notice how your endpoint has changed to`https://api.botify.com/v1/jobs` with `org` and `project` moved into the JSON payload. Or is that firstly? Yeah, definitely firstly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# Get Total Count: How To Get Aggregate Count of All URLs Crawled During Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Load necessary details from files (adjust paths if needed)\n",
    "# Basic error handling for loading is good practice even in tutorials\n",
    "try:\n",
    "    config = json.load(open(\"config.json\"))\n",
    "    api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "    # Get specific config values needed\n",
    "    ORG_SLUG = config['org']\n",
    "    PROJECT_SLUG = config['project']\n",
    "    ANALYSIS_SLUG = config['analysis']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.json' or 'botify_token.txt' not found.\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Key '{e}' not found in 'config.json'.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    exit()\n",
    "\n",
    "# TARGET_MAX_DEPTH is no longer needed for this calculation\n",
    "\n",
    "# --- 2. Function to Get URL Counts Per Depth ---\n",
    "def get_all_urls_by_depth(org, project, analysis, key):\n",
    "    \"\"\"\n",
    "    Fetches URL counts for ALL depths from the Botify API.\n",
    "    Returns a dictionary {depth: count} or None on error.\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\"Authorization\": f\"Token {key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # This payload asks for counts grouped by depth, for the entire crawl\n",
    "    payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [f\"crawl.{analysis}.depth\"],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis}.count_urls_crawl\"}],\n",
    "            \"sort\": [{\"field\": f\"crawl.{analysis}.depth\", \"order\": \"asc\"}]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Requesting total URL count of site from Botify API...\")\n",
    "        response = httpx.post(api_url, headers=headers, json=payload, timeout=120.0)\n",
    "        response.raise_for_status() # Check for HTTP errors (like 4xx, 5xx)\n",
    "        print(\"Data received successfully.\")\n",
    "\n",
    "        # Convert the response list into a more usable {depth: count} dictionary\n",
    "        results = response.json().get(\"results\", [])\n",
    "        depth_counts = {\n",
    "            row[\"dimensions\"][0]: row[\"metrics\"][0]\n",
    "            for row in results\n",
    "            if \"dimensions\" in row and len(row[\"dimensions\"]) > 0 and \\\n",
    "               \"metrics\" in row and len(row[\"metrics\"]) > 0 and \\\n",
    "               isinstance(row[\"dimensions\"][0], int) # Ensure depth is an int\n",
    "        }\n",
    "        return depth_counts\n",
    "\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"API Error: {e.response.status_code} - {e.response.text}\")\n",
    "        return None # Indicate failure\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during API call or processing: {e}\")\n",
    "        return None # Indicate failure\n",
    "\n",
    "# --- 3. Main Calculation (Grand Total) ---\n",
    "# Get the depth data using the function\n",
    "all_depth_data = get_all_urls_by_depth(ORG_SLUG, PROJECT_SLUG, ANALYSIS_SLUG, api_key)\n",
    "\n",
    "# Proceed only if we got data back\n",
    "if all_depth_data is not None:\n",
    "    # Calculate the grand total by summing all counts\n",
    "    grand_total_urls = 0\n",
    "    print(f\"\\nCalculating the grand total number of URLs from all depths...\")\n",
    "\n",
    "    # Loop through all counts returned in the dictionary values and add them up\n",
    "    for count in all_depth_data.values():\n",
    "        grand_total_urls += count\n",
    "\n",
    "    # --- Alternatively, you could use the sum() function directly: ---\n",
    "    # grand_total_urls = sum(all_depth_data.values())\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    # Print the final result\n",
    "    print(f\"\\nResult: Grand Total URLs in Crawl = {grand_total_urls:,}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate total because API data retrieval failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "Requesting total URL count of site from Botify API...\n",
    "Data received successfully.\n",
    "\n",
    "Calculating the grand total number of URLs from all depths...\n",
    "\n",
    "Result: Grand Total URLs in Crawl = 3,000,000\n",
    "\n",
    "**Rationale**: Before doing a download of a CSV it is often worth checking if the number of rows returned will be under the 1-million row API limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "# Get Depth Count: How To Get Aggregate Count of URLs at Particular Click Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Load necessary details from files (adjust paths if needed)\n",
    "# Basic error handling for loading is good practice even in tutorials\n",
    "try:\n",
    "    config = json.load(open(\"config.json\"))\n",
    "    api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "    # Get specific config values needed\n",
    "    ORG_SLUG = config['org']\n",
    "    PROJECT_SLUG = config['project']\n",
    "    ANALYSIS_SLUG = config['analysis']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.json' or 'botify_token.txt' not found.\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Key '{e}' not found in 'config.json'.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the maximum depth for our calculation\n",
    "TARGET_MAX_DEPTH = 5\n",
    "\n",
    "# --- 2. Function to Get URL Counts Per Depth ---\n",
    "def get_all_urls_by_depth(org, project, analysis, key):\n",
    "    \"\"\"\n",
    "    Fetches URL counts for ALL depths from the Botify API.\n",
    "    Returns a dictionary {depth: count} or None on error.\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\"Authorization\": f\"Token {key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # This payload asks for counts grouped by depth, for the entire crawl\n",
    "    payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [f\"crawl.{analysis}.depth\"],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis}.count_urls_crawl\"}],\n",
    "            \"sort\": [{\"field\": f\"crawl.{analysis}.depth\", \"order\": \"asc\"}]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Requesting data from Botify API...\")\n",
    "        response = httpx.post(api_url, headers=headers, json=payload, timeout=120.0)\n",
    "        response.raise_for_status() # Check for HTTP errors (like 4xx, 5xx)\n",
    "        print(\"Data received successfully.\")\n",
    "\n",
    "        # Convert the response list into a more usable {depth: count} dictionary\n",
    "        results = response.json().get(\"results\", [])\n",
    "        depth_counts = {\n",
    "            row[\"dimensions\"][0]: row[\"metrics\"][0]\n",
    "            for row in results\n",
    "            if \"dimensions\" in row and len(row[\"dimensions\"]) > 0 and \\\n",
    "               \"metrics\" in row and len(row[\"metrics\"]) > 0 and \\\n",
    "               isinstance(row[\"dimensions\"][0], int) # Ensure depth is an int\n",
    "        }\n",
    "        return depth_counts\n",
    "\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"API Error: {e.response.status_code} - {e.response.text}\")\n",
    "        return None # Indicate failure\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during API call or processing: {e}\")\n",
    "        return None # Indicate failure\n",
    "\n",
    "# --- 3. Main Calculation ---\n",
    "# Get the depth data using the function\n",
    "all_depth_data = get_all_urls_by_depth(ORG_SLUG, PROJECT_SLUG, ANALYSIS_SLUG, api_key)\n",
    "\n",
    "# Proceed only if we got data back\n",
    "if all_depth_data is not None:\n",
    "    total_count_at_or_below_depth = 0\n",
    "    print(f\"\\nCalculating total for depth <= {TARGET_MAX_DEPTH}...\")\n",
    "\n",
    "    # Loop through the dictionary and sum counts for relevant depths\n",
    "    for depth, count in all_depth_data.items():\n",
    "        if depth <= TARGET_MAX_DEPTH:\n",
    "            total_count_at_or_below_depth += count\n",
    "\n",
    "    # Print the final result\n",
    "    print(f\"\\nResult: Total URLs at depth {TARGET_MAX_DEPTH} or less = {total_count_at_or_below_depth:,}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate total because API data retrieval failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "Attempting to get full URL distribution by depth...\n",
    "\n",
    "Calculating total URLs for depth <= 5 from received data...\n",
    "\n",
    "Total URLs at depth 5 or less: 1,500,000\n",
    "\n",
    "**Rationale**: Before doing a download of a CSV it is often worth checking if the number of rows returned will be under the 1-million row API limit. By using a depth filter, we now have the foundation for reducing depth until we get a downloadable number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "# Get Aggregates: How To Get Map of Click-Depths Aggregates Given Analysis Slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pprint # Keep pprint for the final output as in the original\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Load necessary details from files (adjust paths if needed)\n",
    "try:\n",
    "    config = json.load(open(\"config.json\"))\n",
    "    api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "    # Get specific config values needed\n",
    "    ORG_SLUG = config['org']\n",
    "    PROJECT_SLUG = config['project']\n",
    "    ANALYSIS_SLUG = config['analysis']\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.json' or 'botify_token.txt' not found.\")\n",
    "    exit(1)\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Key '{e}' not found in 'config.json'.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 2. Function to Get URL Counts Per Depth ---\n",
    "# (Kept original function name)\n",
    "def get_urls_by_depth(org, project, analysis, key):\n",
    "    \"\"\"\n",
    "    Fetches URL counts aggregated by depth from the Botify API.\n",
    "    Returns a dictionary {depth: count} or an empty {} on error.\n",
    "    (Matches original functionality return type on error)\n",
    "    \"\"\"\n",
    "    # Use clearer variable name for URL\n",
    "    api_url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\"Authorization\": f\"Token {key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Use clearer variable name for payload\n",
    "    payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [f\"crawl.{analysis}.depth\"],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis}.count_urls_crawl\"}],\n",
    "            \"sort\": [{\"field\": f\"crawl.{analysis}.depth\", \"order\": \"asc\"}]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Requesting URL counts per depth from Botify API...\")\n",
    "        # Use a longer timeout (e.g., 120 seconds)\n",
    "        response = httpx.post(api_url, headers=headers, json=payload, timeout=120.0)\n",
    "        response.raise_for_status() # Check for HTTP errors (like 4xx, 5xx)\n",
    "        print(\"Data received successfully.\")\n",
    "\n",
    "        # Convert the response list into a more usable {depth: count} dictionary\n",
    "        # Add validation during processing\n",
    "        response_data = response.json()\n",
    "        results = response_data.get(\"results\", [])\n",
    "\n",
    "        depth_distribution = {}\n",
    "        print(\"Processing API response...\")\n",
    "        for row in results:\n",
    "             # Validate structure before accessing keys/indices\n",
    "             if \"dimensions\" in row and len(row[\"dimensions\"]) == 1 and \\\n",
    "                \"metrics\" in row and len(row[\"metrics\"]) == 1:\n",
    "                 depth = row[\"dimensions\"][0]\n",
    "                 count = row[\"metrics\"][0]\n",
    "                 # Ensure depth is an integer\n",
    "                 if isinstance(depth, int):\n",
    "                     depth_distribution[depth] = count\n",
    "                 else:\n",
    "                     print(f\"Warning: Skipping row with non-integer depth: {row}\")\n",
    "             else:\n",
    "                 print(f\"Warning: Skipping row with unexpected structure: {row}\")\n",
    "\n",
    "        print(\"Processing complete.\")\n",
    "        return depth_distribution # Return the dictionary with results\n",
    "\n",
    "    # Adopt more specific error handling from the target style\n",
    "    except httpx.ReadTimeout:\n",
    "        print(\"Request timed out after 120 seconds.\")\n",
    "        return {} # Return empty dict per original functionality\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"API Error: {e.response.status_code} - {e.response.reason_phrase}\")\n",
    "        try:\n",
    "            # Attempt to show detailed API error message\n",
    "            error_details = e.response.json()\n",
    "            print(f\"Error details: {error_details}\")\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback if response is not JSON\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return {} # Return empty dict per original functionality\n",
    "    except httpx.RequestError as e:\n",
    "        # Handles other request errors like connection issues\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "        return {} # Return empty dict per original functionality\n",
    "    except (KeyError, IndexError, TypeError, json.JSONDecodeError) as e:\n",
    "         # Catch issues during JSON processing or accessing expected keys/indices\n",
    "         print(f\"Error processing API response: {e}\")\n",
    "         if 'response' in locals(): # Log raw response if available\n",
    "              print(f\"Response Text: {response.text}\")\n",
    "         return {} # Return empty dict per original functionality\n",
    "    except Exception as e:\n",
    "        # Catch-all for any other unexpected errors in this function\n",
    "        print(f\"An unexpected error occurred in get_urls_by_depth: {e}\")\n",
    "        return {} # Return empty dict per original functionality\n",
    "\n",
    "\n",
    "# --- 3. Main Execution ---\n",
    "print(\"\\nStarting script execution to get URL distribution by depth...\")\n",
    "try:\n",
    "    # Call the function using loaded config variables\n",
    "    depth_distribution_result = get_urls_by_depth(\n",
    "        ORG_SLUG, PROJECT_SLUG, ANALYSIS_SLUG, api_key\n",
    "    )\n",
    "\n",
    "    # Check if the result is not an empty dictionary\n",
    "    # An empty dict signifies an error occurred in the function OR no data found\n",
    "    if depth_distribution_result: # A non-empty dict evaluates to True\n",
    "        print(\"\\n--- URL Distribution by Depth ---\")\n",
    "        # Use pprint for formatted output as in the original script\n",
    "        pprint.pprint(depth_distribution_result, width=1)\n",
    "        print(\"---------------------------------\")\n",
    "    else:\n",
    "        # This message prints if the function returned {}\n",
    "        print(\"\\nRetrieved empty distribution. This could be due to an error (check logs above) or no URLs found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch any unexpected errors during the main execution sequence\n",
    "    print(f\"An unexpected error occurred during the main execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```plaintext\n",
    "Configuration loaded successfully.\n",
    "\n",
    "Starting script execution to get URL distribution by depth...\n",
    "Requesting URL counts per depth from Botify API...\n",
    "Data received successfully.\n",
    "Processing API response...\n",
    "Processing complete.\n",
    "\n",
    "--- URL Distribution by Depth (Scaled Generic Example) ---\n",
    "{ 0: 10,        # Start pages (kept small)\n",
    "  1: 550,\n",
    "  2: 28000,     # Rounded 5k * 5.5\n",
    "  3: 275000,    # 50k * 5.5\n",
    "  4: 825000,    # Peak (150k * 5.5)\n",
    "  5: 660000,    # Starting decline (120k * 5.5)\n",
    "  6: 440000,    # 80k * 5.5\n",
    "  7: 275000,    # 50k * 5.5\n",
    "  8: 165000,    # 30k * 5.5\n",
    "  9: 110000,    # 20k * 5.5\n",
    " 10: 55000,     # 10k * 5.5\n",
    " 11: 44000,     # 8k * 5.5\n",
    " 12: 33000,     # 6k * 5.5\n",
    " 13: 28000,     # Rounded 5k * 5.5\n",
    " 14: 22000,     # 4k * 5.5\n",
    " 15: 17000,     # Rounded 3k * 5.5\n",
    " 16: 11000,     # 2k * 5.5\n",
    " 17: 6000,      # Rounded 1k * 5.5\n",
    " 18: 3000,      # Rounded 500 * 5.5\n",
    " 19: 1100,      # 200 * 5.5\n",
    " 20: 550 }      # 100 * 5.5\n",
    "--------------------------------------------------------\n",
    "```\n",
    "\n",
    "**Rationale**: This depth distribution shows how many URLs exist at each click depth level from the homepage (hompage = depth 0). A healthy site typically has most content within 3 or 4 clicks of the homepage. Much more, and it may as well not exist. Such reports help identify potential deep crawl issues, spider-traps, and why (in addition to the infinite spam-cannon of generative AI content), brute-force crawls that *\"make a copy of the Internet\"* are all but dead. And did I mention that excessively crawl-able faceted search makes your site's link-graph look like the Death Star? Yeah, I think I did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "# Get Aggregates: How To Get Map of CUMULATIVE Click-Depths Aggregates Given Analysis Slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import pprint # Keep pprint for the final output\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Load necessary details from files (adjust paths if needed)\n",
    "try:\n",
    "    config = json.load(open(\"config.json\"))\n",
    "    api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "    # Get specific config values needed\n",
    "    ORG_SLUG = config['org']\n",
    "    PROJECT_SLUG = config['project']\n",
    "    ANALYSIS_SLUG = config['analysis']\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.json' or 'botify_token.txt' not found.\")\n",
    "    exit(1)\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Key '{e}' not found in 'config.json'.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 2. Function to Get URL Counts Per Depth ---\n",
    "# (This function remains unchanged as it fetches the base data needed)\n",
    "def get_urls_by_depth(org, project, analysis, key):\n",
    "    \"\"\"\n",
    "    Fetches URL counts aggregated by depth from the Botify API.\n",
    "    Returns a dictionary {depth: count} or an empty {} on error.\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    headers = {\"Authorization\": f\"Token {key}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"collections\": [f\"crawl.{analysis}\"],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [f\"crawl.{analysis}.depth\"],\n",
    "            \"metrics\": [{\"field\": f\"crawl.{analysis}.count_urls_crawl\"}],\n",
    "            \"sort\": [{\"field\": f\"crawl.{analysis}.depth\", \"order\": \"asc\"}]\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        print(\"Requesting URL counts per depth from Botify API...\")\n",
    "        response = httpx.post(api_url, headers=headers, json=payload, timeout=120.0)\n",
    "        response.raise_for_status()\n",
    "        print(\"Data received successfully.\")\n",
    "        response_data = response.json()\n",
    "        results = response_data.get(\"results\", [])\n",
    "        depth_distribution = {}\n",
    "        print(\"Processing API response...\")\n",
    "        for row in results:\n",
    "             if \"dimensions\" in row and len(row[\"dimensions\"]) == 1 and \\\n",
    "                \"metrics\" in row and len(row[\"metrics\"]) == 1:\n",
    "                 depth = row[\"dimensions\"][0]\n",
    "                 count = row[\"metrics\"][0]\n",
    "                 if isinstance(depth, int):\n",
    "                     depth_distribution[depth] = count\n",
    "                 else:\n",
    "                     print(f\"Warning: Skipping row with non-integer depth: {row}\")\n",
    "             else:\n",
    "                 print(f\"Warning: Skipping row with unexpected structure: {row}\")\n",
    "        print(\"Processing complete.\")\n",
    "        return depth_distribution\n",
    "    except httpx.ReadTimeout:\n",
    "        print(\"Request timed out after 120 seconds.\")\n",
    "        return {}\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"API Error: {e.response.status_code} - {e.response.reason_phrase}\")\n",
    "        try:\n",
    "            error_details = e.response.json()\n",
    "            print(f\"Error details: {error_details}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return {}\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "        return {}\n",
    "    except (KeyError, IndexError, TypeError, json.JSONDecodeError) as e:\n",
    "         print(f\"Error processing API response: {e}\")\n",
    "         if 'response' in locals():\n",
    "              print(f\"Response Text: {response.text}\")\n",
    "         return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in get_urls_by_depth: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# --- 3. Main Execution ---\n",
    "print(\"\\nStarting script execution...\")\n",
    "try:\n",
    "    # Call the function to get the dictionary {depth: count_at_depth}\n",
    "    depth_distribution_result = get_urls_by_depth(\n",
    "        ORG_SLUG, PROJECT_SLUG, ANALYSIS_SLUG, api_key\n",
    "    )\n",
    "\n",
    "    # Check if the result is not an empty dictionary (which indicates an error)\n",
    "    if depth_distribution_result: # A non-empty dict evaluates to True\n",
    "        print(\"\\nCalculating cumulative URL counts by depth...\")\n",
    "\n",
    "        # --- Calculate Cumulative Distribution ---\n",
    "        cumulative_depth_distribution = {}\n",
    "        current_cumulative_sum = 0\n",
    "        # Get the depths present and sort them to process in order (0, 1, 2...)\n",
    "        # Handle case where result might be empty just in case, though checked above\n",
    "        sorted_depths = sorted(depth_distribution_result.keys())\n",
    "        max_depth_found = sorted_depths[-1] if sorted_depths else -1\n",
    "\n",
    "        # Iterate from depth 0 up to the maximum depth found in the results\n",
    "        for depth_level in range(max_depth_found + 1):\n",
    "            # Get the count for this specific depth_level from the original results.\n",
    "            # Use .get(depth, 0) in case a depth level has no URLs (e.g., depth 0 might be missing if start page redirected)\n",
    "            count_at_this_level = depth_distribution_result.get(depth_level, 0)\n",
    "\n",
    "            # Add this level's count to the running cumulative sum\n",
    "            current_cumulative_sum += count_at_this_level\n",
    "\n",
    "            # Store the *cumulative* sum in the new dictionary for this depth level\n",
    "            cumulative_depth_distribution[depth_level] = current_cumulative_sum\n",
    "        # --- End Calculation ---\n",
    "\n",
    "        print(\"\\n--- Cumulative URL Distribution by Depth (URLs <= Depth) ---\")\n",
    "        # Use pprint for formatted output of the NEW cumulative dictionary\n",
    "        pprint.pprint(cumulative_depth_distribution, width=1)\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "    else:\n",
    "        # This message prints if the function returned {}\n",
    "        print(\"\\nRetrieved empty distribution. Cannot calculate cumulative counts.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch any unexpected errors during the main execution sequence\n",
    "    print(f\"An unexpected error occurred during the main execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "```plaintext\n",
    "Configuration loaded successfully.\n",
    "\n",
    "Starting script execution...\n",
    "Requesting URL counts per depth from Botify API...\n",
    "Data received successfully.\n",
    "Processing API response...\n",
    "Processing complete.\n",
    "\n",
    "Calculating cumulative URL counts by depth...\n",
    "\n",
    "--- Cumulative URL Distribution by Depth (Scaled Generic Example) ---\n",
    "{ 0: 10,\n",
    "  1: 560,          # (10 + 550)\n",
    "  2: 28560,        # (560 + 28000)\n",
    "  3: 303560,       # (28560 + 275000)\n",
    "  4: 1128560,      # (303560 + 825000) <-- Reaches 1M+\n",
    "  5: 1788560,      # (1128560 + 660000)\n",
    "  6: 2228560,      # (1788560 + 440000) <-- Reaches 2M+\n",
    "  7: 2503560,      # (2228560 + 275000)\n",
    "  8: 2668560,      # (2503560 + 165000)\n",
    "  9: 2778560,      # (2668560 + 110000)\n",
    " 10: 2833560,      # (2778560 + 55000) <-- Growth significantly slower now\n",
    " 11: 2877560,      # (2833560 + 44000)\n",
    " 12: 2910560,      # (2877560 + 33000)\n",
    " 13: 2938560,      # (2910560 + 28000)\n",
    " 14: 2960560,      # (2938560 + 22000)\n",
    " 15: 2977560,      # (2960560 + 17000)\n",
    " 16: 2988560,      # (2977560 + 11000)\n",
    " 17: 2994560,      # (2988560 + 6000)\n",
    " 18: 2997560,      # (2994560 + 3000)\n",
    " 19: 2998660,      # (2997560 + 1100)\n",
    " 20: 2999210 }      # (2998660 + 550) <-- Final cumulative total ~3M\n",
    "------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "**Rationale**: This depth distribution shows how many URLs exist at each click depth level from the homepage (hompage = depth 0). A healthy site typically has most content within 3 or 4 clicks of the homepage. Much more, and it may as well not exist. Such reports help identify potential deep crawl issues, spider-traps, and why (in addition to the infinite spam-cannon of generative AI content), brute-force crawls that *\"make a copy of the Internet\"* are all but dead. And did I mention that excessively crawl-able faceted search makes your site's link-graph look like the Death Star? Yeah, I think I did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "# Download Link Graph: How to Download a Link Graph for a Specified Organization, Project, and Analysis For Website Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration and API key\n",
    "config = json.load(open(\"config.json\"))\n",
    "api_key = open('botify_token.txt').read().strip().split('\\n')[0].strip()\n",
    "org = config['org']\n",
    "project = config['project']\n",
    "analysis = config['analysis']\n",
    "\n",
    "# Define API headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Determine optimal click depth for link graph export\n",
    "def find_optimal_depth(org, project, analysis, max_edges=1000000):\n",
    "    \"\"\"\n",
    "    Determine the highest depth for which the number of edges does not exceed max_edges.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    # httpx doesn't have Session, use Client instead\n",
    "    client = httpx.Client()\n",
    "    previous_edges = 0\n",
    "\n",
    "    for depth in range(1, 10):\n",
    "        data_payload = {\n",
    "            \"collections\": [f\"crawl.{analysis}\"],\n",
    "            \"query\": {\n",
    "                \"dimensions\": [],\n",
    "                \"metrics\": [{\"function\": \"sum\", \"args\": [f\"crawl.{analysis}.outlinks_internal.nb.total\"]}],\n",
    "                \"filters\": {\"field\": f\"crawl.{analysis}.depth\", \"predicate\": \"lte\", \"value\": depth},\n",
    "            },\n",
    "        }\n",
    "\n",
    "        response = client.post(url, headers=headers, json=data_payload)\n",
    "        data = response.json()\n",
    "        edges = data[\"results\"][0][\"metrics\"][0]\n",
    "\n",
    "        print(f\"Depth {depth}: {edges:,} edges\")\n",
    "\n",
    "        if edges > max_edges or edges == previous_edges:\n",
    "            return depth - 1 if depth > 1 else depth, previous_edges\n",
    "\n",
    "        previous_edges = edges\n",
    "\n",
    "    return depth, previous_edges\n",
    "\n",
    "# Export link graph to CSV\n",
    "def export_link_graph(org, project, analysis, chosen_depth, save_path=\"downloads\"):\n",
    "    \"\"\"\n",
    "    Export link graph up to the chosen depth level and save as a CSV.\n",
    "    \"\"\"\n",
    "    url = \"https://api.botify.com/v1/jobs\"\n",
    "    data_payload = {\n",
    "        \"job_type\": \"export\",\n",
    "        \"payload\": {\n",
    "            \"username\": org,\n",
    "            \"project\": project,\n",
    "            \"connector\": \"direct_download\",\n",
    "            \"formatter\": \"csv\",\n",
    "            \"export_size\": 1000000,\n",
    "            \"query\": {\n",
    "                \"collections\": [f\"crawl.{analysis}\"],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [\n",
    "                        \"url\",\n",
    "                        f\"crawl.{analysis}.outlinks_internal.graph.url\",\n",
    "                    ],\n",
    "                    \"metrics\": [],\n",
    "                    \"filters\": {\"field\": f\"crawl.{analysis}.depth\", \"predicate\": \"lte\", \"value\": chosen_depth},\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    export_job_details = response.json()\n",
    "    job_url = f\"https://api.botify.com{export_job_details.get('job_url')}\"\n",
    "\n",
    "    # Polling for job completion\n",
    "    attempts = 300\n",
    "    delay = 3\n",
    "    while attempts > 0:\n",
    "        time.sleep(delay)\n",
    "        response_poll = httpx.get(job_url, headers=headers)\n",
    "        job_status_details = response_poll.json()\n",
    "        if job_status_details[\"job_status\"] == \"DONE\":\n",
    "            download_url = job_status_details[\"results\"][\"download_url\"]\n",
    "            save_as_filename = Path(save_path) / f\"{org}_{project}_{analysis}_linkgraph_depth-{chosen_depth}.csv\"\n",
    "            download_file(download_url, save_as_filename)\n",
    "            return save_as_filename\n",
    "        elif job_status_details[\"job_status\"] == \"FAILED\":\n",
    "            print(\"Export job failed.\")\n",
    "            return None\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        attempts -= 1\n",
    "\n",
    "    print(\"Unable to complete download attempts successfully.\")\n",
    "    return None\n",
    "\n",
    "# Download file function\n",
    "def download_file(url, save_path):\n",
    "    \"\"\"\n",
    "    Download a file from a URL to a specified local path.\n",
    "    \"\"\"\n",
    "    # Fix: httpx.get() doesn't support 'stream' parameter directly\n",
    "    with httpx.Client() as client:\n",
    "        with client.stream(\"GET\", url) as response:\n",
    "            with open(save_path, \"wb\") as file:\n",
    "                for chunk in response.iter_bytes(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "    print(f\"\\nFile downloaded as '{save_path}'\")\n",
    "\n",
    "# Main execution\n",
    "print(\"Determining optimal depth for link graph export...\")\n",
    "chosen_depth, final_edges = find_optimal_depth(org, project, analysis)\n",
    "print(f\"Using depth {chosen_depth} with {final_edges:,} edges for export.\")\n",
    "\n",
    "# Make sure the downloads folder exists\n",
    "downloads_folder = Path(\"downloads\")\n",
    "downloads_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Starting link graph export...\")\n",
    "link_graph_path = export_link_graph(org, project, analysis, chosen_depth, save_path=\"downloads\")\n",
    "\n",
    "if link_graph_path:\n",
    "    print(f\"Link graph saved to: {link_graph_path}\")\n",
    "else:\n",
    "    print(\"Link graph export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "```\n",
    "Determining optimal depth for link graph export...\n",
    "Depth 1: 50,000 edges\n",
    "Depth 2: 120,000 edges\n",
    "Depth 3: 500,000 edges\n",
    "Depth 4: 1,200,000 edges\n",
    "Using depth 3 with 500,000 edges for export.\n",
    "Starting link graph export...\n",
    "Polling for job completion: ...\n",
    "Download URL: https://botify-export-url.com/file.csv\n",
    "File downloaded as 'downloads/org_project_analysis_linkgraph_depth-3.csv'\n",
    "Link graph saved to: downloads/org_project_analysis_linkgraph_depth-3.csv\n",
    "```\n",
    "\n",
    "**Rationale**: And now, the moment you‚Äôve all been waiting for‚Äîthe elusive, hard-to-visualize link-graph of your website. Think Admiral Ackbar scrutinizing a hologram of the Death Star, examining every strength and vulnerability, now superimposed with Google Search Console Clicks and Impressions. The Rebels lean in, studying surprise hot spots and patches of dead wood. Every faceted search site ends up looking like the Death Star. But if you‚Äôve done it right, with solid topical clustering, you‚Äôll have something that resembles broccoli or cauliflower... are those called nodules? Florets? Either way, it‚Äôs a good look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "# Check Link-Graph Enhancements: How To Check What Data is Available to Enhance Link-Graph Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "# Load configuration and API key\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {open('botify_token.txt').read().strip()}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def preview_data(org, project, analysis, depth=1):\n",
    "    \"\"\"Preview data availability before committing to full download\"\"\"\n",
    "    # Get analysis date from the slug (assuming YYYYMMDD format)\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    # Calculate period start (7 days before analysis date)\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"collections\": [\n",
    "            f\"crawl.{analysis}\",\n",
    "            \"search_console\"\n",
    "        ],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\"\n",
    "            ],\n",
    "            \"metrics\": [\n",
    "                \"search_console.period_0.count_impressions\",\n",
    "                \"search_console.period_0.count_clicks\"\n",
    "            ],\n",
    "            \"filters\": {\n",
    "                \"field\": f\"crawl.{analysis}.depth\",\n",
    "                \"predicate\": \"lte\",\n",
    "                \"value\": depth\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": \"search_console.period_0.count_impressions\",\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"periods\": [\n",
    "            [\n",
    "                period_start,\n",
    "                period_end\n",
    "            ]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüîç Sampling data for {org}/{project}/{analysis}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    if response.status_code != 200:\n",
    "        print(\"‚ùå Preview failed:\", response.status_code)\n",
    "        return False\n",
    "        \n",
    "    data = response.json()\n",
    "    if not data.get('results'):\n",
    "        print(\"‚ö†Ô∏è  No preview data available\")\n",
    "        return False\n",
    "        \n",
    "    print(\"\\nüìä Data Sample Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    metrics_found = 0\n",
    "    for result in data['results'][:3]:  # Show just top 3 for cleaner output\n",
    "        url = result['dimensions'][0]\n",
    "        impressions = result['metrics'][0]\n",
    "        clicks = result['metrics'][1]\n",
    "        metrics_found += bool(impressions or clicks)\n",
    "        print(f\"‚Ä¢ URL: {url[:60]}...\")\n",
    "        print(f\"  ‚îî‚îÄ Performance: {impressions:,} impressions, {clicks:,} clicks\")\n",
    "    \n",
    "    print(\"\\nüéØ Data Quality Check\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"‚úì URLs found: {len(data['results'])}\")\n",
    "    print(f\"‚úì Search metrics: {'Available' if metrics_found else 'Not found'}\")\n",
    "    print(f\"‚úì Depth limit: {depth}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis):\n",
    "    \"\"\"Fetch data based on BQLv2 query for a specific Botify analysis.\"\"\"\n",
    "    collection = f\"crawl.{analysis}\"\n",
    "    \n",
    "    # Calculate dates\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # Base dimensions that should always be available in crawl\n",
    "    base_dimensions = [\n",
    "        f\"{collection}.url\",\n",
    "        f\"{collection}.depth\",\n",
    "    ]\n",
    "    \n",
    "    # Optional dimensions that might not be available\n",
    "    optional_dimensions = [\n",
    "        f\"{collection}.segments.pagetype.value\",\n",
    "        f\"{collection}.compliant.is_compliant\",\n",
    "        f\"{collection}.compliant.main_reason\",\n",
    "        f\"{collection}.canonical.to.equal\",\n",
    "        f\"{collection}.sitemaps.present\",\n",
    "        f\"{collection}.js.rendering.exec\",\n",
    "        f\"{collection}.js.rendering.ok\"\n",
    "    ]\n",
    "    \n",
    "    # Optional metrics from other collections\n",
    "    optional_metrics = [\n",
    "        \"search_console.period_0.count_impressions\",\n",
    "        \"search_console.period_0.count_clicks\"\n",
    "    ]\n",
    "    \n",
    "    # First, let's check which collections are available\n",
    "    collections = [collection]  # Using full collection name\n",
    "    try:\n",
    "        # We could add an API call here to check available collections\n",
    "        # For now, let's assume search_console might be available\n",
    "        collections.append(\"search_console\")\n",
    "    except Exception as e:\n",
    "        print(f\"Search Console data not available: {e}\")\n",
    "    \n",
    "    data_payload = {\n",
    "        \"collections\": collections,\n",
    "        \"query\": {\n",
    "            \"dimensions\": base_dimensions + optional_dimensions,\n",
    "            \"metrics\": optional_metrics if \"search_console\" in collections else [],\n",
    "            \"filters\": {\n",
    "                \"field\": f\"{collection}.depth\",\n",
    "                \"predicate\": \"lte\",\n",
    "                \"value\": 2\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": \"search_console.period_0.count_impressions\" if \"search_console\" in collections else f\"{collection}.depth\",\n",
    "                    \"order\": \"desc\" if \"search_console\" in collections else \"asc\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"periods\": [[period_start, period_end]] if \"search_console\" in collections else None\n",
    "    }\n",
    "\n",
    "    print(f\"Query payload: {json.dumps(data_payload, indent=2)}\")\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error response: {response.text}\")\n",
    "        response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Define all possible columns\n",
    "    all_columns = ['url', 'depth', 'pagetype', 'compliant', 'reason', 'canonical', \n",
    "                  'sitemap', 'js_exec', 'js_ok', 'impressions', 'clicks']\n",
    "    \n",
    "    # Create DataFrame with available data\n",
    "    results = []\n",
    "    for item in data['results']:\n",
    "        # Fill missing dimensions/metrics with None\n",
    "        row = item['dimensions']\n",
    "        if 'metrics' in item:\n",
    "            row.extend(item['metrics'])\n",
    "        while len(row) < len(all_columns):\n",
    "            row.append(None)\n",
    "        results.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=all_columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fetch_fields(org: str, project: str, collection: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch available fields for a given collection from the Botify API.\n",
    "    \n",
    "    Args:\n",
    "        org: Organization slug\n",
    "        project: Project slug  \n",
    "        collection: Collection name (e.g. 'crawl.20241108')\n",
    "        \n",
    "    Returns:\n",
    "        List of field IDs available in the collection\n",
    "    \"\"\"\n",
    "    fields_url = f\"https://api.botify.com/v1/projects/{org}/{project}/collections/{collection}\"\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(fields_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        fields_data = response.json()\n",
    "        return [\n",
    "            field['id'] \n",
    "            for dataset in fields_data.get('datasets', [])\n",
    "            for field in dataset.get('fields', [])\n",
    "        ]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching fields for collection '{collection}': {e}\")\n",
    "        return []\n",
    "\n",
    "def check_compliance_fields(org, project, analysis):\n",
    "    \"\"\"Check available compliance fields in a more structured way.\"\"\"\n",
    "    collection = f\"crawl.{analysis}\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # Group compliance fields by category\n",
    "    compliance_categories = {\n",
    "        'Basic Compliance': [\n",
    "            'compliant.is_compliant',\n",
    "            'compliant.main_reason',\n",
    "            'compliant.reason.http_code',\n",
    "            'compliant.reason.content_type',\n",
    "            'compliant.reason.canonical',\n",
    "            'compliant.reason.noindex',\n",
    "            'compliant.detailed_reason'\n",
    "        ],\n",
    "        'Performance': [\n",
    "            'scoring.issues.slow_first_to_last_byte_compliant',\n",
    "            'scoring.issues.slow_render_time_compliant',\n",
    "            'scoring.issues.slow_server_time_compliant',\n",
    "            'scoring.issues.slow_load_time_compliant'\n",
    "        ],\n",
    "        'SEO': [\n",
    "            'scoring.issues.duplicate_query_kvs_compliant'\n",
    "        ],\n",
    "        'Outlinks': [\n",
    "            'outlinks_errors.non_compliant.nb.follow.unique',\n",
    "            'outlinks_errors.non_compliant.nb.follow.total',\n",
    "            'outlinks_errors.non_compliant.urls'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüîç Field Availability Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    available_count = 0\n",
    "    total_count = sum(len(fields) for fields in compliance_categories.values())\n",
    "    \n",
    "    available_fields = []\n",
    "    for category, fields in compliance_categories.items():\n",
    "        available_in_category = 0\n",
    "        print(f\"\\nüìë {category}\")\n",
    "        print(\"-\" * 30)\n",
    "        for field in fields:\n",
    "            full_field = f\"{collection}.{field}\"\n",
    "            # Test field availability with a minimal query\n",
    "            test_query = {\n",
    "                \"collections\": [collection],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [full_field],\n",
    "                    \"filters\": {\"field\": f\"{collection}.depth\", \"predicate\": \"eq\", \"value\": 0}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = httpx.post(url, headers=headers, json=test_query, timeout=60)\n",
    "                if response.status_code == 200:\n",
    "                    available_in_category += 1\n",
    "                    available_count += 1\n",
    "                    print(f\"‚úì {field.split('.')[-1]}\")\n",
    "                    available_fields.append(field)\n",
    "                else:\n",
    "                    print(f\"√ó {field.split('.')[-1]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"? {field.split('.')[-1]} (error checking)\")\n",
    "    \n",
    "    coverage = (available_count / total_count) * 100\n",
    "    print(f\"\\nüìä Field Coverage: {coverage:.1f}%\")\n",
    "    return available_fields\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution logic\"\"\"\n",
    "    try:\n",
    "        with open('config.json') as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: config.json file not found\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: config.json is not valid JSON\")\n",
    "        return\n",
    "    \n",
    "    org = config.get('org')\n",
    "    project = config.get('project')\n",
    "    analysis = config.get('analysis')\n",
    "    \n",
    "    if not all([org, project, analysis]):\n",
    "        print(\"Error: Missing required fields in config.json (org, project, analysis)\")\n",
    "        return\n",
    "    \n",
    "    print(\"Previewing data availability...\")\n",
    "    if preview_data(org, project, analysis, depth=2):\n",
    "        print(\"Data preview successful. Proceeding with full export...\")\n",
    "        print(\"Fetching BQLv2 data...\")\n",
    "        df = get_bqlv2_data(org, project, analysis)\n",
    "        print(\"\\nData Preview:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to CSV\n",
    "        Path(\"downloads\").mkdir(parents=True, exist_ok=True)\n",
    "        output_file = f\"downloads/{org}_{project}_{analysis}_metadata.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nData saved to {output_file}\")\n",
    "        \n",
    "        # Use check_compliance_fields\n",
    "        check_compliance_fields(org, project, analysis)\n",
    "    else:\n",
    "        print(\"Data preview failed. Please check configuration and try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Previewing data availability...\n",
    "    \n",
    "    üîç Sampling data for example/retail-division/20241108\n",
    "    ==================================================\n",
    "    \n",
    "    üìä Data Sample Analysis\n",
    "    ------------------------------\n",
    "    ‚Ä¢ URL: https://www.example.com/...\n",
    "      ‚îî‚îÄ Performance: 123,456 impressions, 12,345 clicks\n",
    "    ‚Ä¢ URL: https://www.example.com/site/retail/seasonal-sale/pcmcat...\n",
    "      ‚îî‚îÄ Performance: 98,765 impressions, 8,765 clicks\n",
    "    ‚Ä¢ URL: https://www.example.com/site/misc/daily-deals/pcmcat2480...\n",
    "      ‚îî‚îÄ Performance: 54,321 impressions, 4,321 clicks\n",
    "    \n",
    "    üéØ Data Quality Check\n",
    "    ------------------------------\n",
    "    ‚úì URLs found: 404\n",
    "    ‚úì Search metrics: Available\n",
    "    ‚úì Depth limit: 2\n",
    "    Data preview successful. Proceeding with full export...\n",
    "    Fetching BQLv2 data...\n",
    "    Query payload: {\n",
    "      \"collections\": [\n",
    "        \"crawl.20241108\",\n",
    "        \"search_console\"\n",
    "      ],\n",
    "      \"query\": {\n",
    "        \"dimensions\": [\n",
    "          \"crawl.20241108.url\",\n",
    "          \"crawl.20241108.depth\",\n",
    "          \"crawl.20241108.segments.pagetype.value\",\n",
    "          \"crawl.20241108.compliant.is_compliant\",\n",
    "          \"crawl.20241108.compliant.main_reason\",\n",
    "          \"crawl.20241108.canonical.to.equal\",\n",
    "          \"crawl.20241108.sitemaps.present\",\n",
    "          \"crawl.20241108.js.rendering.exec\",\n",
    "          \"crawl.20241108.js.rendering.ok\"\n",
    "        ],\n",
    "        \"metrics\": [\n",
    "          \"search_console.period_0.count_impressions\",\n",
    "          \"search_console.period_0.count_clicks\"\n",
    "        ],\n",
    "        \"filters\": {\n",
    "          \"field\": \"crawl.20241108.depth\",\n",
    "          \"predicate\": \"lte\",\n",
    "          \"value\": 2\n",
    "        },\n",
    "        \"sort\": [\n",
    "          {\n",
    "            \"field\": \"search_console.period_0.count_impressions\",\n",
    "            \"order\": \"desc\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"periods\": [\n",
    "        [\n",
    "          \"2024-11-01\",\n",
    "          \"2024-11-08\"\n",
    "        ]\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "    Data Preview:\n",
    "                                                                                                                            url  \\\n",
    "    0  https://www.example.com/realm/shops/merchants-quarter/enchanted-items/\n",
    "    1  https://www.example.com/realm/elven-moonlight-potion-azure/\n",
    "    2  https://www.example.com/realm/dwarven-decorative-runes-sapphire/   \n",
    "    3  https://www.example.com/realm/legendary-artifacts/master-crafted-items/   \n",
    "    4  ttps://www.example.com/realm/orcish-war-drums-obsidian/    \n",
    "    \n",
    "       depth       pagetype  compliant     reason canonical  sitemap  js_exec  \\\n",
    "    0      0           home       True  Indexable      True    False     True   \n",
    "    1      2            pdp       True  Indexable      True    False     True   \n",
    "    2      1            plp       True  Indexable      True    False     True   \n",
    "    3      1       category       True  Indexable      True    False     True   \n",
    "    4      1           main       True  Indexable      True    False     True   \n",
    "    \n",
    "       js_ok  impressions  clicks  \n",
    "    0  False       123456   12345  \n",
    "    1  False        98765    8765  \n",
    "    2  False        54321    4321  \n",
    "    3  False        11111    1111  \n",
    "    4  False        12345    1234  \n",
    "    \n",
    "    Data saved to downloads/example_retail-division_20241108_metadata.csv\n",
    "\n",
    "**Rationale**: Just because you happen to work at an enterprise SEO company and possess this peculiar intersection of skills‚Äîlike crafting prompts that give LLMs instant deep-knowledge (think Neo suddenly knowing kung fu)‚Äîdoesn't mean you actually understand BQL. In fact, needing to write this prompt rather proves the opposite... wait, did I just create a paradox? Anyway, there's a very subtle chicken-and-egg problem that this file in general and this example in particular helps address: ***validation of collection fields*** so you can template automations without them being too fragile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "# Color-Code Link-Graphs: How To Download Data to Enhance Website Link-Graph Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Load configuration and API key\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {open('botify_token.txt').read().strip()}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def preview_data(org, project, analysis, depth=1):\n",
    "    \"\"\"Preview data availability before committing to full download\"\"\"\n",
    "    # Get analysis date from the slug (assuming YYYYMMDD format)\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    # Calculate period start (7 days before analysis date)\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"collections\": [\n",
    "            f\"crawl.{analysis}\",\n",
    "            \"search_console\"\n",
    "        ],\n",
    "        \"query\": {\n",
    "            \"dimensions\": [\n",
    "                f\"crawl.{analysis}.url\"\n",
    "            ],\n",
    "            \"metrics\": [\n",
    "                \"search_console.period_0.count_impressions\",\n",
    "                \"search_console.period_0.count_clicks\"\n",
    "            ],\n",
    "            \"filters\": {\n",
    "                \"field\": f\"crawl.{analysis}.depth\",\n",
    "                \"predicate\": \"lte\",\n",
    "                \"value\": depth\n",
    "            },\n",
    "            \"sort\": [\n",
    "                {\n",
    "                    \"field\": \"search_console.period_0.count_impressions\",\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"periods\": [\n",
    "            [\n",
    "                period_start,\n",
    "                period_end\n",
    "            ]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüîç Sampling data for {org}/{project}/{analysis}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    response = httpx.post(url, headers=headers, json=data_payload)\n",
    "    if response.status_code != 200:\n",
    "        print(\"‚ùå Preview failed:\", response.status_code)\n",
    "        return False\n",
    "        \n",
    "    data = response.json()\n",
    "    if not data.get('results'):\n",
    "        print(\"‚ö†Ô∏è  No preview data available\")\n",
    "        return False\n",
    "        \n",
    "    print(\"\\nüìä Data Sample Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    metrics_found = 0\n",
    "    for result in data['results'][:3]:  # Show just top 3 for cleaner output\n",
    "        url = result['dimensions'][0]\n",
    "        impressions = result['metrics'][0]\n",
    "        clicks = result['metrics'][1]\n",
    "        metrics_found += bool(impressions or clicks)\n",
    "        print(f\"‚Ä¢ URL: {url[:60]}...\")\n",
    "        print(f\"  ‚îî‚îÄ Performance: {impressions:,} impressions, {clicks:,} clicks\")\n",
    "    \n",
    "    print(\"\\nüéØ Data Quality Check\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"‚úì URLs found: {len(data['results'])}\")\n",
    "    print(f\"‚úì Search metrics: {'Available' if metrics_found else 'Not found'}\")\n",
    "    print(f\"‚úì Depth limit: {depth}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_bqlv2_data(org, project, analysis):\n",
    "    \"\"\"Fetch BQLv2 data using jobs endpoint\"\"\"\n",
    "    # Calculate periods\n",
    "    analysis_date = datetime.strptime(analysis, '%Y%m%d')\n",
    "    period_start = (analysis_date - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    period_end = analysis_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = \"https://api.botify.com/v1/jobs\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"job_type\": \"export\",\n",
    "        \"payload\": {\n",
    "            \"username\": org,\n",
    "            \"project\": project,\n",
    "            \"connector\": \"direct_download\",\n",
    "            \"formatter\": \"csv\",\n",
    "            \"export_size\": 1000000,\n",
    "            \"query\": {\n",
    "                \"collections\": [\n",
    "                    f\"crawl.{analysis}\",\n",
    "                    \"search_console\"\n",
    "                ],\n",
    "                \"periods\": [[period_start, period_end]],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [\n",
    "                        f\"crawl.{analysis}.url\", \n",
    "                        f\"crawl.{analysis}.depth\",\n",
    "                        f\"crawl.{analysis}.segments.pagetype.value\",\n",
    "                        f\"crawl.{analysis}.compliant.is_compliant\",\n",
    "                        f\"crawl.{analysis}.compliant.main_reason\",\n",
    "                        f\"crawl.{analysis}.canonical.to.equal\",\n",
    "                        f\"crawl.{analysis}.sitemaps.present\",\n",
    "                        f\"crawl.{analysis}.js.rendering.exec\",\n",
    "                        f\"crawl.{analysis}.js.rendering.ok\"\n",
    "                    ],\n",
    "                    \"metrics\": [\n",
    "                        \"search_console.period_0.count_impressions\",\n",
    "                        \"search_console.period_0.count_clicks\"\n",
    "                    ],\n",
    "                    \"filters\": {\n",
    "                        \"field\": f\"crawl.{analysis}.depth\",\n",
    "                        \"predicate\": \"lte\",\n",
    "                        \"value\": 2\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\\nStarting export job...\")\n",
    "    response = httpx.post(url, json=data_payload, headers=headers)\n",
    "    job_data = response.json()\n",
    "    job_url = f\"https://api.botify.com{job_data['job_url']}\"\n",
    "    print(f\"Job created successfully (ID: {job_data['job_id']})\")\n",
    "    \n",
    "    print(\"\\nPolling for job completion: \", end=\"\", flush=True)\n",
    "    while True:\n",
    "        time.sleep(5)  # Poll every 5 seconds\n",
    "        status = httpx.get(job_url, headers=headers).json()\n",
    "        print(f\"\\nCurrent status: {status['job_status']}\")\n",
    "        \n",
    "        if status['job_status'] in ['COMPLETE', 'DONE']:\n",
    "            download_url = status['results']['download_url']\n",
    "            print(f\"\\nDownload URL: {download_url}\")\n",
    "            \n",
    "            # Download and process the file\n",
    "            gz_filename = \"export.csv.gz\"\n",
    "            csv_filename = \"export.csv\"\n",
    "            \n",
    "            # Download gzipped file\n",
    "            response = httpx.get(download_url)\n",
    "            with open(gz_filename, \"wb\") as gz_file:\n",
    "                gz_file.write(response.content)\n",
    "            print(f\"File downloaded as '{gz_filename}'\")\n",
    "            \n",
    "            # Decompress and read into DataFrame\n",
    "            with gzip.open(gz_filename, \"rb\") as f_in:\n",
    "                with open(csv_filename, \"wb\") as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            print(f\"File decompressed as '{csv_filename}'\")\n",
    "            \n",
    "            # Read CSV into DataFrame\n",
    "            df = pd.read_csv(csv_filename, names=[\n",
    "                'url', 'depth', 'pagetype', 'compliant', 'reason', \n",
    "                'canonical', 'sitemap', 'js_exec', 'js_ok',\n",
    "                'impressions', 'clicks'\n",
    "            ])\n",
    "            \n",
    "            # Cleanup temporary files\n",
    "            os.remove(gz_filename)\n",
    "            os.remove(csv_filename)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        elif status['job_status'] == 'FAILED':\n",
    "            print(f\"\\nJob failed. Error details: {status}\")\n",
    "            raise Exception(\"Export job failed\")\n",
    "            \n",
    "        elif status['job_status'] in ['CREATED', 'PROCESSING']:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nUnexpected status: {status}\")\n",
    "            raise Exception(f\"Unexpected job status: {status['job_status']}\")\n",
    "\n",
    "def fetch_fields(org: str, project: str, collection: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch available fields for a given collection from the Botify API.\n",
    "    \n",
    "    Args:\n",
    "        org: Organization slug\n",
    "        project: Project slug  \n",
    "        collection: Collection name (e.g. 'crawl.20241108')\n",
    "        \n",
    "    Returns:\n",
    "        List of field IDs available in the collection\n",
    "    \"\"\"\n",
    "    fields_url = f\"https://api.botify.com/v1/projects/{org}/{project}/collections/{collection}\"\n",
    "    \n",
    "    try:\n",
    "        response = httpx.get(fields_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        fields_data = response.json()\n",
    "        return [\n",
    "            field['id'] \n",
    "            for dataset in fields_data.get('datasets', [])\n",
    "            for field in dataset.get('fields', [])\n",
    "        ]\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Error fetching fields for collection '{collection}': {e}\")\n",
    "        return []\n",
    "\n",
    "def check_compliance_fields(org, project, analysis):\n",
    "    \"\"\"Check available compliance fields in a more structured way.\"\"\"\n",
    "    collection = f\"crawl.{analysis}\"\n",
    "    url = f\"https://api.botify.com/v1/projects/{org}/{project}/query\"\n",
    "    \n",
    "    # Group compliance fields by category\n",
    "    compliance_categories = {\n",
    "        'Basic Compliance': [\n",
    "            'compliant.is_compliant',\n",
    "            'compliant.main_reason',\n",
    "            'compliant.reason.http_code',\n",
    "            'compliant.reason.content_type',\n",
    "            'compliant.reason.canonical',\n",
    "            'compliant.reason.noindex',\n",
    "            'compliant.detailed_reason'\n",
    "        ],\n",
    "        'Performance': [\n",
    "            'scoring.issues.slow_first_to_last_byte_compliant',\n",
    "            'scoring.issues.slow_render_time_compliant',\n",
    "            'scoring.issues.slow_server_time_compliant',\n",
    "            'scoring.issues.slow_load_time_compliant'\n",
    "        ],\n",
    "        'SEO': [\n",
    "            'scoring.issues.duplicate_query_kvs_compliant'\n",
    "        ],\n",
    "        'Outlinks': [\n",
    "            'outlinks_errors.non_compliant.nb.follow.unique',\n",
    "            'outlinks_errors.non_compliant.nb.follow.total',\n",
    "            'outlinks_errors.non_compliant.urls'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüîç Field Availability Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    available_count = 0\n",
    "    total_count = sum(len(fields) for fields in compliance_categories.values())\n",
    "    \n",
    "    available_fields = []\n",
    "    for category, fields in compliance_categories.items():\n",
    "        available_in_category = 0\n",
    "        print(f\"\\nüìë {category}\")\n",
    "        print(\"-\" * 30)\n",
    "        for field in fields:\n",
    "            full_field = f\"{collection}.{field}\"\n",
    "            # Test field availability with a minimal query\n",
    "            test_query = {\n",
    "                \"collections\": [collection],\n",
    "                \"query\": {\n",
    "                    \"dimensions\": [full_field],\n",
    "                    \"filters\": {\"field\": f\"{collection}.depth\", \"predicate\": \"eq\", \"value\": 0}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = httpx.post(url, headers=headers, json=test_query)\n",
    "                if response.status_code == 200:\n",
    "                    available_in_category += 1\n",
    "                    available_count += 1\n",
    "                    print(f\"‚úì {field.split('.')[-1]}\")\n",
    "                    available_fields.append(field)\n",
    "                else:\n",
    "                    print(f\"√ó {field.split('.')[-1]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"? {field.split('.')[-1]} (error checking)\")\n",
    "    \n",
    "    coverage = (available_count / total_count) * 100\n",
    "    print(f\"\\nüìä Field Coverage: {coverage:.1f}%\")\n",
    "    return available_fields\n",
    "\n",
    "def download_and_process_csv(download_url, output_filename):\n",
    "    \"\"\"Download and decompress CSV from Botify API.\"\"\"\n",
    "    gz_filename = f\"{output_filename}.gz\"\n",
    "    \n",
    "    # Download gzipped file\n",
    "    response = httpx.get(download_url)\n",
    "    with open(gz_filename, \"wb\") as gz_file:\n",
    "        gz_file.write(response.content)\n",
    "    print(f\"Downloaded: {gz_filename}\")\n",
    "    \n",
    "    # Decompress to CSV\n",
    "    with gzip.open(gz_filename, \"rb\") as f_in:\n",
    "        with open(output_filename, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(f\"Decompressed to: {output_filename}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(gz_filename)\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution logic\"\"\"\n",
    "    try:\n",
    "        with open('config.json') as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: config.json file not found\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: config.json is not valid JSON\")\n",
    "        return\n",
    "    \n",
    "    org = config.get('org')\n",
    "    project = config.get('project')\n",
    "    analysis = config.get('analysis')\n",
    "    \n",
    "    if not all([org, project, analysis]):\n",
    "        print(\"Error: Missing required fields in config.json (org, project, analysis)\")\n",
    "        return\n",
    "    \n",
    "    print(\"Previewing data availability...\")\n",
    "    if preview_data(org, project, analysis, depth=2):\n",
    "        print(\"Data preview successful. Proceeding with full export...\")\n",
    "        print(\"Fetching BQLv2 data...\")\n",
    "        df = get_bqlv2_data(org, project, analysis)\n",
    "        print(\"\\nData Preview:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to CSV\n",
    "        Path(\"downloads\").mkdir(parents=True, exist_ok=True)\n",
    "        output_file = f\"downloads/{org}_{project}_{analysis}_metadata.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nData saved to {output_file}\")\n",
    "        \n",
    "        # Use check_compliance_fields\n",
    "        check_compliance_fields(org, project, analysis)\n",
    "    else:\n",
    "        print(\"Data preview failed. Please check configuration and try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Previewing data availability...\n",
    "    \n",
    "    üîç Sampling data for example/retail-division/20241108\n",
    "    ==================================================\n",
    "    \n",
    "    üìä Data Sample Analysis\n",
    "    ------------------------------\n",
    "    ‚Ä¢ URL: https://www.example.com/...\n",
    "      ‚îî‚îÄ Performance: 123,456 impressions, 12,345 clicks\n",
    "    ‚Ä¢ URL: https://www.example.com/site/retail/seasonal-sale/pcmcat...\n",
    "      ‚îî‚îÄ Performance: 98,765 impressions, 8,765 clicks\n",
    "    ‚Ä¢ URL: https://www.example.com/site/misc/daily-deals/pcmcat2480...\n",
    "      ‚îî‚îÄ Performance: 54,321 impressions, 4,321 clicks\n",
    "    \n",
    "    üéØ Data Quality Check\n",
    "    ------------------------------\n",
    "    ‚úì URLs found: 404\n",
    "    ‚úì Search metrics: Available\n",
    "    ‚úì Depth limit: 2\n",
    "    Data preview successful. Proceeding with full export...\n",
    "    Fetching BQLv2 data...\n",
    "    \n",
    "    Starting export job...\n",
    "    Job created successfully (ID: 12345)\n",
    "    \n",
    "    Polling for job completion: \n",
    "    Current status: CREATED\n",
    "    .\n",
    "    Current status: PROCESSING\n",
    "    .\n",
    "    Current status: DONE\n",
    "    \n",
    "    Download URL: https://example.cloudfront.net/exports/a/b/c/abc123def456/example-2024-11-10.csv.gz\n",
    "    File downloaded as 'export.csv.gz'\n",
    "    File decompressed as 'export.csv'\n",
    "    \n",
    "    Data Preview:\n",
    "                                                                                                                            url  \\\n",
    "    0  https://www.example.com/realm/shops/merchants-quarter/enchanted-items/\n",
    "    1  https://www.example.com/realm/elven-moonlight-potion-azure/\n",
    "    2  https://www.example.com/realm/dwarven-decorative-runes-sapphire/   \n",
    "    3  https://www.example.com/realm/legendary-artifacts/master-crafted-items/   \n",
    "    4  ttps://www.example.com/realm/orcish-war-drums-obsidian/  \n",
    "    \n",
    "       depth             pagetype  compliant     reason canonical  sitemap  \\\n",
    "    0      2             category       True  Indexable      True    False   \n",
    "    1      2                  pdp       True  Indexable      True     True   \n",
    "    2      2                  plp       True  Indexable      True     True   \n",
    "    3      2               review       True  Indexable      True    False   \n",
    "    4      2                 main       True  Indexable      True     True   \n",
    "    \n",
    "       js_exec  js_ok  impressions  clicks  \n",
    "    0     True  False       12345     123   \n",
    "    1     True  False        9876      98   \n",
    "    2     True  False        5432      54   \n",
    "    3     True  False        1111      11   \n",
    "    4     True  False         987       9   \n",
    "    \n",
    "    Data saved to downloads/example_retail-division_20241108_metadata.csv\n",
    "\n",
    "**Rationale**: BQL and enterprise SEO diagnostics provide powerful tools for comprehensive website analysis. This code transforms link structures into detailed visualizations of your site's SEO health, presenting search signals and performance metrics in a multi-dimensional view. Similar to how diagnostic imaging reveals underlying conditions, these analyses expose strengths, vulnerabilities, and technical issues within your site architecture. Prepare your site for the increasing number of AI crawlers by optimizing your content and structure. Ensure your schema.org structured data is properly implemented to support real-time crawls that evaluate product availability and other critical information.\n",
    " \n",
    "Next-generation SEO requires adapting to these AI-driven changes. Now, let's examine the process of converting from BQLv1 to the collection-based BQLv2 format..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "# Web Logs: How To Check If A Project Has a Web Logs Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import os\n",
    "# No typing imports needed\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG_FILE = \"config.json\"\n",
    "TOKEN_FILE = \"botify_token.txt\"\n",
    "# The specific collection ID we are looking for\n",
    "TARGET_LOG_COLLECTION_ID = \"logs\"\n",
    "\n",
    "# --- !!! EASY OVERRIDE SECTION !!! ---\n",
    "# Set these variables to directly specify org/project, bypassing config.json\n",
    "# Leave as None to use config.json or prompts.\n",
    "# https://app.botify.com/michaellevin-org/mikelev.in\n",
    "# ORG_OVERRIDE = None\n",
    "# PROJECT_OVERRIDE = None\n",
    "ORG_OVERRIDE = \"michaellevin-org\"\n",
    "PROJECT_OVERRIDE = \"mikelev.in\"\n",
    "# Example:\n",
    "# ORG_OVERRIDE = \"my-direct-org\"\n",
    "# PROJECT_OVERRIDE = \"my-direct-project.com\"\n",
    "# --- END OVERRIDE SECTION ---\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_api_key():\n",
    "    \"\"\"Loads the API key from the token file. Returns None on error.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(TOKEN_FILE):\n",
    "            print(f\"Error: Token file '{TOKEN_FILE}' not found.\")\n",
    "            return None\n",
    "        with open(TOKEN_FILE) as f:\n",
    "            api_key = f.read().strip().split('\\n')[0].strip()\n",
    "            if not api_key:\n",
    "                print(f\"Error: Token file '{TOKEN_FILE}' is empty.\")\n",
    "                return None\n",
    "        return api_key\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading API key: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_org_project_from_config():\n",
    "    \"\"\"Loads org and project from config file. Returns (None, None) on error or if missing.\"\"\"\n",
    "    org_config = None\n",
    "    project_config = None\n",
    "    try:\n",
    "        if os.path.exists(CONFIG_FILE):\n",
    "            with open(CONFIG_FILE) as f:\n",
    "                config_data = json.load(f)\n",
    "                org_config = config_data.get('org')\n",
    "                project_config = config_data.get('project')\n",
    "        # No error if file doesn't exist, just return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: '{CONFIG_FILE}' contains invalid JSON.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load org/project from {CONFIG_FILE}: {e}\")\n",
    "    return org_config, project_config\n",
    "\n",
    "def get_api_headers(api_key):\n",
    "    \"\"\"Returns standard API headers.\"\"\"\n",
    "    return {\n",
    "        \"Authorization\": f\"Token {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "def check_if_log_collection_exists(org_slug, project_slug, api_key):\n",
    "    \"\"\"\n",
    "    Checks if a collection with ID 'logs' exists for the given org and project.\n",
    "    Returns True if found, False otherwise or on error.\n",
    "    \"\"\"\n",
    "    if not org_slug or not project_slug or not api_key:\n",
    "        print(\"Error: Org slug, project slug, and API key are required for check.\")\n",
    "        return False\n",
    "\n",
    "    collections_url = f\"https://api.botify.com/v1/projects/{org_slug}/{project_slug}/collections\"\n",
    "    headers = get_api_headers(api_key)\n",
    "\n",
    "    print(f\"\\nChecking for collection '{TARGET_LOG_COLLECTION_ID}' in {org_slug}/{project_slug}...\")\n",
    "\n",
    "    try:\n",
    "        response = httpx.get(collections_url, headers=headers, timeout=60.0)\n",
    "\n",
    "        if response.status_code == 401:\n",
    "            print(\"Error: Authentication failed (401). Check your API token.\")\n",
    "            return False\n",
    "        elif response.status_code == 403:\n",
    "             print(\"Error: Forbidden (403). You may not have access to this project or endpoint.\")\n",
    "             return False\n",
    "        elif response.status_code == 404:\n",
    "             print(\"Error: Project not found (404). Check org/project slugs.\")\n",
    "             return False\n",
    "\n",
    "        response.raise_for_status() # Raise errors for other bad statuses (like 5xx)\n",
    "        collections_data = response.json()\n",
    "\n",
    "        if not isinstance(collections_data, list):\n",
    "             print(f\"Error: Unexpected API response format. Expected a list.\")\n",
    "             return False\n",
    "\n",
    "        for collection in collections_data:\n",
    "            if isinstance(collection, dict) and collection.get('id') == TARGET_LOG_COLLECTION_ID:\n",
    "                print(f\"Success: Found collection with ID '{TARGET_LOG_COLLECTION_ID}'.\")\n",
    "                return True\n",
    "\n",
    "        print(f\"Result: Collection with ID '{TARGET_LOG_COLLECTION_ID}' was not found in the list.\")\n",
    "        return False\n",
    "\n",
    "    except httpx.HTTPStatusError as e:\n",
    "         print(f\"API Error checking collections: {e.response.status_code}\")\n",
    "         return False\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Network error checking collections: {e}\")\n",
    "        return False\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Could not decode the API response as JSON.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during check: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Main Function ---\n",
    "\n",
    "def run_check(org_override=None, project_override=None):\n",
    "    \"\"\"\n",
    "    Orchestrates the check for the 'logs' collection.\n",
    "    Uses override values if provided, otherwise falls back to config file, then prompts.\n",
    "    \"\"\"\n",
    "    print(\"Starting Botify Log Collection Check...\")\n",
    "\n",
    "    # 1. Load API Key (Essential)\n",
    "    api_key = load_api_key()\n",
    "    if not api_key:\n",
    "        print(\"Cannot proceed without a valid API key.\")\n",
    "        return # Exit the function\n",
    "\n",
    "    # 2. Determine Org and Project to use\n",
    "    org_config, project_config = load_org_project_from_config()\n",
    "\n",
    "    # Apply overrides if they exist\n",
    "    org_to_use = org_override if org_override is not None else org_config\n",
    "    project_to_use = project_override if project_override is not None else project_config\n",
    "\n",
    "    # If still missing after config and overrides, prompt the user\n",
    "    if not org_to_use:\n",
    "        print(f\"Organization slug not found in config or override.\")\n",
    "        org_to_use = input(\"Enter the organization slug: \").strip()\n",
    "\n",
    "    if not project_to_use:\n",
    "        print(f\"Project slug not found in config or override.\")\n",
    "        project_to_use = input(\"Enter the project slug: \").strip()\n",
    "\n",
    "    # Final check before running API call\n",
    "    if not org_to_use or not project_to_use:\n",
    "        print(\"Organization and Project slugs are required to run the check. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 3. Run the core check function\n",
    "    has_logs = check_if_log_collection_exists(org_to_use, project_to_use, api_key)\n",
    "\n",
    "    # 4. Report Final Result\n",
    "    print(\"\\n--- Check Complete ---\")\n",
    "    if has_logs:\n",
    "        print(f\"The project '{org_to_use}/{project_to_use}' appears to HAVE a '{TARGET_LOG_COLLECTION_ID}' collection available.\")\n",
    "    else:\n",
    "        print(f\"The project '{org_to_use}/{project_to_use}' does NOT appear to have a '{TARGET_LOG_COLLECTION_ID}' collection available (or an error occurred).\")\n",
    "\n",
    "\n",
    "# --- Script Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the main function, passing the override values defined at the top\n",
    "    run_check(org_override=ORG_OVERRIDE, project_override=PROJECT_OVERRIDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "\n",
    "    Starting Botify Log Collection Check...\n",
    "    \n",
    "    Checking for collection 'logs' in example-org/example.com...\n",
    "    Success: Found collection with ID 'logs'.\n",
    "    \n",
    "    --- Check Complete ---\n",
    "    The project 'example-org/example.com' appears to HAVE a 'logs' collection available.\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "    Starting Botify Log Collection Check...\n",
    "    \n",
    "    Checking for collection 'logs' in example-org/example.com...\n",
    "    Result: Collection with ID 'logs' was not found in the list.\n",
    "    \n",
    "    --- Check Complete ---\n",
    "    The project 'example-org/example.com' does NOT appear to have a 'logs' collection available (or an error occurred)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "# Migrating from BQLv1 to BQLv2\n",
    "\n",
    "This guide explains how to convert BQLv1 queries to BQLv2 format, with practical examples and validation helpers.\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### API Endpoint Changes\n",
    "\n",
    "- **BQLv1**: `/v1/analyses/{username}/{website}/{analysis}/urls`\n",
    "- **BQLv2**: `/v1/projects/{username}/{website}/query`\n",
    "\n",
    "### Key Structural Changes\n",
    "\n",
    "1. Collections replace URL parameters\n",
    "2. Fields become dimensions\n",
    "3. All fields require collection prefixes\n",
    "4. Areas are replaced with explicit filters\n",
    "\n",
    "## Query Conversion Examples\n",
    "\n",
    "### 1. Basic URL Query\n",
    "\n",
    "```json\n",
    "// BQLv1 (/v1/analyses/user/site/20210801/urls?area=current&previous_crawl=20210715)\n",
    "{\n",
    "  \"fields\": [\n",
    "    \"url\",\n",
    "    \"http_code\",\n",
    "    \"previous.http_code\"\n",
    "  ],\n",
    "  \"filters\": {\n",
    "    \"field\": \"indexable.is_indexable\",\n",
    "    \"predicate\": \"eq\",\n",
    "    \"value\": true\n",
    "  }\n",
    "}\n",
    "\n",
    "// BQLv2 (/v1/projects/user/site/query)\n",
    "{\n",
    "  \"collections\": [\n",
    "    \"crawl.20210801\",\n",
    "    \"crawl.20210715\"\n",
    "  ],\n",
    "  \"query\": {\n",
    "    \"dimensions\": [\n",
    "      \"crawl.20210801.url\",\n",
    "      \"crawl.20210801.http_code\",\n",
    "      \"crawl.20210715.http_code\"\n",
    "    ],\n",
    "    \"metrics\": [],\n",
    "    \"filters\": {\n",
    "      \"field\": \"crawl.20210801.indexable.is_indexable\",\n",
    "      \"predicate\": \"eq\",\n",
    "      \"value\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. Aggregation Query\n",
    "\n",
    "```json\n",
    "// BQLv1 (/v1/analyses/user/site/20210801/urls/aggs)\n",
    "[\n",
    "  {\n",
    "    \"aggs\": [\n",
    "      {\n",
    "        \"metrics\": [\"count\"],\n",
    "        \"group_by\": [\n",
    "          {\n",
    "            \"distinct\": {\n",
    "              \"field\": \"segments.pagetype.depth_1\",\n",
    "              \"order\": {\"value\": \"asc\"},\n",
    "              \"size\": 300\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "// BQLv2\n",
    "{\n",
    "  \"collections\": [\"crawl.20210801\"],\n",
    "  \"query\": {\n",
    "    \"dimensions\": [\n",
    "      \"crawl.20210801.segments.pagetype.depth_1\"\n",
    "    ],\n",
    "    \"metrics\": [\n",
    "      \"crawl.20210801.count_urls_crawl\"\n",
    "    ],\n",
    "    \"sort\": [0]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Area Filters\n",
    "\n",
    "BQLv1's area parameter is replaced with explicit filters in BQLv2:\n",
    "\n",
    "#### New URLs Filter\n",
    "```json\n",
    "{\n",
    "  \"and\": [\n",
    "    {\n",
    "      \"field\": \"crawl.20210801.url_exists_crawl\",\n",
    "      \"value\": true\n",
    "    },\n",
    "    {\n",
    "      \"field\": \"crawl.20210715.url_exists_crawl\",\n",
    "      \"value\": false\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Disappeared URLs Filter\n",
    "```json\n",
    "{\n",
    "  \"and\": [\n",
    "    {\n",
    "      \"field\": \"crawl.20210801.url_exists_crawl\",\n",
    "      \"value\": false\n",
    "    },\n",
    "    {\n",
    "      \"field\": \"crawl.20210715.url_exists_crawl\",\n",
    "      \"value\": true\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Conversion Helper Functions\n",
    "\n",
    "```python\n",
    "def validate_bql_v2(query):\n",
    "    \"\"\"Validate BQLv2 query structure\"\"\"\n",
    "    required_keys = {'collections', 'query'}\n",
    "    query_keys = {'dimensions', 'metrics', 'filters'}\n",
    "    \n",
    "    if not all(key in query for key in required_keys):\n",
    "        raise ValueError(f\"Missing required keys: {required_keys}\")\n",
    "    if not any(key in query['query'] for key in query_keys):\n",
    "        raise ValueError(f\"Query must contain one of: {query_keys}\")\n",
    "    for collection in query['collections']:\n",
    "        if not collection.startswith('crawl.'):\n",
    "            raise ValueError(f\"Invalid collection format: {collection}\")\n",
    "    return True\n",
    "\n",
    "def convert_url_query(query_v1, current_analysis, previous_analysis=None):\n",
    "    \"\"\"Convert BQLv1 URL query to BQLv2\"\"\"\n",
    "    collections = [f\"crawl.{current_analysis}\"]\n",
    "    if previous_analysis:\n",
    "        collections.append(f\"crawl.{previous_analysis}\")\n",
    "    \n",
    "    # Convert fields to dimensions\n",
    "    dimensions = []\n",
    "    for field in query_v1.get('fields', []):\n",
    "        if field.startswith('previous.'):\n",
    "            if not previous_analysis:\n",
    "                raise ValueError(\"Previous analysis required for previous fields\")\n",
    "            field = field.replace('previous.', '')\n",
    "            dimensions.append(f\"crawl.{previous_analysis}.{field}\")\n",
    "        else:\n",
    "            dimensions.append(f\"crawl.{current_analysis}.{field}\")\n",
    "    \n",
    "    # Convert filters\n",
    "    filters = None\n",
    "    if 'filters' in query_v1:\n",
    "        filters = {\n",
    "            \"field\": f\"crawl.{current_analysis}.{query_v1['filters']['field']}\",\n",
    "            \"predicate\": query_v1['filters']['predicate'],\n",
    "            \"value\": query_v1['filters']['value']\n",
    "        }\n",
    "    \n",
    "    query_v2 = {\n",
    "        \"collections\": collections,\n",
    "        \"query\": {\n",
    "            \"dimensions\": dimensions,\n",
    "            \"metrics\": [],\n",
    "            \"filters\": filters\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    validate_bql_v2(query_v2)\n",
    "    return query_v2\n",
    "```\n",
    "\n",
    "## Key Conversion Rules\n",
    "\n",
    "1. **Collections**\n",
    "   - Add `collections` array with `crawl.{analysis}` format\n",
    "   - Include both analyses for comparison queries\n",
    "\n",
    "2. **Fields to Dimensions**\n",
    "   - Prefix fields with `crawl.{analysis}.`\n",
    "   - Replace `previous.` prefix with `crawl.{previous_analysis}.`\n",
    "\n",
    "3. **Metrics**\n",
    "   - Convert aggregation metrics to appropriate BQLv2 metric fields\n",
    "   - Use empty array when no metrics needed\n",
    "\n",
    "4. **Filters**\n",
    "   - Prefix filter fields with collection name\n",
    "   - Replace area parameters with explicit URL existence filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "# Size Up Botify Open API Swagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Script to Download Botify OpenAPI Spec, Save Locally, and Calculate Token Count\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Purpose:\n",
    "# This script fetches the OpenAPI (Swagger) specification for the Botify API,\n",
    "# saves it to a local file ('botify_openapi_spec.json'), and then calculates\n",
    "# and displays the number of tokens it represents using the 'tiktoken' library.\n",
    "# This helps in estimating if the content fits LLM context windows and provides\n",
    "# the file for further use.\n",
    "\n",
    "import httpx  # For making HTTP requests\n",
    "import json   # For potential JSON validation (optional here)\n",
    "import tiktoken # For tokenizing text\n",
    "import os     # For path operations\n",
    "\n",
    "# --- Configuration ---\n",
    "SWAGGER_URL = \"https://api.botify.com/v1/swagger.json\"\n",
    "LOCAL_SWAGGER_FILENAME = \"botify_openapi_spec.json\"  # Filename for saving the spec\n",
    "# Using \"cl100k_base\" encoding, common for gpt-4, gpt-3.5-turbo, etc.\n",
    "TIKTOKEN_ENCODING_NAME = \"cl100k_base\"\n",
    "\n",
    "def fetch_swagger_save_and_count_tokens():\n",
    "    \"\"\"\n",
    "    Fetches the Botify API Swagger JSON, saves it locally, prints its size,\n",
    "    and calculates the number of tokens it contains.\n",
    "    \"\"\"\n",
    "    print(f\"INFO: Attempting to download Botify OpenAPI specification from: {SWAGGER_URL}\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Fetch the Swagger JSON content\n",
    "        with httpx.Client(timeout=30.0) as client:\n",
    "            response = client.get(SWAGGER_URL)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        swagger_content_str = response.text\n",
    "        content_bytes = response.content\n",
    "        \n",
    "        print(f\"SUCCESS: Downloaded OpenAPI specification.\")\n",
    "        print(f\"         Size: {len(content_bytes):,} bytes\")\n",
    "\n",
    "        # Step 1.5: Save the downloaded content to a local file\n",
    "        # Rationale: To provide the user with the actual file for copy-pasting\n",
    "        #            or other direct uses.\n",
    "        try:\n",
    "            # Determine script's directory to save the file next to it\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            local_file_path = os.path.join(script_dir, LOCAL_SWAGGER_FILENAME)\n",
    "        except NameError: \n",
    "            # __file__ is not defined (e.g., if running in a Jupyter cell directly pasted, not as a .py file)\n",
    "            # Save in the current working directory instead.\n",
    "            local_file_path = LOCAL_SWAGGER_FILENAME\n",
    "\n",
    "        with open(local_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(swagger_content_str)\n",
    "        print(f\"SUCCESS: OpenAPI specification saved locally as: '{os.path.abspath(local_file_path)}'\")\n",
    "\n",
    "        # Optional: Verify if it's valid JSON\n",
    "        # try:\n",
    "        #     json.loads(swagger_content_str)\n",
    "        #     print(\"         Content is valid JSON.\")\n",
    "        # except json.JSONDecodeError:\n",
    "        #     print(\"WARNING: Content may not be valid JSON, but token counting will proceed on raw text.\")\n",
    "\n",
    "        # Step 2: Initialize the tiktoken encoder\n",
    "        try:\n",
    "            encoding = tiktoken.get_encoding(TIKTOKEN_ENCODING_NAME)\n",
    "            print(f\"INFO: Using tiktoken encoding: '{TIKTOKEN_ENCODING_NAME}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not load tiktoken encoding '{TIKTOKEN_ENCODING_NAME}': {e}\")\n",
    "            print(\"       Please ensure 'tiktoken' is installed correctly.\")\n",
    "            return\n",
    "\n",
    "        # Step 3: Encode the content and count the tokens\n",
    "        tokens = encoding.encode(swagger_content_str)\n",
    "        token_count = len(tokens)\n",
    "\n",
    "        print(f\"\\n--- Tokenization Complete ---\")\n",
    "        print(f\"Number of tokens (using '{TIKTOKEN_ENCODING_NAME}'): {token_count:,}\")\n",
    "        \n",
    "        if token_count < 100000:\n",
    "            print(\"INFO: Token count is comfortably within a typical 128k context window.\")\n",
    "        elif token_count <= 130000:\n",
    "            print(\"INFO: Token count is within the ~100K-130K range mentioned. It should fit, but it's substantial.\")\n",
    "        else:\n",
    "            print(\"WARNING: Token count exceeds 130K. It might be too large for a single POST submission depending on the exact model limit.\")\n",
    "            \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"ERROR: HTTP error occurred while fetching Swagger spec: {e.response.status_code} - {e.response.text}\")\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"ERROR: Network request error occurred: {e}\")\n",
    "    except IOError as e:\n",
    "        print(f\"ERROR: Could not save the Swagger specification to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Script Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_swagger_save_and_count_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "# All Botify API Endpoints: How Do You Generate a Python Code Example for Every Botify Endpoint Given Their OpenAPI Swagger\n",
    "\n",
    "Botify OpenAPI Swagger File: [https://api.botify.com/v1/swagger.json](https://api.botify.com/v1/swagger.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Script to Generate Paginated LLM Training Data for Botify API\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Purpose:\n",
    "# This script fetches the Botify OpenAPI spec and generates a comprehensive, paginated\n",
    "# markdown document suitable for both LLM training and a custom web front-end.\n",
    "# The final file is saved to a configurable output path.\n",
    "\n",
    "import httpx\n",
    "import json\n",
    "import tiktoken\n",
    "import os\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# --- Configuration ---\n",
    "SWAGGER_URL = \"https://api.botify.com/v1/swagger.json\"\n",
    "TIKTOKEN_ENCODING_NAME = \"cl100k_base\"\n",
    "\n",
    "# --- Output Configuration ---\n",
    "# Set this to a specific directory path (e.g., \"../../training\", \"/path/to/output\").\n",
    "# If set to None or an empty string, it will save to a default 'training' folder\n",
    "# in the current working directory.\n",
    "OPTIONAL_HARWIRED_OUTPUT_PATH = \"../../training\"\n",
    "\n",
    "# Filename for the output, specifically named for the documentation plugin.\n",
    "OUTPUT_FILENAME = \"botify_open_api.md\"\n",
    "\n",
    "# The separator used to paginate the document in the web application.\n",
    "PAGINATION_SEPARATOR = \"-\" * 80\n",
    "\n",
    "# --- Global Variable for Notebook Persistence ---\n",
    "llm_training_markdown = \"\"\n",
    "\n",
    "def generate_python_code_example(method: str, path: str, endpoint_details: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generates a practical, professional Python code example for a given API endpoint.\"\"\"\n",
    "    lines = [\n",
    "        \"```python\",\n",
    "        f\"# Example: {method.upper()} {endpoint_details.get('summary', path)}\",\n",
    "        \"import httpx\",\n",
    "        \"import json\",\n",
    "        \"\",\n",
    "        \"# Assumes your Botify API token is in a file named 'botify_token.txt'\",\n",
    "        \"try:\",\n",
    "        \"    with open('botify_token.txt') as f:\",\n",
    "        \"        token = f.read().strip()\",\n",
    "        \"except FileNotFoundError:\",\n",
    "        \"    print(\\\"Error: 'botify_token.txt' not found. Please create it.\\\")\",\n",
    "        \"    token = 'YOUR_API_TOKEN'  # Fallback\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    formatted_path = path\n",
    "    all_params = endpoint_details.get('parameters', [])\n",
    "    path_params = [p for p in all_params if p.get('in') == 'path']\n",
    "    \n",
    "    path_params_from_url = [p_name.strip('{}') for p_name in path.split('/') if p_name.startswith('{')]\n",
    "    declared_param_names = {p['name'] for p in path_params}\n",
    "\n",
    "    for p_name in path_params_from_url:\n",
    "        if p_name not in declared_param_names:\n",
    "            path_params.append({'name': p_name})\n",
    "\n",
    "    if path_params:\n",
    "        lines.append(\"# --- Define Path Parameters ---\")\n",
    "        for p in path_params:\n",
    "            param_name = p['name']\n",
    "            placeholder_value = f\"YOUR_{param_name.upper()}\"\n",
    "            lines.append(f\"{param_name} = \\\"{placeholder_value}\\\"\")\n",
    "            formatted_path = formatted_path.replace(f\"{{{param_name}}}\", f\"{{{param_name}}}\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    lines.append(\"# --- Construct the Request ---\")\n",
    "    lines.append(f\"url = f\\\"[https://api.botify.com/v1](https://api.botify.com/v1){formatted_path}\\\"\")\n",
    "    lines.append(\"headers = {\")\n",
    "    lines.append(\"    'Authorization': f'Token {token}',\")\n",
    "    lines.append(\"    'Content-Type': 'application/json'\")\n",
    "    lines.append(\"}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    query_params = [p for p in all_params if p.get('in') == 'query']\n",
    "    if query_params:\n",
    "        lines.append(\"# Define query parameters\")\n",
    "        lines.append(\"query_params = {\")\n",
    "        for p in query_params:\n",
    "            sample_value = \"'example_value'\" if p.get('type', 'string') == 'string' else '123'\n",
    "            lines.append(f\"    '{p.get('name')}': {sample_value},  # Type: {p.get('type', 'N/A')}, Required: {p.get('required', False)}\")\n",
    "        lines.append(\"}\")\n",
    "\n",
    "    body_param = next((p for p in all_params if p.get('in') == 'body'), None)\n",
    "    if body_param:\n",
    "        lines.append(\"# Define the JSON payload for the request body\")\n",
    "        lines.append(\"json_payload = {\")\n",
    "        lines.append(\"    'key': 'value'  # Replace with actual data\")\n",
    "        lines.append(\"}\")\n",
    "\n",
    "    lines.append(\"\\n# --- Send the Request ---\")\n",
    "    lines.append(\"try:\")\n",
    "    lines.append(\"    with httpx.Client(timeout=30.0) as client:\")\n",
    "    request_args = [\"url\", \"headers=headers\"]\n",
    "    if query_params:\n",
    "        request_args.append(\"params=query_params\")\n",
    "    if body_param:\n",
    "        request_args.append(\"json=json_payload\")\n",
    "    lines.append(f\"        response = client.{method.lower()}({', '.join(request_args)})\")\n",
    "    lines.append(\"        response.raise_for_status()\")\n",
    "    lines.append(\"        print('Success! Response:')\")\n",
    "    lines.append(\"        print(json.dumps(response.json(), indent=2))\")\n",
    "    lines.append(\"except httpx.HTTPStatusError as e:\")\n",
    "    lines.append(\"    print(f'HTTP Error: {e.response.status_code} - {e.response.text}')\")\n",
    "    lines.append(\"except httpx.RequestError as e:\")\n",
    "    lines.append(f\"    print(f'Request error: {{e}}')\")\n",
    "    lines.append(\"except Exception as e:\")\n",
    "    lines.append(f\"    print(f'An unexpected error occurred: {{e}}')\")\n",
    "    lines.append(\"```\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def generate_api_documentation_markdown(spec: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generates a complete markdown document from the OpenAPI specification.\"\"\"\n",
    "    all_endpoint_docs_as_blocks = []\n",
    "    all_endpoints = []\n",
    "\n",
    "    for path, methods in spec.get('paths', {}).items():\n",
    "        for method, details in methods.items():\n",
    "            if method.lower() not in ['get', 'post', 'put', 'delete', 'patch']:\n",
    "                continue\n",
    "            all_endpoints.append({'method': method, 'path': path, 'details': details})\n",
    "\n",
    "    for endpoint in all_endpoints:\n",
    "        try:\n",
    "            method, path, details = endpoint['method'], endpoint['path'], endpoint['details']\n",
    "            endpoint_block = []\n",
    "            tag = details.get('tags', ['Uncategorized'])[0]\n",
    "            summary = details.get('summary', 'No summary provided.')\n",
    "            description = details.get('description', 'No detailed description available.')\n",
    "            parameters = details.get('parameters', [])\n",
    "\n",
    "            endpoint_block.extend([\n",
    "                f\"# `{method.upper()} {path}`\", \"\",\n",
    "                f\"**Category:** `{tag}`\", \"\",\n",
    "                f\"**Summary:** {summary}\", \"\",\n",
    "                \"**Description:**\", f\"{description}\", \"\"\n",
    "            ])\n",
    "\n",
    "            if parameters:\n",
    "                endpoint_block.append(\"**Parameters:**\")\n",
    "                endpoint_block.append(\"| Name | Location (in) | Required | Type | Description |\")\n",
    "                endpoint_block.append(\"|---|---|---|---|---|\")\n",
    "                for p in parameters:\n",
    "                    param_name = p.get('name', 'N/A')\n",
    "                    param_in = p.get('in', 'N/A')\n",
    "                    param_req = p.get('required', False)\n",
    "                    param_type = p.get('type', 'N/A')\n",
    "                    param_desc = p.get('description', '').replace('|', ' ')\n",
    "                    endpoint_block.append(f\"| `{param_name}` | {param_in} | {param_req} | `{param_type}` | {param_desc} |\")\n",
    "                endpoint_block.append(\"\")\n",
    "\n",
    "            endpoint_block.append(\"**Python Example:**\")\n",
    "            endpoint_block.append(generate_python_code_example(method, path, details))\n",
    "            \n",
    "            all_endpoint_docs_as_blocks.append(\"\\n\".join(endpoint_block))\n",
    "        \n",
    "        except Exception:\n",
    "            print(f\"---\")\n",
    "            print(f\"WARNING: Could not process endpoint: {endpoint.get('method', 'N/A').upper()} {endpoint.get('path', 'N/A')}\")\n",
    "            print(f\"  ERROR DETAILS BELOW:\")\n",
    "            traceback.print_exc()\n",
    "            print(f\"---\")\n",
    "            continue\n",
    "\n",
    "    paginated_content = f\"\\n\\n{PAGINATION_SEPARATOR}\\n\\n\".join(all_endpoint_docs_as_blocks)\n",
    "    main_header = [\n",
    "        \"# Botify API Bootcamp\", \"\",\n",
    "        \"This document provides detailed information and Python code examples for every endpoint in the Botify API...\", \"\"\n",
    "    ]\n",
    "    return \"\\n\".join(main_header) + paginated_content\n",
    "\n",
    "def create_llm_training_document():\n",
    "    \"\"\"Main orchestrator function.\"\"\"\n",
    "    global llm_training_markdown\n",
    "    print(f\"INFO: Attempting to download Botify OpenAPI specification from: {SWAGGER_URL}\")\n",
    "    try:\n",
    "        with httpx.Client(timeout=30.0) as client:\n",
    "            response = client.get(SWAGGER_URL)\n",
    "            response.raise_for_status()\n",
    "            spec = response.json()\n",
    "        print(\"SUCCESS: Downloaded OpenAPI specification.\")\n",
    "\n",
    "        print(\"INFO: Generating API documentation with pagination separators...\")\n",
    "        generated_markdown = generate_api_documentation_markdown(spec)\n",
    "        print(\"SUCCESS: API documentation generated.\")\n",
    "\n",
    "        print(f\"INFO: Calculating token count using '{TIKTOKEN_ENCODING_NAME}'...\")\n",
    "        encoding = tiktoken.get_encoding(TIKTOKEN_ENCODING_NAME)\n",
    "        tokens = encoding.encode(generated_markdown)\n",
    "        token_count = len(tokens)\n",
    "        print(f\"SUCCESS: Tokenization complete.\")\n",
    "\n",
    "        token_line = f\"**Total Estimated Token Count**: `{token_count:,}` (using `{TIKTOKEN_ENCODING_NAME}`)\"\n",
    "        llm_training_markdown = generated_markdown.replace(\"# Botify API Bootcamp\", f\"# Botify API Bootcamp\\n\\n{token_line}\")\n",
    "\n",
    "        print(\"\\n--- Analysis Complete ---\")\n",
    "        print(token_line)\n",
    "        if token_count < 200000:\n",
    "            print(\"‚úÖ INFO: The generated documentation should fit within a large context window.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è WARNING: Token count is very large and may exceed some context windows.\")\n",
    "        \n",
    "        # --- Determine the final output path using the optional hardwired path ---\n",
    "        if OPTIONAL_HARWIRED_OUTPUT_PATH and OPTIONAL_HARWIRED_OUTPUT_PATH.strip():\n",
    "            final_output_dir = Path(OPTIONAL_HARWIRED_OUTPUT_PATH)\n",
    "            print(f\"INFO: Using optional hardwired output path: {final_output_dir.resolve()}\")\n",
    "        else:\n",
    "            final_output_dir = Path(\"training\") # Default to 'training' in CWD\n",
    "            print(f\"INFO: No hardwired path set. Using default directory: {final_output_dir.resolve()}\")\n",
    "        \n",
    "        output_path = final_output_dir / OUTPUT_FILENAME\n",
    "\n",
    "        print(f\"INFO: Saving paginated documentation file to: '{output_path.resolve()}'\")\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(llm_training_markdown)\n",
    "        print(\"SUCCESS: Documentation file saved.\")\n",
    "\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        error_message = f\"ERROR: HTTP error during download: {e.response.status_code} - {e.response.text}\"\n",
    "        print(error_message)\n",
    "        llm_training_markdown = f\"# Error\\n\\n{error_message}\"\n",
    "    except Exception:\n",
    "        print(\"ERROR: An unexpected critical error occurred during the process.\")\n",
    "        traceback.print_exc()\n",
    "        llm_training_markdown = f\"# Error\\n\\nAn unexpected critical error occurred. See console for traceback.\"\n",
    "\n",
    "# --- Script Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    create_llm_training_document()\n",
    "    if llm_training_markdown and not llm_training_markdown.startswith(\"# Error\"):\n",
    "        print(f\"\\nSUCCESS: Global variable 'llm_training_markdown' is now populated and file has been created.\")\n",
    "    else:\n",
    "        print(\"\\nERROR: Process did not complete successfully. Check warnings above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "# Create Documentation For Pipulate From This Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Self-Export and Custom Markdown Processing Script\n",
    "# -------------------------------------------------------------------\n",
    "# Purpose:\n",
    "# This script, when run as a cell in a Jupyter Notebook, automates the conversion\n",
    "# of the notebook itself into a clean, well-structured Markdown (.md) file.\n",
    "# The process involves two main stages:\n",
    "#\n",
    "# 1. Self-Export to Raw Markdown:\n",
    "#    - The script first identifies the current Jupyter Notebook file.\n",
    "#    - It then uses 'jupyter nbconvert' to convert the notebook into a standard\n",
    "#      Markdown file, saved in the same directory as the notebook.\n",
    "#\n",
    "# 2. Custom Post-Processing with Heading-Based Delineators:\n",
    "#    - The script then applies a custom processing function to this Markdown file.\n",
    "#    - This custom processing includes:\n",
    "#      a. Removing any YAML frontmatter (text between '---' markers at the\n",
    "#         very start of the document).\n",
    "#      b. Inserting a wide visual delineator (80 hyphens with surrounding\n",
    "#         blank lines) *before* every H1 Markdown heading (`# Heading`)\n",
    "#         that is not inside a fenced code block. This leverages the semantic\n",
    "#         structure of headings for separation.\n",
    "#      c. Prepending a global header (currently configured to be blank).\n",
    "#    - The processed content is then saved. If an optional hardwired output path\n",
    "#      is specified, the final file goes there; otherwise, it overwrites the\n",
    "#      initial Markdown file in the notebook's directory. This results in a\n",
    "#      single .md file named after the original notebook, with clear section\n",
    "#      delineations.\n",
    "#\n",
    "# Rationale:\n",
    "# This workflow produces LLM-ready Markdown. The H1-triggered delineators\n",
    "# enhance readability and provide strong parsing cues for segmenting content\n",
    "# based on major sections (as typically denoted by H1s). This approach relies\n",
    "# on the semantic use of H1 headings in the notebook's Markdown cells to create\n",
    "# logical breaks, offering a more content-aware separation than generic\n",
    "# between-cell markers.\n",
    "\n",
    "# --- Configuration: Optional Hardwired Output Path ---\n",
    "# Set this to a specific directory path (e.g., \"/path/to/your/output_folder\" or \"C:\\\\path\\\\to\\\\output\")\n",
    "# if you want the final Markdown file to always be saved there.\n",
    "# If None or an empty string, the Markdown file will be saved in the same\n",
    "# directory as the Jupyter Notebook (original behavior).\n",
    "# IMPORTANT: If providing a path, ensure the script has write permissions to it.\n",
    "#            The script will attempt to create the directory if it doesn't exist.\n",
    "OPTIONAL_HARWIRED_OUTPUT_PATH = \"../../training\" # Example: \"/mnt/c/Users/YourUser/Documents/NotebookExports\"\n",
    "# OPTIONAL_HARWIRED_OUTPUT_PATH = \"output_markdown\" # Example: a relative path\n",
    "\n",
    "# --- Part 1: Necessary Imports ---\n",
    "# Purpose: Import required Python standard libraries for file operations,\n",
    "#          running external commands, text processing, and date/time.\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "# --- Part 2: Custom Markdown Post-Processing Function ---\n",
    "# Purpose: This function takes the Markdown content produced by 'nbconvert'\n",
    "#          and applies cleaning (frontmatter removal) and custom separator insertion.\n",
    "\n",
    "def process_markdown(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Processes a Markdown file generated from a Jupyter Notebook.\n",
    "    It removes YAML frontmatter and inserts a wide horizontal rule delineator\n",
    "    before each H1 heading (that is not within a code block).\n",
    "    The input and output paths can be the same for in-place modification.\n",
    "\n",
    "    Args:\n",
    "        input_path (str or Path): Path to the input Markdown file.\n",
    "        output_path (str or Path): Path where the processed Markdown will be saved.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if processing is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    input_p = Path(input_path)\n",
    "    output_p = Path(output_path)\n",
    "\n",
    "    print(f\"INFO: Applying custom post-processing to: '{input_p.name}' (output will be '{output_p.resolve()}')\")\n",
    "    \n",
    "    try:\n",
    "        with open(input_p, 'r', encoding='utf-8') as infile:\n",
    "            content = infile.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input file for process_markdown not found: '{input_p}'\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not read input file '{input_p}': {e}\")\n",
    "        return False\n",
    "        \n",
    "    # Step 2.1: Remove YAML frontmatter from the very beginning of the document.\n",
    "    content_after_frontmatter_removal = re.sub(r'^\\s*---.*?---\\s*', '', content, flags=re.DOTALL | re.MULTILINE)\n",
    "    \n",
    "    # --- Step 2.2: Insert Wide Delineators Before H1 Headings ---\n",
    "    # Rationale: This \"slice-and-dicer\" section iterates through the Markdown content\n",
    "    #            line by line, identifying H1 headings (e.g., \"# Heading\") that are\n",
    "    #            not part of a fenced code block. A prominent visual separator\n",
    "    #            (80 hyphens) is inserted before such headings to clearly mark major\n",
    "    #            sections, aiding both visual scanning and LLM parsing.\n",
    "\n",
    "    lines = content_after_frontmatter_removal.splitlines()\n",
    "    processed_lines = []\n",
    "    in_code_block = False  # State variable to track if currently inside a fenced code block\n",
    "    SEPARATOR = \"-\" * 80   # The wide delineator\n",
    "\n",
    "    # Determine if the very first significant content line is an H1.\n",
    "    # We might not want a separator before the absolute first H1 (e.g., the main document title).\n",
    "    first_actual_content_line_index = -1\n",
    "    for idx, line_text in enumerate(lines):\n",
    "        if line_text.strip():\n",
    "            first_actual_content_line_index = idx\n",
    "            break\n",
    "            \n",
    "    for i, line in enumerate(lines):\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Toggle state if a fenced code block starts or ends\n",
    "        if stripped_line.startswith(\"```\"):\n",
    "            in_code_block = not in_code_block\n",
    "        \n",
    "        is_h1_outside_code = not in_code_block and line.startswith(\"# \")\n",
    "\n",
    "        if is_h1_outside_code:\n",
    "            # Add separator if this H1 is NOT the very first significant line of content.\n",
    "            # This prevents a separator at the very top if the document starts with an H1.\n",
    "            if i > first_actual_content_line_index or \\\n",
    "               (i == first_actual_content_line_index and i > 0) or \\\n",
    "               (i == 0 and first_actual_content_line_index > 0 and lines[0].strip() == \"\"): # handles H1 after initial blank lines\n",
    "                # Ensure a blank line before the separator if the previous content wasn't blank\n",
    "                if processed_lines and processed_lines[-1].strip() != \"\":\n",
    "                    processed_lines.append(\"\")\n",
    "                \n",
    "                processed_lines.append(SEPARATOR)\n",
    "                processed_lines.append(\"\") # Blank line after separator, before the H1 heading\n",
    "        \n",
    "        processed_lines.append(line) # Add the current line (H1 or otherwise)\n",
    "\n",
    "    content_with_separators = \"\\n\".join(processed_lines)\n",
    "    \n",
    "    # Step 2.3: Prepare the global header for the final document.\n",
    "    header_comment = \"\" # Currently blank, as per your last preference.\n",
    "    final_content = header_comment + content_with_separators\n",
    "    \n",
    "    try:\n",
    "        # Step 2.4: Write the processed content back to the Markdown file.\n",
    "        output_p.parent.mkdir(parents=True, exist_ok=True) # Ensure output directory exists\n",
    "        with open(output_p, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(final_content)\n",
    "        print(f\"SUCCESS: Custom post-processing complete. Output: '{output_p.resolve()}'\")\n",
    "        return True \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not write final processed file '{output_p}': {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Part 3: Orchestration Logic for Self-Export and Processing ---\n",
    "def export_notebook_and_apply_custom_processing():\n",
    "    \"\"\"\n",
    "    Orchestrates the notebook self-export and custom Markdown processing.\n",
    "    \"\"\"\n",
    "    current_notebook_filename_ipynb = None\n",
    "    current_notebook_dir = os.getcwd() \n",
    "\n",
    "    # --- Step A: Determine the current notebook's filename and directory ---\n",
    "    env_notebook_path_str = os.environ.get('JPY_SESSION_NAME')\n",
    "    if env_notebook_path_str and env_notebook_path_str.endswith(\".ipynb\"):\n",
    "        path_obj = Path(env_notebook_path_str)\n",
    "        if path_obj.is_absolute():\n",
    "            current_notebook_filename_ipynb = path_obj.name\n",
    "            current_notebook_dir = str(path_obj.parent)\n",
    "        else: \n",
    "            potential_path = Path(os.getcwd()) / env_notebook_path_str\n",
    "            if potential_path.exists() and potential_path.is_file():\n",
    "                current_notebook_filename_ipynb = potential_path.name\n",
    "                current_notebook_dir = str(potential_path.parent)\n",
    "            else: \n",
    "                current_notebook_filename_ipynb = path_obj.name # Assume it's relative to cwd if not found absolute\n",
    "        if current_notebook_filename_ipynb:\n",
    "            print(f\"INFO: Notebook identified as '{current_notebook_filename_ipynb}' in '{current_notebook_dir}' (via JPY_SESSION_NAME).\")\n",
    "\n",
    "    if not current_notebook_filename_ipynb:\n",
    "        try:\n",
    "            import ipynbname \n",
    "            notebook_path_obj = ipynbname.path()\n",
    "            current_notebook_filename_ipynb = notebook_path_obj.name\n",
    "            current_notebook_dir = str(notebook_path_obj.parent)\n",
    "            print(f\"INFO: Notebook identified as '{current_notebook_filename_ipynb}' in '{current_notebook_dir}' (via 'ipynbname').\")\n",
    "        except ImportError:\n",
    "            print(\"WARNING: 'ipynbname' package not found. For robust name detection, consider `pip install ipynbname`.\")\n",
    "        except Exception as e: \n",
    "            print(f\"WARNING: 'ipynbname' could not determine notebook path (is notebook saved and trusted?): {e}\")\n",
    "            \n",
    "    if not current_notebook_filename_ipynb:\n",
    "        print(\"CRITICAL ERROR: Could not determine the current notebook's .ipynb filename.\")\n",
    "        return\n",
    "\n",
    "    markdown_basename = Path(current_notebook_filename_ipynb).stem + \".md\"\n",
    "    \n",
    "    # This is where nbconvert will initially place its output\n",
    "    nbconvert_output_path_in_notebook_dir = Path(current_notebook_dir) / markdown_basename\n",
    "\n",
    "    # Determine the final path for the processed Markdown file\n",
    "    final_save_directory_path = Path(current_notebook_dir) # Default\n",
    "    if OPTIONAL_HARWIRED_OUTPUT_PATH and OPTIONAL_HARWIRED_OUTPUT_PATH.strip():\n",
    "        prospective_hardwired_path = Path(OPTIONAL_HARWIRED_OUTPUT_PATH)\n",
    "        try:\n",
    "            prospective_hardwired_path.mkdir(parents=True, exist_ok=True)\n",
    "            final_save_directory_path = prospective_hardwired_path\n",
    "            print(f\"INFO: Using hardwired output directory: '{final_save_directory_path.resolve()}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not create or access hardwired output directory '{prospective_hardwired_path}': {e}.\")\n",
    "            print(f\"        Defaulting to notebook's directory for final output: '{Path(current_notebook_dir).resolve()}'\")\n",
    "            # final_save_directory_path remains current_notebook_dir\n",
    "    else:\n",
    "        print(f\"INFO: Output directory not hardwired. Using notebook's directory for final output: '{final_save_directory_path.resolve()}'\")\n",
    "\n",
    "    final_processed_markdown_path = final_save_directory_path / markdown_basename\n",
    "\n",
    "    # --- Step B: Convert the current notebook to standard Markdown ---\n",
    "    # nbconvert will output <notebook_name>.md into current_notebook_dir (its CWD)\n",
    "    nbconvert_command = [\n",
    "        \"jupyter\", \"nbconvert\",\n",
    "        \"--to\", \"markdown\",\n",
    "        \"--MarkdownExporter.exclude_output=True\",  # Prevent cell outputs from being written\n",
    "        current_notebook_filename_ipynb, \n",
    "        # '--output-dir', current_notebook_dir, # Explicitly stating, though it's default with CWD\n",
    "        # '--output', markdown_basename, # nbconvert derives this from input name\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nINFO: Exporting '{current_notebook_filename_ipynb}' to '{nbconvert_output_path_in_notebook_dir}' using 'jupyter nbconvert'...\")\n",
    "    # print(f\"        Constructed nbconvert_command list: {nbconvert_command}\")\n",
    "    # print(f\"        Running command string (for display): {' '.join(nbconvert_command)}\")\n",
    "    # print(f\"        Working directory: '{current_notebook_dir}'\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            nbconvert_command,\n",
    "            capture_output=True, text=True, check=True,\n",
    "            cwd=current_notebook_dir \n",
    "        )\n",
    "        print(f\"SUCCESS: Notebook initially exported by 'nbconvert' to '{nbconvert_output_path_in_notebook_dir}'.\")\n",
    "        if result.stdout and result.stdout.strip(): \n",
    "            print(\"--- nbconvert stdout ---\")\n",
    "            print(result.stdout.strip())\n",
    "        if result.stderr and result.stderr.strip(): \n",
    "            print(\"--- nbconvert stderr (info/warnings) ---\")\n",
    "            print(result.stderr.strip())\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nCRITICAL ERROR: 'jupyter' command (for nbconvert) was not found.\")\n",
    "        print(\"Ensure Jupyter is installed and in your system's PATH.\")\n",
    "        return\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\nERROR: 'jupyter nbconvert' failed with exit code {e.returncode}.\")\n",
    "        if e.stdout and e.stdout.strip(): print(f\"--- nbconvert stdout ---\\n{e.stdout.strip()}\")\n",
    "        if e.stderr and e.stderr.strip(): print(f\"--- nbconvert stderr ---\\n{e.stderr.strip()}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: An unexpected error occurred during 'nbconvert' execution: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Step C: Apply custom post-processing (frontmatter removal & H1 delineators) ---\n",
    "    if nbconvert_output_path_in_notebook_dir.exists():\n",
    "        processing_successful = process_markdown(\n",
    "            input_path=nbconvert_output_path_in_notebook_dir, \n",
    "            output_path=final_processed_markdown_path\n",
    "        )\n",
    "        \n",
    "        if processing_successful:\n",
    "            # If a hardwired output path was used, and it resulted in a *different* location\n",
    "            # than the notebook's directory, then the initial nbconvert output in the\n",
    "            # notebook's directory should be removed.\n",
    "            hardwired_path_used_and_different = (\n",
    "                OPTIONAL_HARWIRED_OUTPUT_PATH and\n",
    "                OPTIONAL_HARWIRED_OUTPUT_PATH.strip() and\n",
    "                final_save_directory_path.resolve() != Path(current_notebook_dir).resolve()\n",
    "            )\n",
    "            \n",
    "            if hardwired_path_used_and_different:\n",
    "                if nbconvert_output_path_in_notebook_dir.exists(): # Check it exists before trying to delete\n",
    "                    try:\n",
    "                        nbconvert_output_path_in_notebook_dir.unlink()\n",
    "                        print(f\"INFO: Removed intermediate Markdown file from notebook directory: '{nbconvert_output_path_in_notebook_dir}' as output was redirected to '{final_processed_markdown_path}'.\")\n",
    "                    except OSError as e:\n",
    "                        print(f\"WARNING: Could not remove intermediate Markdown file '{nbconvert_output_path_in_notebook_dir}': {e}\")\n",
    "            # If hardwired_path_used_and_different is False, it means either:\n",
    "            # 1. No hardwired path was used (output is in notebook_dir, overwrite happened via process_markdown, no delete needed).\n",
    "            # 2. Hardwired path was used but it resolved to the same as notebook_dir (overwrite happened, no delete needed).\n",
    "        else: # processing_successful is False\n",
    "            print(f\"WARNING: Custom post-processing encountered an issue. The intended output '{final_processed_markdown_path}' may require review or might not be complete.\")\n",
    "            # In case of processing failure, we leave the intermediate nbconvert output in place for debugging.\n",
    "\n",
    "        # Note: If OPTIONAL_HARWIRED_OUTPUT_PATH directs output to a different location\n",
    "        # and processing is successful, the initial nbconvert output in the notebook's\n",
    "        # directory is removed. This ensures the final file exists only in the specified\n",
    "        # hardwired location. If no hardwired path is used, or if it points to the\n",
    "        # same directory as the notebook, the initial file is overwritten by the\n",
    "        # processed version in the notebook's directory.\n",
    "    else: # nbconvert_output_path_in_notebook_dir does not exist\n",
    "        print(f\"\\nERROR: The 'nbconvert' output file '{nbconvert_output_path_in_notebook_dir}' was not found. Skipping custom post-processing.\")\n",
    "\n",
    "# --- Part 4: Execute the Orchestration ---\n",
    "export_notebook_and_apply_custom_processing()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
