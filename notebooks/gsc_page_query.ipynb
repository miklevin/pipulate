{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Setup Requirements for GSC API Access\n",
    "\n",
    "Before running this script, you'll need to set up:\n",
    "\n",
    "1. **Google Cloud Platform (GCP) Project**:\n",
    "   - Create or use an existing GCP project at [console.cloud.google.com](https://console.cloud.google.com)\n",
    "   - Enable the **Google Search Console API** in your project\n",
    "\n",
    "2. **Service Account**:\n",
    "   - Go to IAM & Admin > Service Accounts in GCP\n",
    "   - Create a new service account with a descriptive name\n",
    "   - Download the JSON key file (save as `service-account-key.json`)\n",
    "   - Copy the service account email address to a text file (`service_account.txt`)\n",
    "\n",
    "3. **GSC Permissions**:\n",
    "   - Open Google Search Console for your site\n",
    "   - Go to Settings > Users and Permissions\n",
    "   - Add the service account email as a user with \"Full\" permissions\n",
    "   - Wait a few minutes for permissions to propagate\n",
    "\n",
    "This gives your script API access to your GSC data without requiring browser-based authentication each time it runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "threshold = 31  # For striking distance keywords, for example position 11 or higher\n",
    "working_folder = \"/home/mike/repos/pipulate/notebooks\"  # Leave off the trailing slash\n",
    "semrush_file = f\"{working_folder}/semrush_bulk.csv\"  # From SEMrush Keyword Overview\n",
    "\n",
    "def check_requirements():\n",
    "    missing_items = []\n",
    "    \n",
    "    # Check working folder\n",
    "    if not os.path.exists(working_folder):\n",
    "        missing_items.append(f\"Working folder: {working_folder}\")\n",
    "    \n",
    "    # Check service account files\n",
    "    service_account_txt = f'{working_folder}/service_account.txt'\n",
    "    service_account_json = f'{working_folder}/service-account-key.json'\n",
    "    \n",
    "    if not os.path.exists(service_account_txt):\n",
    "        missing_items.append(f\"Service account email file: {service_account_txt}\")\n",
    "    if not os.path.exists(service_account_json):\n",
    "        missing_items.append(f\"Service account key file: {service_account_json}\")\n",
    "    \n",
    "    print(\"✓ All required files and locations are present\")\n",
    "\n",
    "# Run the checks\n",
    "check_requirements()\n",
    "\n",
    "# Read service account email from text file\n",
    "with open(f'{working_folder}/service_account.txt', 'r') as f:\n",
    "    service_account_email = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Path to your service account key JSON file\n",
    "SERVICE_ACCOUNT_FILE = f'{working_folder}/service-account-key.json'\n",
    "\n",
    "# Define the required scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/webmasters']\n",
    "\n",
    "# Authenticate using service account\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "# Build the service\n",
    "webmasters_service = build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "# List sites (properties) accessible to this service account\n",
    "site_list = webmasters_service.sites().list().execute()\n",
    "\n",
    "# Print the list of sites\n",
    "print(\"Sites accessible to this service account:\")\n",
    "for site in site_list.get('siteEntry', []):\n",
    "    url = site.get('siteUrl')\n",
    "    permission_level = site.get('permissionLevel')\n",
    "    print(f\"URL: {url}, Permission: {permission_level}\")\n",
    "\n",
    "print(f\"\\nTotal sites: {len(site_list.get('siteEntry', []))}\")\n",
    "# print(f\"Service account email: {service_account_email}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# CELL: Find Most Recent GSC Data Date\n",
    "# ======================================================================\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the site we're working with (from first code block)\n",
    "target_site = site_list['siteEntry'][0]['siteUrl']\n",
    "print(f\"Finding most recent data for site: {target_site}\")\n",
    "\n",
    "# Function to check if data exists for a specific date\n",
    "def check_date_has_data(service, site_url, check_date):\n",
    "    # Create a simple 1-day query with minimal dimensions\n",
    "    date_str = check_date.strftime('%Y-%m-%d')\n",
    "    test_request = {\n",
    "        'startDate': date_str,\n",
    "        'endDate': date_str,\n",
    "        'dimensions': ['query'],  # Just query dimension for a quick check\n",
    "        'rowLimit': 1  # We only need to know if any data exists\n",
    "    }\n",
    "    \n",
    "    # Execute the query\n",
    "    response = service.searchanalytics().query(siteUrl=site_url, body=test_request).execute()\n",
    "    return len(response.get('rows', [])) > 0\n",
    "\n",
    "# Start with yesterday and work backwards\n",
    "current_date = datetime.now().date() - timedelta(days=1)\n",
    "max_days_to_check = 10  # Limit how far back we'll check\n",
    "days_checked = 0\n",
    "\n",
    "print(f\"Starting with date: {current_date}\")\n",
    "\n",
    "# Loop until we find data or hit our limit\n",
    "while days_checked < max_days_to_check:\n",
    "    print(f\"Checking if data exists for {current_date}...\", end=\" \")\n",
    "    \n",
    "    if check_date_has_data(webmasters_service, target_site, current_date):\n",
    "        print(\"✓ Data found!\")\n",
    "        most_recent_data_date = current_date\n",
    "        break\n",
    "    else:\n",
    "        print(\"✗ No data\")\n",
    "        current_date -= timedelta(days=1)\n",
    "        days_checked += 1\n",
    "        time.sleep(0.5)  # Small pause to avoid rate limiting\n",
    "    \n",
    "if days_checked >= max_days_to_check:\n",
    "    print(f\"Warning: Couldn't find data in the last {max_days_to_check} days\")\n",
    "    most_recent_data_date = current_date + timedelta(days=1)  # Use the last date we didn't check\n",
    "else:\n",
    "    print(f\"\\nMost recent GSC data available is for: {most_recent_data_date}\")\n",
    "    \n",
    "# Calculate a reasonable default date range (3 months ending at most recent date)\n",
    "default_end_date = most_recent_data_date\n",
    "default_start_date = default_end_date - timedelta(days=3)\n",
    "\n",
    "print(f\"Recommended date range for full queries:\")\n",
    "print(f\"  Start: {default_start_date}\")\n",
    "print(f\"  End: {default_end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# CELL: Query GSC Data with 3-Day Date Range and Convert to DataFrame\n",
    "# ======================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set pandas display options to show all rows/columns without limits\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Use a 3-day range instead of 90 days\n",
    "default_start_date = default_end_date - timedelta(days=3)\n",
    "\n",
    "# Prepare the query request using the updated dates\n",
    "request = {\n",
    "    'startDate': default_start_date.strftime('%Y-%m-%d'),\n",
    "    'endDate': default_end_date.strftime('%Y-%m-%d'),\n",
    "    'dimensions': ['query', 'page'],\n",
    "    'rowLimit': 25000,  # Maximum allowed per request\n",
    "    'startRow': 0\n",
    "}\n",
    "\n",
    "print(f\"Querying data for site: {target_site}\")\n",
    "print(f\"Date range: {default_start_date.strftime('%Y-%m-%d')} to {default_end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# ======================================================================\n",
    "# FUNCTION: Fetch All GSC Data (with pagination)\n",
    "# ======================================================================\n",
    "\n",
    "def fetch_all_gsc_data(service, site_url, request, max_rows=100000):\n",
    "    \"\"\"\n",
    "    Fetch all data from Google Search Console API with pagination.\n",
    "    \n",
    "    Args:\n",
    "        service: The GSC API service object\n",
    "        site_url: The site URL to query\n",
    "        request: The query request body\n",
    "        max_rows: Maximum total rows to fetch (default: 100000)\n",
    "        \n",
    "    Returns:\n",
    "        List of row data from GSC\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    rows_fetched = 0\n",
    "    start_row = 0\n",
    "    \n",
    "    while rows_fetched < max_rows:\n",
    "        # Update the startRow in the request\n",
    "        request['startRow'] = start_row\n",
    "        \n",
    "        # Execute the query\n",
    "        response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()\n",
    "        \n",
    "        # Get the rows from the response\n",
    "        current_rows = response.get('rows', [])\n",
    "        \n",
    "        # If no more rows returned, we're done\n",
    "        if not current_rows:\n",
    "            break\n",
    "            \n",
    "        # Add these rows to our list\n",
    "        all_rows.extend(current_rows)\n",
    "        \n",
    "        # Update counters for next iteration\n",
    "        num_current_rows = len(current_rows)\n",
    "        rows_fetched += num_current_rows\n",
    "        start_row += num_current_rows\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Fetched {rows_fetched} rows so far...\")\n",
    "        \n",
    "    print(f\"Completed fetching {rows_fetched} total rows\")\n",
    "    return all_rows\n",
    "\n",
    "\n",
    "# Execute the query using our existing function\n",
    "all_data = fetch_all_gsc_data(webmasters_service, target_site, request)\n",
    "\n",
    "# Convert the API response to a DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Split the 'keys' column into separate columns\n",
    "if 'keys' in df.columns:\n",
    "    # The first element in keys is 'query', the second is 'page'\n",
    "    df['query'] = df['keys'].apply(lambda x: x[0])\n",
    "    df['page'] = df['keys'].apply(lambda x: x[1])\n",
    "    df = df.drop('keys', axis=1)\n",
    "\n",
    "# Ensure metrics are numeric types\n",
    "for col in ['clicks', 'impressions', 'position', 'ctr']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "# Convert CTR to percentage for better readability\n",
    "df['ctr'] = df['ctr'] * 100\n",
    "\n",
    "# Preview the DataFrame\n",
    "print(\"\\nDataFrame Preview (30 rows):\")\n",
    "print(df.head(30))\n",
    "\n",
    "# Print basic stats about the data\n",
    "print(f\"\\nTotal number of rows: {len(df)}\")\n",
    "print(f\"Unique queries: {df['query'].nunique()}\")\n",
    "print(f\"Unique pages: {df['page'].nunique()}\")\n",
    "print(f\"Total clicks: {df['clicks'].sum()}\")\n",
    "print(f\"Total impressions: {df['impressions'].sum()}\")\n",
    "print(f\"Average position: {df['position'].mean():.2f}\")\n",
    "print(f\"Average CTR: {df['ctr'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# CELL: Find Striking Distance Keywords\n",
    "# ======================================================================\n",
    "\n",
    "print(f\"Finding striking distance keywords (position >= {threshold})...\")\n",
    "\n",
    "# Step 1: For each query, find its best position across all pages\n",
    "query_best_position = df.groupby('query')['position'].min().reset_index()\n",
    "\n",
    "# Step 2: Filter to keep only queries where even the best position is >= threshold\n",
    "striking_distance_queries = query_best_position[query_best_position['position'] >= threshold]['query'].tolist()\n",
    "print(f\"Found {len(striking_distance_queries)} queries with all positions >= {threshold}\")\n",
    "\n",
    "# Step 3: Create a dataframe with only these striking distance keywords\n",
    "striking_df = df[df['query'].isin(striking_distance_queries)].copy()\n",
    "\n",
    "# Step 4: Aggregate data by query (summing clicks and impressions)\n",
    "striking_agg = striking_df.groupby('query').agg({\n",
    "    'clicks': 'sum',\n",
    "    'impressions': 'sum',\n",
    "    'position': 'mean'  # Average position across all pages\n",
    "}).reset_index()\n",
    "\n",
    "# Sort by impressions (highest first) to prioritize higher volume opportunities\n",
    "striking_agg = striking_agg.sort_values('impressions', ascending=False)\n",
    "\n",
    "# Print details about the striking distance keywords\n",
    "print(\"\\nStriking Distance Keywords (30 rows):\")\n",
    "print(striking_agg.head(30))\n",
    "print(f\"\\nTotal striking distance keywords: {len(striking_agg)}\")\n",
    "\n",
    "# Extract the complete list of keywords, sorted by impressions\n",
    "all_striking_keywords = striking_agg['query'].tolist()\n",
    "print(f\"Prepared {len(all_striking_keywords)} keywords for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# CELL: Clean Keywords for SEMrush\n",
    "# ======================================================================\n",
    "\n",
    "# Set to True to use NLTK analysis, False to use only basic cleaning\n",
    "USE_NLTK_ANALYSIS = False  # Easy toggle\n",
    "\n",
    "# If NLTK is enabled, try to import it (install if needed)\n",
    "if USE_NLTK_ANALYSIS:\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        # Uncomment to download necessary data (first time only)\n",
    "        # nltk.download('punkt')\n",
    "    except ImportError:\n",
    "        print(\"NLTK not installed. Run: pip install nltk\")\n",
    "        USE_NLTK_ANALYSIS = False\n",
    "        \n",
    "# Basic cleaning function\n",
    "def clean_keywords(keywords_list):\n",
    "    cleaned_keywords = []\n",
    "    too_complex = []\n",
    "    \n",
    "    for keyword in keywords_list:\n",
    "        # Remove quotes and normalize spacing\n",
    "        cleaned = keyword.replace('\"', '').replace('\"', '').strip()\n",
    "        \n",
    "        # Check if too long (more than 5 words might be problematic)\n",
    "        if len(cleaned.split()) > 5:\n",
    "            too_complex.append(keyword)\n",
    "            continue\n",
    "            \n",
    "        # Check for special characters (except basic punctuation)\n",
    "        if any(c for c in cleaned if not c.isalnum() and c not in [' ', '-', '.']):\n",
    "            # Try removing special characters\n",
    "            cleaned = ''.join(c for c in cleaned if c.isalnum() or c == ' ')\n",
    "            \n",
    "        # Remove \"vs\" comparisons as they often get flagged\n",
    "        if \" vs \" in cleaned.lower():\n",
    "            words = cleaned.lower().split(\" vs \")\n",
    "            cleaned = words[0]  # Take just the first term\n",
    "            \n",
    "        cleaned_keywords.append(cleaned.strip())\n",
    "        \n",
    "    return cleaned_keywords, too_complex\n",
    "\n",
    "# More advanced analysis using NLTK (if enabled)\n",
    "def analyze_keyword_complexity(keyword):\n",
    "    if not USE_NLTK_ANALYSIS:\n",
    "        return \"skipped_analysis\"\n",
    "        \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(keyword.lower())\n",
    "    \n",
    "    # Check length\n",
    "    if len(tokens) > 5:\n",
    "        return \"too_long\"\n",
    "    \n",
    "    # Check for special tokens\n",
    "    if any(not token.isalnum() for token in tokens):\n",
    "        return \"special_chars\"\n",
    "    \n",
    "    # Check for comparison terms\n",
    "    if \"vs\" in tokens or \"versus\" in tokens:\n",
    "        return \"comparison\"\n",
    "    \n",
    "    return \"ok\"\n",
    "\n",
    "# Process all keywords\n",
    "print(f\"Processing {len(all_striking_keywords)} keywords...\")\n",
    "cleaned_keywords, removed_keywords = clean_keywords(all_striking_keywords)\n",
    "\n",
    "# Analyze complexity if enabled\n",
    "if USE_NLTK_ANALYSIS:\n",
    "    complexity_results = {}\n",
    "    for kw in cleaned_keywords:\n",
    "        result = analyze_keyword_complexity(kw)\n",
    "        if result != \"ok\":\n",
    "            complexity_results[kw] = result\n",
    "    \n",
    "    print(f\"\\nKeyword complexity analysis:\")\n",
    "    for kw, result in complexity_results.items():\n",
    "        print(f\"- '{kw}': {result}\")\n",
    "\n",
    "# Take the top 100 cleaned keywords\n",
    "top_100_cleaned = cleaned_keywords[:100]\n",
    "semrush_formatted = \"\\n\".join(top_100_cleaned)\n",
    "\n",
    "print(f\"\\nCleaned {len(all_striking_keywords)} keywords to {len(cleaned_keywords)}\")\n",
    "print(f\"Removed {len(removed_keywords)} complex keywords\")\n",
    "print(f\"Selected top 100 for SEMrush\")\n",
    "\n",
    "print(\"\\nTop 100 keywords for SEMrush (copy/paste ready):\")\n",
    "print(\"Run the next cell to copy/paste the keywords into SEMrush\")\n",
    "# print(semrush_formatted)\n",
    "\n",
    "# Optionally save to file\n",
    "# with open('semrush_keywords.txt', 'w') as f:\n",
    "#     f.write(semrush_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(semrush_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_items = []\n",
    "# Check SEMrush file\n",
    "if not os.path.exists(semrush_file):\n",
    "    missing_items.append(f\"SEMrush data file: {semrush_file}\")\n",
    "\n",
    "if missing_items:\n",
    "    print(\"Error: The following required files or locations are missing:\")\n",
    "    for item in missing_items:\n",
    "        print(f\"  - {item}\")\n",
    "    print(\"\\nPlease ensure all required files are in place and adjust the paths if needed:\")\n",
    "    print(f\"1. Place your SEMrush data file in the working folder and update 'semrush_file' path\")\n",
    "    sys.exit(1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_semrush = pd.read_csv(semrush_file)\n",
    "print(df_semrush.columns)\n",
    "print(df_semrush.head())\n",
    "print(df_semrush.describe())\n",
    "print(df_semrush.info())\n",
    "print(df_semrush.shape)\n",
    "print(df_semrush.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# CELL: Join GSC Striking Distance Keywords with SEMrush Data\n",
    "# ======================================================================\n",
    "import numpy as np\n",
    "\n",
    "# First, let's clean up the SEMrush dataframe a bit (nulls and capitalization)\n",
    "df_semrush['Keyword'] = df_semrush['Keyword'].str.lower()  # Make lowercase to match GSC data\n",
    "df_semrush_clean = df_semrush.copy()\n",
    "\n",
    "# Now join the striking distance keywords with SEMrush data\n",
    "merged_df = striking_agg.merge(\n",
    "    df_semrush_clean,\n",
    "    left_on='query',\n",
    "    right_on='Keyword',\n",
    "    how='left'  # Left join to keep all striking distance keywords\n",
    ")\n",
    "\n",
    "# Fill NaN values for Volume (some keywords may not be in SEMrush)\n",
    "merged_df['Volume'] = merged_df['Volume'].fillna(0).astype(int)\n",
    "merged_df['Keyword Difficulty'] = merged_df['Keyword Difficulty'].fillna(0)\n",
    "merged_df['CPC (USD)'] = merged_df['CPC (USD)'].fillna(0.0)\n",
    "\n",
    "# Create an opportunity score (basic version)\n",
    "# High impressions + decent position + high volume + lower difficulty = better opportunity\n",
    "merged_df['Opportunity Score'] = (\n",
    "    merged_df['impressions'] * 0.4 +                # More impressions is good\n",
    "    merged_df['Volume'] * 0.4 +                     # More volume is good\n",
    "    (100 - merged_df['Keyword Difficulty']) * 0.1 + # Lower difficulty is good\n",
    "    (30 - merged_df['position']).clip(lower=0) * 0.1 # Better position (closer to 11) is good\n",
    ")\n",
    "\n",
    "# Sort by opportunity score\n",
    "merged_df = merged_df.sort_values('Opportunity Score', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top Opportunities (Keywords with SEMrush data):\")\n",
    "columns_to_show = ['query', 'impressions', 'position', 'Volume', 'Keyword Difficulty', 'CPC (USD)', 'Opportunity Score']\n",
    "print(merged_df[columns_to_show].head(30))\n",
    "\n",
    "# Summarize match rate\n",
    "total_keywords = len(striking_agg)\n",
    "matched_keywords = merged_df['Volume'].replace(0, np.nan).count()\n",
    "match_rate = (matched_keywords / total_keywords) * 100\n",
    "\n",
    "print(f\"\\nKeyword Match Statistics:\")\n",
    "print(f\"Total striking distance keywords: {total_keywords}\")\n",
    "print(f\"Keywords found in SEMrush: {matched_keywords} ({match_rate:.1f}%)\")\n",
    "\n",
    "# Create a list of prioritized keywords based on this analysis\n",
    "# This filters to keep only keywords that have SEMrush data\n",
    "semrush_matched_keywords = merged_df[merged_df['Volume'] > 0].copy()\n",
    "\n",
    "print(f\"\\nTop 20 Opportunity Keywords (with SEMrush data):\")\n",
    "for idx, row in semrush_matched_keywords.head(20).iterrows():\n",
    "    print(f\"{row['query']} - Vol: {row['Volume']} - KD: {row['Keyword Difficulty']} - Pos: {row['position']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# CELL: Generate SEO Title Optimization Template\n",
    "# ======================================================================\n",
    "\n",
    "# Function to generate the SEO table and template\n",
    "def generate_seo_template(df, num_keywords=20, filter_list=None):\n",
    "    # Default filter list if none provided\n",
    "    if filter_list is None:\n",
    "        filter_list = [\"mike levin\"]\n",
    "    \n",
    "    # Filter to only include rows with volume > 0 and exclude filter_list terms\n",
    "    filtered_df = df[df['Volume'] > 0].copy()\n",
    "    for term in filter_list:\n",
    "        filtered_df = filtered_df[~filtered_df['query'].str.contains(term, case=False)]\n",
    "    \n",
    "    # Sort by Opportunity Score and take top N\n",
    "    filtered_df = filtered_df.sort_values('Opportunity Score', ascending=False).head(num_keywords)\n",
    "    \n",
    "    # Define column widths\n",
    "    col_widths = {\n",
    "        'keyword': 30,\n",
    "        'relevance': 10,\n",
    "        'volume': 8,\n",
    "        'kd': 6,\n",
    "        'position': 10,\n",
    "        'opportunity': 12\n",
    "    }\n",
    "    \n",
    "    # Generate the table header with proper column widths\n",
    "    header = f\"| {'Keyword'.ljust(col_widths['keyword'])} | {'Relevance'.center(col_widths['relevance'])} | {'Volume'.center(col_widths['volume'])} | {'KD'.center(col_widths['kd'])} | {'Position'.center(col_widths['position'])} | {'Opportunity'.center(col_widths['opportunity'])} |\"\n",
    "    separator = f\"|{'-' * (col_widths['keyword'] + 2)}|{'-' * (col_widths['relevance'] + 2)}|{'-' * (col_widths['volume'] + 2)}|{'-' * (col_widths['kd'] + 2)}|{'-' * (col_widths['position'] + 2)}|{'-' * (col_widths['opportunity'] + 2)}|\"\n",
    "    \n",
    "    table = header + \"\\n\" + separator + \"\\n\"\n",
    "    \n",
    "    # Generate the table rows with proper alignment\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        keyword = row['query']\n",
    "        # Truncate long keywords with ellipsis\n",
    "        if len(keyword) > col_widths['keyword'] - 1:\n",
    "            keyword = keyword[:col_widths['keyword'] - 4] + \"...\"\n",
    "        \n",
    "        volume = str(int(row['Volume']))\n",
    "        kd = f\"{row['Keyword Difficulty']:.1f}\"\n",
    "        position = f\"{row['position']:.1f}\"\n",
    "        opportunity = f\"{row['Opportunity Score']:.1f}\"\n",
    "        \n",
    "        # Format the row with proper alignment\n",
    "        table_row = f\"| {keyword.ljust(col_widths['keyword'])} | {'?%'.center(col_widths['relevance'])} | {volume.rjust(col_widths['volume'])} | {kd.rjust(col_widths['kd'])} | {position.rjust(col_widths['position'])} | {opportunity.rjust(col_widths['opportunity'])} |\"\n",
    "        table += table_row + \"\\n\"\n",
    "    \n",
    "    # Generate the complete template\n",
    "    template = f\"\"\"\n",
    "# SEO Title & Permalink Optimization Template\n",
    "\n",
    "## Top Keyword Opportunities\n",
    "\n",
    "{table}\n",
    "---\n",
    "\n",
    "# Paste article here\n",
    "\n",
    "---\n",
    "\n",
    "Analyze this article draft and the provided SEO data table. \n",
    "\n",
    "1. Identify the 3-5 striking distance keywords from the table that best match the article's actual content and themes.\n",
    "\n",
    "2. For each matching keyword, rate its relevance to the article content on a scale of 0-100%.\n",
    "\n",
    "3. Based on the most relevant, high-opportunity keywords, suggest:\n",
    "   - An engaging title in Title Case (60-70 characters max)\n",
    "   - A permalink slug using hyphenated lowercase (3-5 words max)\n",
    "   - Brief rationale explaining why this keyword strategy makes sense\n",
    "   \n",
    "4. The title should accurately represent the article content while incorporating the target keyword(s) naturally.\n",
    "\n",
    "5. Consider user intent: would someone searching this term be satisfied by this content?\n",
    "\"\"\"\n",
    "    \n",
    "    return template\n",
    "\n",
    "# Generate and display the template\n",
    "seo_template = generate_seo_template(merged_df, num_keywords=100)\n",
    "print(seo_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
