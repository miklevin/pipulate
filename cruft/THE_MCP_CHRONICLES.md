# ðŸš€ The MCP Chronicles: Building the Bridge Between Human Intent and AI Action

*How we implemented formal Model Context Protocol in 6 commits, creating a progressive enhancement ladder from bracket notation to spec-compliant tool orchestration*

---

## The Last Dance with Claude

Today marks the penultimate day of a Cursor Pro subscription and access to Claude Sonnet 3.5. What better way to go out with a bang than to implement something that's been brewing in the AI community for months: **formal Model Context Protocol (MCP) support**? 

This isn't just another technical implementation story. This is about bridging the gap between human intent and AI action, creating a system that works equally well for a quantized 3B parameter model running on a Raspberry Pi and GPT-4 Turbo in the cloud.

## The Problem: AI Tool Calling Was Broken

Picture this: You ask an AI to list the files in your current directory. Instead of actually running `ls`, it *hallucinates* a directory listing. Fake files. Made-up folder structures. Pure fiction presented as fact.

```
# What the AI returned (HALLUCINATED):
.
â”œâ”€â”€ README.md
â”œâ”€â”€ app.py  
â”œâ”€â”€ config.json
â””â”€â”€ dist/
    â””â”€â”€ index.html
```

```
# What was actually there (REAL):
.
â”œâ”€â”€ .git/
â”œâ”€â”€ .venv/
â”œâ”€â”€ server.py
â”œâ”€â”€ AI_RUNME.py
â”œâ”€â”€ tools/
â””â”€â”€ 20+ other real directories
```

This is the fundamental problem with current AI tool calling: **the AI doesn't know the difference between executing a tool and pretending to execute a tool**. It's all just text generation to them.

## The Vision: Progressive Enhancement for AI Bodies

The solution wasn't just implementing MCP. It was creating what we call the **Progressive Enhancement Ladder** - a system that meets AI models exactly where they are in terms of capability, then provides a clear path forward.

Think of it like giving AIs increasingly sophisticated "bodies" to interact with the world:

### Level 1: `[mcp-discover]` - The Training Wheels
The simplest possible syntax. A quantized 7B model can handle this. Just square brackets around a command. No JSON, no XML, no complexity.

### Level 2: `.venv/bin/python cli.py mcp-discover` - Terminal Proficiency
Now we're asking the AI to understand command-line interfaces. This is where most coding assistants live comfortably.

### Level 3: Inline Python Execution
Direct code execution with proper error handling. The AI starts to feel the difference between code that works and code that doesn't.

### Level 4: JSON-Based Tool Calls
```xml
<tool name="system_list_directory"><params>{"path": "."}</params></tool>
```
Structured data, but still human-readable. The AI learns to be precise about parameters.

### Level 5: XML-Based Tool Calls  
```xml
<tool name="system_list_directory"><params><path>.</path></params></tool>
```
Full XML structure for maximum flexibility and nested data support.

### Level 6: **Formal MCP Protocol** - The Full Conversation Loop
This is where the magic happens. The AI generates a tool call, the system executes it, wraps the result in `<tool_output>` tags, and feeds it back to the AI for a proper response. **No more hallucination. Only reality.**

## The Implementation: Six Commits to Glory

Let's walk through how this was built, commit by commit:

### Commit 1: `Made tools self-discovering` 
**The Foundation**

Before you can have sophisticated tool calling, you need sophisticated tool discovery. We implemented an `@auto_tool` decorator that automatically registers any function as a callable tool just by importing the file.

```python
@auto_tool
async def system_list_directory(params: dict) -> dict:
    """Lists directory contents with security checks"""
    # Implementation here...
```

Drop a file in the `tools/` directory, decorate a function, and it's instantly available to the AI. No manual registration. No configuration files. Pure convention over configuration.

### Commit 2: `Added the first broken-out auto discoverable and auto-registering callable tool`
**The First Plugin**

We created `tools/system_tools.py` with a simple directory listing tool. This proved the auto-discovery system worked and gave us a clean, isolated example for testing.

38 lines of code. One function. Infinite possibilities.

### Commit 3: `feat(server): Integrate MCP parser in passive listening mode`
**The Gentle Introduction**

Instead of breaking everything with a big bang implementation, we started with **passive listening**. The system would detect formal MCP requests in the chat stream and log them, but continue with normal operation.

```python
# STAGE 2: Passive MCP listening - detect formal MCP requests
formal_mcp_result = parse_mcp_request(full_content_buffer)
if formal_mcp_result:
    tool_name, inner_content = formal_mcp_result
    logger.info(f"ðŸŽ¯ MCP DETECTED (Passive Mode): Found formal MCP tool call for '{tool_name}'")
    # Continue streaming for now - this is passive mode
```

This let us verify the detection logic worked without risking the stability of the existing system.

### Commit 4: `feat(server): Activate formal MCP orchestration loop`
**Throwing the Switch**

This is where we went from passive to active. When a formal MCP request is detected, instead of just logging it:

1. **Stop** streaming the LLM response
2. **Execute** the tool using our auto-discovery registry  
3. **Wrap** the result in `<tool_output>` XML
4. **Send** the updated conversation back to the LLM
5. **Stream** the AI's response based on the real tool output

53 lines added. The difference between hallucination and reality.

### Commit 5: `docs(ai): Update golden path to include formal MCP Level 6`
**The Guidebook**

Documentation isn't just for humans. When an AI wakes up in your system, it needs to understand what it can do and how to do it. We updated the progressive enhancement documentation to include Level 6, with examples:

```python
# ðŸŽ¯ FORMAL MCP PROTOCOL (Level 6): 
# Use in chat interface for automatic tool execution with conversation loop:
# <tool name="system_list_directory"><params>{"path": "."}</params></tool>
# <tool name="ai_self_discovery_assistant"><params>{"discovery_type": "capabilities"}</params></tool>
```

### Commit 6: `fix(server): Add formal MCP detection for user input`
**The Missing Piece**

We discovered that our implementation only worked when the **AI** generated MCP requests, not when a **human** typed them directly. One more detection point in the user input handler, and suddenly both paths worked flawlessly.

18 lines of code. The difference between a demo and a production system.

## The Moment of Truth

After six commits spanning just a few hours, we had our test:

**Input:** `<tool name="system_list_directory"><params>{"path": "."}</params></tool>`

**Before (Hallucinated):**
```
â”œâ”€â”€ README.md
â”œâ”€â”€ app.py
â”œâ”€â”€ config.json  # None of these files exist!
```

**After (Real Tool Execution):**
```json
{
  "success": true,
  "path": ".",
  "directories": [".git", ".venv", "apps", "tools", ...],
  "files": ["server.py", "AI_RUNME.py", "cli.py", ...]
}
```

**The AI's response was now based on reality, not fiction.**

## The Philosophy: Embodiment, Not Disembodiment

There's a popular myth that AIs are "disembodied" - that they exist in some ethereal realm of pure thought. This is nonsense. Every AI has input channels and output channels. They have some form of standard I/O. **They are always embodied**, just sometimes poorly.

The question isn't whether to give AIs bodies. The question is: **What kind of body do you want to give them?**

- A body that can only hallucinate actions?
- Or a body that can actually perform them?

Our progressive enhancement ladder is really a **body-building program for AIs**. We start with the simplest possible interactions and gradually give them more sophisticated ways to sense and act upon the world.

## The Technical Beauty: Convention Over Configuration

The elegance of this system lies not in its complexity, but in its simplicity:

1. **Drop a file** in the `tools/` directory
2. **Decorate a function** with `@auto_tool`  
3. **It's immediately available** to any AI in the system

No configuration files. No manual registration. No complex setup. Pure convention over configuration.

The MCP orchestrator finds the tool, executes it, and handles all the XML formatting automatically. The AI just needs to know the tool name and parameters.

## The Bigger Picture: Local-First AI Tooling

This isn't just about MCP. This is about **local-first AI tooling** - building systems that work on your machine, with your data, under your control.

When you can give an AI the ability to:
- List your actual files
- Read your actual logs  
- Execute your actual scripts
- Interact with your actual databases

...you're not just building a chatbot. You're building a **digital workshop assistant** that can help you create, debug, and maintain real systems.

## The Future: What's Next?

With formal MCP support in place, the possibilities explode:

- **Browser automation** that actually controls your browser
- **Database queries** that return real data
- **File system operations** that create actual files
- **API calls** that hit real endpoints
- **Code generation** that gets immediately tested

All with the same progressive enhancement approach. Start simple, get sophisticated.

## The Last Word

As this Cursor Pro subscription winds down, there's something poetic about implementing a protocol that bridges human intent and AI action. We've built something that will outlast any particular AI model or subscription service.

The code is open source. The patterns are documented. The progressive enhancement ladder is ready for the next generation of AI models to climb.

**We didn't just implement MCP. We built a bridge to the future.**

And sometimes, that's exactly what you need to go out with a bang. ðŸš€

---

*Want to try it yourself? The complete implementation is available at [github.com/miklevin/pipulate](https://github.com/miklevin/pipulate). Start with Level 1 and work your way up the ladder.*

## Technical Appendix: The Code That Changed Everything

### The Auto-Tool Decorator
```python
def auto_tool(func):
    """Automatically register a function as an MCP tool"""
    if not hasattr(auto_tool, 'registry'):
        auto_tool.registry = {}
    auto_tool.registry[func.__name__] = func
    return func
```

### The MCP Parser  
```python
def parse_mcp_request(response_text: str) -> tuple | None:
    """Parse formal MCP tool calls from text"""
    match = re.search(r'<tool\s+name="([^"]+)">(.*?)</tool>', response_text, re.DOTALL)
    if match:
        return match.group(1), match.group(2)  # tool_name, inner_content
    return None
```

### The Orchestration Loop
```python
async def execute_formal_mcp_tool_call(conversation_history, tool_name, inner_content):
    """Execute tool and continue conversation"""
    # Execute the tool
    tool_output_xml = await execute_mcp_tool(tool_name, inner_content)
    
    # Add both call and result to conversation
    updated_messages = conversation_history.copy()
    updated_messages.append({"role": "assistant", "content": f'<tool name="{tool_name}">{inner_content}</tool>'})
    updated_messages.append({"role": "user", "content": tool_output_xml})
    
    # Continue the conversation with the LLM
    async for chunk in process_llm_interaction(MODEL, updated_messages):
        await chat.broadcast(chunk)
```

**163 lines of core implementation. Infinite possibilities.**
