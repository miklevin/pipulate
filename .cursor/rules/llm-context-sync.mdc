---
description: 
globs: 
alwaysApply: false
---
# LLM Context Synchronization Pattern

This rule ensures the LLM maintains awareness of what users see in the UI, enabling more contextual responses.

## Core Principle
The LLM should be "seeing" everything the user sees, plus relevant behind-the-scenes context. This is achieved through Pipulate's OrderedMessageQueue system, which automatically synchronizes UI messages with the LLM's conversation history and maintains workflow state.

## Implementation Patterns

### 1. Message Queue Usage
The OrderedMessageQueue is the primary way to send messages to both the UI and LLM:
```python
await self.message_queue.add(
    pipulate,
    message,
    verbatim=True,  # True for direct messages, False for LLM processing
    role="system",  # "system", "user", or "assistant"
    spaces_before=None,  # Optional spacing before message
    spaces_after=1      # Optional spacing after message
)
```

### 2. Workflow State Tracking
The OrderedMessageQueue automatically tracks workflow state:
```python
# State is updated automatically based on message content
[WORKFLOW STATE: Not yet started]
[INFO] Workflow initialized...

[WORKFLOW STATE: Waiting for Step 1 input]
[PROMPT] Step 1: Please enter Your Name.

[WORKFLOW STATE: Step 1 completed]
[INFO] Step 1 complete: Name entered
```

### 3. Message Types and Context

#### System State Changes
```python
await self.message_queue.add(
    pip,
    "⚠️ Invalid Botify API token. Please check before finalizing.",
    verbatim=True,
    role="system"
)
# Results in:
# [WORKFLOW STATE: Current state]
# [INFO] ⚠️ Invalid Botify API token...
```

#### User Feedback
```python
await self.message_queue.add(
    pip,
    f"✅ Operation successful: {formatted_detail}",
    verbatim=True,
    role="user"
)
# Results in:
# [WORKFLOW STATE: Current state]
# [USER INPUT] ✅ Operation successful...
```

#### LLM Interactions
```python
await self.message_queue.add(
    pip,
    f"Greet {formatted_username} as their assistant",
    verbatim=False,  # Will be processed by LLM
    role="system"
)
# Results in:
# [WORKFLOW STATE: Current state]
# [PROMPT] Greet user...
```

### 4. Message Queue Features

The OrderedMessageQueue system ensures:
- Ordered message delivery
- Automatic conversation history management
- Proper role attribution (system/user/assistant)
- URL escaping for links
- Consistent spacing and formatting
- Workflow state tracking and context
- Automatic step progress detection

### 5. State Context Markers

Every message is automatically prefixed with workflow state:
- `[WORKFLOW STATE: Not yet started]` - Initial state
- `[WORKFLOW STATE: Waiting for Step X input]` - Step awaiting user input
- `[WORKFLOW STATE: Step X completed]` - Step completion state

### 6. Message Type Markers

Messages are automatically categorized:
- `[PROMPT]` - System messages requesting input
- `[INFO]` - System messages providing information
- `[USER INPUT]` - Messages from the user
- `[RESPONSE]` - Messages from the assistant

## Implementation in [plugins/020_hello_workflow.py](mdc:plugins/020_hello_workflow.py)

### Key Points for Context Sync

1. **Message Queue is the Single Source of Truth**
   - All user-visible messages should go through message_queue.add()
   - The queue automatically handles conversation history and state tracking
   ```python
   await self.message_queue.add(pip, message, verbatim=True)
   ```

2. **UI Text Elements**
   - When showing explanatory text, use the message queue:
   ```python
   explanation = "Text shown to user..."
   await self.message_queue.add(pip, explanation, verbatim=True)
   return P(explanation, style=pip.get_style("muted"))
   ```

3. **State Changes**
   - State is tracked automatically based on message content:
   ```python
   state_msg = f"Step {step_num} complete: {value}"
   await self.message_queue.add(pip, state_msg, verbatim=True)
   # State automatically updates to completed
   ```

4. **Validation & Errors**
   - Sync validation messages:
   ```python
   is_valid, error_msg, error_component = pip.validate_step_input(...)
   if not is_valid:
       await self.message_queue.add(pip, f"Validation failed: {error_msg}", verbatim=True)
   ```

## Best Practices

1. Always use message_queue.add() for any message that should be visible to either the user or LLM
2. Keep messages concise and descriptive
3. Use appropriate roles (system/user/assistant)
4. Set verbatim=True for direct messages, False for LLM processing
5. Consider spacing parameters for visual clarity
6. Let the queue handle state tracking automatically
7. Use standard message formats for consistent state detection

## Common Sync Points

- Landing page loads
- Step transitions
- Form submissions
- Validation results
- State changes (finalize/unfinalize)
- Error messages
- Help text display
- Generated suggestions

## Example Workflow

```python
# 1. Show empty form
await self.message_queue.add(pip, "Showing name input form. No name has been entered yet.", verbatim=True)
# State: Waiting for input

# 2. User submits form
await self.message_queue.add(pip, f"User submitted name: {user_val}", verbatim=True)
# State: Processing submission

# 3. Validation result
if not is_valid:
    await self.message_queue.add(pip, f"Validation failed: {error_msg}", verbatim=True)
    return error_component
# State: Validation failed

# 4. Success confirmation
await self.message_queue.add(pip, f"{step.show}: {processed_val}", verbatim=True)
# State: Step completed
```

This pattern ensures the LLM maintains awareness of the user's current context in the workflow through the centralized message queue system, which automatically handles conversation history management, message ordering, and workflow state tracking.
