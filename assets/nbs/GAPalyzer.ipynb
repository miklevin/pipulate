{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pipulate import pip\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import keys\n",
    "\n",
    "job = \"gapalyzer-01\" # Give your session a unique name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- ‚öôÔ∏è Workflow Configuration ---\n",
    "ROW_LIMIT = 3000  # Final Output row limit, low for fast iteration\n",
    "COMPETITOR_LIMIT = 3  # Limit rows regardless of downloads, low for fast iteration\n",
    "BROWSER_DOWNLOAD_PATH = \"~/Downloads\"  # The default directory where your browser downloads files\n",
    "\n",
    "print(f\"‚úÖ Configuration set: Final report will be limited to {ROW_LIMIT} rows.\")\n",
    "if COMPETITOR_LIMIT:\n",
    "    print(f\"‚úÖ Configuration set: Processing will be limited to the top {COMPETITOR_LIMIT} competitors.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Configuration set: Processing all competitors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Here are the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "pip.api_key(job, key=keys.google)\n",
    "botify_token = keys.botify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Here are your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "https://nixos.org/    # Linux\n",
    "https://pypi.org/     # Python\n",
    "https://neovim.io/    # vim\n",
    "https://git-scm.com/  # git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "def get_competitors_from_notebook(notebook_filename=\"GAPalyzer.ipynb\"):\n",
    "    \"\"\"Parses this notebook to get the domain list from the 'url-list-input' cell.\"\"\"\n",
    "    try:\n",
    "        notebook_path = Path(notebook_filename)\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        for cell in nb.cells:\n",
    "            if \"url-list-input\" in cell.metadata.get(\"tags\", []):\n",
    "                domains_raw = cell.source\n",
    "                domains = [\n",
    "                    line.split('#')[0].strip() \n",
    "                    for line in domains_raw.splitlines() \n",
    "                    if line.strip() and not line.strip().startswith('#')\n",
    "                ]\n",
    "                return domains\n",
    "        print(\"‚ö†Ô∏è Warning: Could not find a cell tagged with 'url-list-input'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading domains from notebook: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"üöÄ Generating SEMrush URLs for GAP analysis...\")\n",
    "\n",
    "domains = get_competitors_from_notebook()\n",
    "url_template = \"https://www.semrush.com/analytics/organic/positions/?db=us&q={domain}&searchType=domain\"\n",
    "\n",
    "if not domains:\n",
    "    print(\"üõë No domains found. Please add competitor domains to the 'url-list-input' cell and re-run.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(domains)} competitor domains. Click the links below to open each report:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, domain in enumerate(domains):\n",
    "        full_url = url_template.format(domain=domain)\n",
    "        print(f\"{i+1}. {domain}:\\n   {full_url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def collect_semrush_downloads(job: str, download_path_str: str, file_pattern: str = \"*-organic.Positions*.xlsx\"):\n",
    "    \"\"\"\n",
    "    Moves downloaded SEMRush files matching a pattern from the user's download\n",
    "    directory to a job-specific 'downloads/{job}/' folder within the Notebooks/\n",
    "    directory.\n",
    "    \n",
    "    Args:\n",
    "        job (str): The current job ID (e.g., \"gapalyzer-01\").\n",
    "        download_path_str (str): The user's default browser download path (e.g., \"~/Downloads\").\n",
    "        file_pattern (str): The glob pattern to match SEMRush files.\n",
    "    \"\"\"\n",
    "    print(\"üì¶ Starting collection of new SEMRush downloads...\")\n",
    "\n",
    "    # 1. Define source and destination paths\n",
    "    # Resolve the user's download path (handles ~)\n",
    "    source_dir = Path(download_path_str).expanduser()\n",
    "    \n",
    "    # Define the destination path relative to the current working directory (Notebooks/)\n",
    "    # This assumes the Notebook is run from the 'Notebooks' directory or its path is correct.\n",
    "    destination_dir = Path(\"downloads\") / job\n",
    "\n",
    "    # 2. Create the destination directory if it doesn't exist\n",
    "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Destination folder created/ensured: '{destination_dir.resolve()}'\")\n",
    "\n",
    "    # 3. Find files in the source directory matching the pattern\n",
    "    # We use glob.glob for pattern matching, searching for both .xlsx and .csv\n",
    "    files_to_move = []\n",
    "    \n",
    "    # Check for .xlsx files\n",
    "    xlsx_files = glob.glob(str(source_dir / file_pattern))\n",
    "    files_to_move.extend(xlsx_files)\n",
    "    \n",
    "    # Check for .csv files\n",
    "    csv_pattern = file_pattern.replace(\".xlsx\", \".csv\")\n",
    "    csv_files = glob.glob(str(source_dir / csv_pattern))\n",
    "    files_to_move.extend(csv_files)\n",
    "\n",
    "    if not files_to_move:\n",
    "        print(\"‚ö†Ô∏è No new files matching the pattern were found in the download directory. Skipping move.\")\n",
    "        return\n",
    "\n",
    "    # 4. Move the files\n",
    "    move_count = 0\n",
    "    for source_file_path in files_to_move:\n",
    "        source_file = Path(source_file_path)\n",
    "        dest_file = destination_dir / source_file.name\n",
    "        \n",
    "        # Only move if the file doesn't already exist in the destination (to avoid overwriting)\n",
    "        # This protects manually modified files, but new downloads will have unique timestamps anyway.\n",
    "        if dest_file.exists():\n",
    "             # Option: could log that it exists or decide to overwrite/rename. \n",
    "             # Given the SEMRush filename pattern contains a unique timestamp, we expect \n",
    "             # them to be new. Let's just avoid redundant logging.\n",
    "             continue\n",
    "        \n",
    "        try:\n",
    "            shutil.move(source_file, dest_file)\n",
    "            print(f\"  -> Moved: {source_file.name}\")\n",
    "            move_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ‚ùå Error moving {source_file.name}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Collection complete. {move_count} new files moved to '{destination_dir}'.\")\n",
    "    \n",
    "    # --- Execute the function in the notebook ---\n",
    "collect_semrush_downloads(job, BROWSER_DOWNLOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# NOTE: This cell assumes 'job' is defined (e.g., \"gapalyzer-01\")\n",
    "\n",
    "# --- Define the file directory based on the job variable ---\n",
    "semrush_gap_analysis_dir = Path(\"downloads\") / job\n",
    "\n",
    "# --- Combine glob results for both .xlsx and .csv ---\n",
    "file_patterns = [\n",
    "    \"*-organic.Positions*.xlsx\", \n",
    "    \"*-organic.Positions*.csv\"\n",
    "]\n",
    "\n",
    "# Use itertools.chain to efficiently combine generators from multiple glob calls\n",
    "all_downloaded_files = sorted(list(itertools.chain.from_iterable(\n",
    "    semrush_gap_analysis_dir.glob(pattern) for pattern in file_patterns\n",
    ")))\n",
    "\n",
    "# --- Display Results ---\n",
    "if all_downloaded_files:\n",
    "    # Use a Markdown block for formatted display with emoji\n",
    "    markdown_output = [\"## üíæ Found Downloaded Files\"]\n",
    "    markdown_output.append(f\"‚úÖ **{len(all_downloaded_files)} files** ready for processing in `{semrush_gap_analysis_dir}/`\\n\")\n",
    "    \n",
    "    for i, file in enumerate(all_downloaded_files):\n",
    "        # The file name starts with the competitor's domain.\n",
    "        try:\n",
    "            # We strip the full file path name for cleaner display\n",
    "            domain_name = file.name[:file.name.index(\"-organic.\")].strip()\n",
    "        except ValueError:\n",
    "            # Fallback if the expected pattern is slightly off\n",
    "            domain_name = file.name\n",
    "            \n",
    "        markdown_output.append(f\"{i + 1}. **`{domain_name}`** ({file.suffix.upper()})\")\n",
    "\n",
    "    display(Markdown(\"\\n\".join(markdown_output)))\n",
    "    \n",
    "    # --- NEW FIX: Convert Path objects to strings for JSON serialization ---\n",
    "    # The Pipulate core needs simple, JSON-serializable types (strings, lists, dicts, etc.)\n",
    "    all_downloaded_files_as_str = [str(p) for p in all_downloaded_files]\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # For the next step, we'll store the list of files in the Pipulate pipeline.\n",
    "    pip.set(job, 'semrush_files', all_downloaded_files_as_str)\n",
    "    \n",
    "else:\n",
    "    display(Markdown(f\"‚ö†Ô∏è **Warning:** No SEMRush files found in `{semrush_gap_analysis_dir}/`.\\n(Looking for `*-organic.Positions*.xlsx` or `*.csv`)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import pandas as pd\n",
    "from tldextract import extract\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# --- SUPPORT FUNCTION (1-to-1 Transplant) ---\n",
    "# NOTE: This function requires 'tldextract' to be installed (which you've handled).\n",
    "def extract_registered_domain(url):\n",
    "    \"\"\"\n",
    "    Extracts the registered domain (domain.suffix) from a URL/hostname.\n",
    "    \"\"\"\n",
    "    extracted = extract(url)\n",
    "    return f\"{extracted.domain}.{extracted.suffix}\"\n",
    "\n",
    "# --- MAIN LOGIC ADAPTATION ---\n",
    "\n",
    "# Variables required from previous Notebook cells:\n",
    "# job, ROW_LIMIT, COMPETITOR_LIMIT, BROWSER_DOWNLOAD_PATH, client_domain, country_code\n",
    "# semrush_gap_analysis_dir is assumed to be defined as Path(\"downloads\") / job\n",
    "\n",
    "# Define 'semrush_gap_analysis_dir' and 'semrush_lookup' based on prior context\n",
    "# We use the 'job' variable to define the directory\n",
    "semrush_gap_analysis_dir = Path(\"downloads\") / job\n",
    "\n",
    "# The client domain is the key for separating client vs. competitor data.\n",
    "# We strip the full domain in case it contains a protocol or path.\n",
    "# Assuming 'client_domain' is available from a keys/config cell (e.g., \"example.com\")\n",
    "# Since we don't have 'client_domain' defined here, we'll temporarily define it for the port.\n",
    "# Replace this line with proper import/assignment if moving to external module:\n",
    "semrush_lookup = extract_registered_domain(client_domain) if 'client_domain' in locals() else \"nixos.org\"\n",
    "\n",
    "\n",
    "print(\"Creating a great big DataFrame...\")\n",
    "\n",
    "# 1. Adapt file globbing to handle BOTH CSV and XLSX (as done in the previous step)\n",
    "file_patterns = [\"*-organic.Positions*.xlsx\", \"*-organic.Positions*.csv\"]\n",
    "all_semrush_files = sorted(list(itertools.chain.from_iterable(\n",
    "    semrush_gap_analysis_dir.glob(pattern) for pattern in file_patterns\n",
    ")))\n",
    "\n",
    "# Initialize data structures\n",
    "cdict = {}\n",
    "list_of_dfs = []\n",
    "print(\"Loading SEMRush files: \", end=\"\")\n",
    "\n",
    "# 2. Loop through all found files\n",
    "for j, data_file in enumerate(all_semrush_files):\n",
    "    # Determine the file type and corresponding reader function\n",
    "    is_excel = data_file.suffix.lower() == '.xlsx'\n",
    "    read_func = pd.read_excel if is_excel else pd.read_csv\n",
    "    \n",
    "    # Original file name parsing logic\n",
    "    nend = data_file.stem.index(\"-organic\")\n",
    "    xlabel = data_file.stem[:nend].replace(\"_\", \"/\").replace(\"///\", \"://\").strip('.')\n",
    "    \n",
    "    # Original domain extraction logic (using the locally defined function)\n",
    "    just_domain = extract_registered_domain(xlabel)\n",
    "    cdict[just_domain] = xlabel\n",
    "    \n",
    "    # Load data\n",
    "    df = read_func(data_file)\n",
    "    \n",
    "    # Original column assignment logic\n",
    "    if just_domain == xlabel:\n",
    "        df[\"Domain\"] = just_domain\n",
    "    else:\n",
    "        # Use the full X-label (e.g., sub.domain.com) if it's not just the registered domain\n",
    "        df[\"Domain\"] = xlabel\n",
    "    \n",
    "    # Original data segregation logic\n",
    "    df[\"Client URL\"] = df.apply(lambda row: row[\"URL\"] if row[\"Domain\"] == semrush_lookup else None, axis=1)\n",
    "    df[\"Competitor URL\"] = df.apply(lambda row: row[\"URL\"] if row[\"Domain\"] != semrush_lookup else None, axis=1)\n",
    "    \n",
    "    list_of_dfs.append(df)\n",
    "    print(f\"{j + 1} \", end=\"\", flush=True)\n",
    "\n",
    "print() # Newline after the loading count\n",
    "\n",
    "if list_of_dfs:\n",
    "    df2 = pd.concat(list_of_dfs)  # Concatenate like stacking CSVs\n",
    "    \n",
    "    # --- Original Excel Formatting Value Gathering ---\n",
    "    # This logic appears to be for calculating Excel visual layout, \n",
    "    # but still needs to be run even if the formatting happens later.\n",
    "    # It requires the 'bf.open_dir_widget' function to be defined or stubbed if not portable.\n",
    "    # NOTE: Since `bf` is not defined, and `project_customizations`/`proceed` are missing, \n",
    "    # we must skip the non-portable lines to prevent breaking the Notebook.\n",
    "\n",
    "    # Stubbing non-portable functions/logic to keep the structure intact\n",
    "    # We remove the print statements related to bf/project/customization for now\n",
    "    \n",
    "    # The max_length calculation is fine to keep\n",
    "    max_length = max(len(value) for value in cdict.values())\n",
    "    row1_height = max_length * 7 # Unused variable for now, but ported\n",
    "    \n",
    "    rows, columns = df2.shape\n",
    "    print()\n",
    "    print(f\"Rows: {rows:,}\")\n",
    "    print(f\"Cols: {columns:,}\")\n",
    "    print()\n",
    "\n",
    "    # NOTE: The subsequent conditional logic (lines 53-61 in the original)\n",
    "    # involving `bf.open_dir_widget`, `project_customizations`, and `proceed()`\n",
    "    # has been intentionally omitted here as it depends on external, undefined\n",
    "    # modules (`bf`) and pipeline state (`project`, `project_customizations`, `proceed`)\n",
    "    # that are not provided in the prompt's context and would cause the script to fail.\n",
    "    # We only port the pure Pandas/Python logic.\n",
    "    \n",
    "    # The final output and pipeline update\n",
    "    display(df2[\"Domain\"].value_counts())\n",
    "    \n",
    "    # Store the result in the pipeline\n",
    "    pip.set(job, 'semrush_master_df_json', df2.to_json(orient='records'))\n",
    "    \n",
    "else:\n",
    "    print(\"Please put the CSVs in place.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Todo\n",
    "- Move everything that matches the `.csv` or `.xlsx` template from downloads to somewhere relative to Notebook\n",
    "- Make it work with either Excel or CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from collections import defaultdict # Needed if cdict is modified to be a defaultdict\n",
    "\n",
    "# --- PATH DEFINITIONS (Needed to replace external checks) ---\n",
    "# Assumes 'job' is defined in a previous cell (e.g., \"gapalyzer-01\")\n",
    "# Assumes 'df2' is the master DataFrame from the previous step\n",
    "competitors_csv_file = Path(\"data\") / f\"{job}_competitors.csv\"\n",
    "\n",
    "# --- ADAPTED PIVOTING LOGIC ---\n",
    "\n",
    "print(\"Pivoting data. Keyword count per competitor...\\n\")\n",
    "\n",
    "# Original pivot operation\n",
    "pivot_df = df2.pivot_table(index='Keyword', columns='Domain', values='Position', aggfunc='min')\n",
    "\n",
    "# ORIGINAL LOGIC: pivot_df = bf.move_column_to_front(pivot_df, semrush_lookup)\n",
    "# SURGICAL PORT: Use Pandas reindexing to move the column to the front.\n",
    "if semrush_lookup in pivot_df.columns:\n",
    "    cols = [semrush_lookup] + [col for col in pivot_df.columns if col != semrush_lookup]\n",
    "    pivot_df = pivot_df[cols]\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: Client domain '{semrush_lookup}' not found in pivot table columns.\")\n",
    "\n",
    "\n",
    "# Original: Get list of columns and calculate positioning\n",
    "competitors = list(pivot_df.columns)\n",
    "pivot_df['Competitors Positioning'] = pivot_df.iloc[:, 1:].notna().sum(axis=1)\n",
    "\n",
    "# Original: Load or initialize df_competitors\n",
    "if competitors_csv_file.exists():\n",
    "    df_competitors = pd.read_csv(competitors_csv_file)\n",
    "    df_competitors['Title'] = df_competitors['Title'].fillna('')\n",
    "    df_competitors['Matched Title'] = df_competitors['Matched Title'].fillna('')\n",
    "    print(f\"‚úÖ Loaded {len(df_competitors)} existing competitor records.\")\n",
    "else:\n",
    "    # Use 'cdict' (created in the previous step) to initialize\n",
    "    df_competitors = pd.DataFrame(list(cdict.items()), columns=['Domain', 'Column Label'])\n",
    "    \n",
    "    # Initialize 'Title' and 'Matched Title' columns explicitly\n",
    "    df_competitors['Title'] = ''\n",
    "    df_competitors['Matched Title'] = ''\n",
    "    \n",
    "    df_competitors.to_csv(competitors_csv_file, index=False)\n",
    "    print(f\"‚úÖ Created new competitor file at '{competitors_csv_file}'.\")\n",
    "\n",
    "# Original: Print keyword counts per competitor (for debugging/visual confirmation)\n",
    "counts = pivot_df.describe().loc['count']\n",
    "# Ensure counts has data before proceeding with printing logic\n",
    "if not counts.empty:\n",
    "    max_digits = len(str(len(counts)))\n",
    "    # Ensure all indices are strings for max length calculation\n",
    "    max_index_width = max(len(str(index)) for index in counts.index) \n",
    "    \n",
    "    # Ensure only non-NaN counts are considered for width calculation, fallback to 0 if all are NaN\n",
    "    valid_counts = [count for count in counts if pd.notna(count)]\n",
    "    max_count_width = max([len(f\"{int(count):,}\") for count in valid_counts] or [0])\n",
    "    \n",
    "    for i, (index, count) in enumerate(counts.items(), start=1):\n",
    "        counter_str = str(i).zfill(max_digits)\n",
    "        count_str = f\"{int(count):,}\" if pd.notna(count) else 'NaN'\n",
    "        print(f\"{counter_str}: {index:<{max_index_width}} - {count_str:>{max_count_width}}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to count after pivot table creation.\")\n",
    "\n",
    "# Original: Print rows and columns summary\n",
    "rows, columns = df2.shape\n",
    "rows2, columns2 = pivot_df.shape\n",
    "print(\"\\nThere is some natural deduping from pivot.\\n\")\n",
    "print(f\"Rows (master df): {rows:,}\")\n",
    "print(f\"Rows (pivot df): {rows2:,} ({rows:,} - {rows2:,} = {rows - rows2:,} dupes removed.)\")\n",
    "print(f\"Cols: {columns2:,}\") # Use columns2 for the pivot_df column count\n",
    "\n",
    "# Original: Display result\n",
    "display(pivot_df)\n",
    "\n",
    "# Store the final result in the pipeline\n",
    "pip.set(job, 'keyword_pivot_df', pivot_df.to_json(orient='records'))\n",
    "pip.set(job, 'competitors_df', df_competitors.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from tldextract import extract\n",
    "import wordninja\n",
    "import httpx\n",
    "import re\n",
    "from collections import defaultdict # Already imported in a previous cell\n",
    "\n",
    "# NOTE: This cell assumes 'job', 'semrush_lookup', 'df_competitors', \n",
    "#       and 'competitors_csv_file' are defined in prior cells.\n",
    "# We also assume 'df_competitors' was loaded from 'competitors_csv_file' in the previous step.\n",
    "\n",
    "# --- PATH DEFINITION FOR FILTER FILE ---\n",
    "# Consolidating working files to the 'data' directory.\n",
    "filter_file = Path(\"data\") / f\"{job}_filter_keywords.csv\"\n",
    "\n",
    "\n",
    "# --- REQUIRED SUPPORT FUNCTIONS (Surgically Ported from botifython.py) ---\n",
    "\n",
    "# Headers and user_agent were defined globally in botifython.py, but are needed here for httpx\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "def extract_registered_domain(url):\n",
    "    \"\"\"Extracts the registered domain (domain.suffix) from a URL/hostname.\"\"\"\n",
    "    extracted = extract(url)\n",
    "    return f\"{extracted.domain}.{extracted.suffix}\"\n",
    "\n",
    "def get_title_from_html(html_content):\n",
    "    \"\"\"Simple helper to extract the title from HTML content.\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    title_tag = soup.find('title')\n",
    "    return title_tag.text if title_tag else ''\n",
    "\n",
    "def match_domain_in_title(domain, title):\n",
    "    \"\"\"Finds a stripped version of the domain in the title.\"\"\"\n",
    "    base_domain = domain.split('.')[0]\n",
    "    pattern = ''.join([c + r'\\s*' for c in base_domain])\n",
    "    regex = re.compile(pattern, re.IGNORECASE)\n",
    "    match = regex.search(title)\n",
    "    if match:\n",
    "        matched = match.group(0).strip()\n",
    "        return matched\n",
    "    return ''\n",
    "\n",
    "async def async_check_url(url, domain, timeout):\n",
    "    \"\"\"Asynchronously checks a single domain and extracts title/matched title.\"\"\"\n",
    "    # Timeout is intentionally high (120s from the original context)\n",
    "    try:\n",
    "        async with httpx.AsyncClient(follow_redirects=True, headers=headers, timeout=timeout) as client:\n",
    "            response = await client.get(url)\n",
    "            if response.status_code == 200:\n",
    "                if str(response.url) != url:\n",
    "                    print(f\"Redirected to {response.url} for {url}\")\n",
    "                title = get_title_from_html(response.text)\n",
    "                matched_title = match_domain_in_title(domain, title)\n",
    "                return str(response.url), title, matched_title, True\n",
    "            else:\n",
    "                print(f\"Status Code {response.status_code} for {url}\")\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Request failed for {url}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {url}: {str(e)}\")\n",
    "    return url, None, None, False\n",
    "\n",
    "def test_domains(domains, timeout=120):\n",
    "    \"\"\"Orchestrates async checks for a list of domains.\"\"\"\n",
    "    print(f\"Giving up to {timeout} seconds to visit all sites...\")\n",
    "    tasks = [async_check_url(f'https://{domain}', domain, timeout) for domain in domains]\n",
    "    results = asyncio.run(async_test_domains(domains, tasks))\n",
    "    \n",
    "    domain_results = {}\n",
    "    for domain, result in zip(domains, results):\n",
    "        # Handle exceptions gracefully as in the original bf.test_domains (part of the transplant)\n",
    "        if isinstance(result, Exception):\n",
    "            domain_results[domain] = {'url': None, 'title': None, 'matched_title': None}\n",
    "        else:\n",
    "            domain_results[domain] = {'url': result[0], 'title': result[1], 'matched_title': result[2]}\n",
    "    return domain_results\n",
    "\n",
    "async def async_test_domains(domains, tasks):\n",
    "    \"\"\"Internal helper for asyncio.gather.\"\"\"\n",
    "    return await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "def split_domain_name(domain):\n",
    "    \"\"\"Splits a concatenated domain name into human-readable words (requires wordninja).\"\"\"\n",
    "    words = wordninja.split(domain)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# --- MAIN WORKFLOW LOGIC ---\n",
    "\n",
    "print(\"Visiting competitor homepages for title tags for filters...\\n\")\n",
    "\n",
    "# Original logic required to run async in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Lowercase existing matched titles for comparison\n",
    "df_competitors['Matched Title'] = df_competitors['Matched Title'].str.lower()\n",
    "\n",
    "# Find domains where 'Title' column is empty ('') or NaN\n",
    "# Using .isna() on a string column returns False for '', so we check explicitly for the empty string\n",
    "needs_titles = df_competitors[\n",
    "    (df_competitors['Title'].isna()) | (df_competitors['Title'] == '')\n",
    "].copy()\n",
    "\n",
    "if not needs_titles.empty:\n",
    "    # 1. Scrape Titles\n",
    "    print(f\"Gathering Titles for {len(needs_titles)} domains.\")\n",
    "    results = test_domains(needs_titles['Domain'].tolist())\n",
    "    \n",
    "    # 2. Prepare and Merge Data\n",
    "    data_to_add = {\n",
    "        'Domain': [],\n",
    "        'Title': [],\n",
    "        'Matched Title': []\n",
    "    }\n",
    "    \n",
    "    for domain, info in results.items():\n",
    "        data_to_add['Domain'].append(domain)\n",
    "        data_to_add['Title'].append(info['title'] if info['title'] else '')\n",
    "        data_to_add['Matched Title'].append(info['matched_title'] if info['matched_title'] else '')\n",
    "    \n",
    "    new_data_df = pd.DataFrame(data_to_add)\n",
    "    \n",
    "    # Use original combine_first logic for non-destructive update\n",
    "    df_competitors.set_index('Domain', inplace=True)\n",
    "    new_data_df.set_index('Domain', inplace=True)\n",
    "    df_competitors = new_data_df.combine_first(df_competitors)\n",
    "    df_competitors.reset_index(inplace=True)\n",
    "    \n",
    "    # Lowercase and persist the updated data\n",
    "    df_competitors['Matched Title'] = df_competitors['Matched Title'].str.lower()\n",
    "    df_competitors.to_csv(competitors_csv_file, index=False)\n",
    "    print(f\"‚úÖ Updated competitor titles and saved to '{competitors_csv_file}'.\")\n",
    "\n",
    "\n",
    "# --- Create Keyword Filters ---\n",
    "\n",
    "# Remove '.com' from both lists to create more generic keyword filters\n",
    "extracted_domains = [extract_registered_domain(domain).replace('.com', '') for domain in df_competitors['Domain']]\n",
    "matched_titles = [title.replace('.com', '') for title in df_competitors['Matched Title'].tolist() if title]\n",
    "\n",
    "# Split domain names using wordninja (e.g., 'barenecessities' -> 'bare necessities')\n",
    "split_domains = [split_domain_name(domain) for domain in extracted_domains]\n",
    "\n",
    "# Combine all lists, strip whitespace, and deduplicate\n",
    "combined_list = [x.strip() for x in extracted_domains + matched_titles + split_domains if x]\n",
    "combined_list = sorted(list(set(combined_list)))\n",
    "\n",
    "# Persist to external filter file (allows user editing)\n",
    "if not filter_file.exists():\n",
    "    df_filter = pd.DataFrame(combined_list, columns=['Filter'])\n",
    "    df_filter.to_csv(filter_file, index=False)\n",
    "    print(f\"‚úÖ Created initial keyword filter file at '{filter_file}' for user editing.\")\n",
    "else:\n",
    "    print(f\"‚òëÔ∏è Keyword filter file already exists at '{filter_file}'. Skipping creation.\")\n",
    "\n",
    "# Store the final competitors DF in the pipeline\n",
    "pip.set(job, 'competitors_df', df_competitors.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# NOTE: This cell assumes 'df2' (the result of the aggregation step) is available.\n",
    "\n",
    "print(\"Adjusting SEMRush columns that were not part of competitor-columns pivot...\")\n",
    "\n",
    "# Assign aggregating function to each metric\n",
    "# The chosen functions are critical for creating a single, best-case summary per keyword:\n",
    "# - 'min' for Position: Gives the *best* rank achieved across all competitors who rank.\n",
    "# - 'max' for Search Volume/Number of Results/Timestamp: Captures the highest value reported.\n",
    "# - 'sum' for Traffic/Traffic Cost: Aggregates the total value across all competitor results.\n",
    "# - 'mean' for Difficulty/CPC/Competition: Averages the difficulty/cost across all reporting sources.\n",
    "# - 'first' for categorical data (URLs, Intents, SERP Features): Chooses the first encountered value.\n",
    "agg_funcs = {\n",
    "    'Position': 'min',\n",
    "    'Search Volume': 'max',\n",
    "    'CPC': 'mean',\n",
    "    'Traffic': 'sum',\n",
    "    'Traffic (%)': 'mean',\n",
    "    'Traffic Cost': 'sum',\n",
    "    'Keyword Difficulty': 'mean',\n",
    "    'Previous position': 'first',\n",
    "    'Competition': 'mean',\n",
    "    'Number of Results': 'max',\n",
    "    'Timestamp': 'max',\n",
    "    'SERP Features by Keyword': 'first',\n",
    "    'Keyword Intents': 'first',\n",
    "    'Position Type': 'first',\n",
    "    'URL': 'first',\n",
    "    'Competitor URL': 'first',\n",
    "    'Client URL': 'first'\n",
    "}\n",
    "\n",
    "# Apply the aggregation across the combined dataset (df2)\n",
    "agg_df = df2.groupby('Keyword').agg(agg_funcs).reset_index()\n",
    "\n",
    "# Add a derived metric: Keyword word count\n",
    "agg_df['Number of Words'] = agg_df[\"Keyword\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Drop the 'Position' column: It was only used for the pivot/min operation,\n",
    "# but it's redundant/misleading now that the competitor position data is in pivot_df.\n",
    "agg_df.drop(columns=['Position'], inplace=True)\n",
    "\n",
    "print(\"Table of aggregates prepared.\")\n",
    "\n",
    "display(agg_df)\n",
    "\n",
    "# Store the aggregated metrics in the pipeline\n",
    "pip.set(job, 'keyword_aggregate_df_json', agg_df.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# NOTE: This cell assumes 'job', 'pivot_df', 'agg_df', and 'filter_file' are defined in prior cells.\n",
    "\n",
    "# --- PATH DEFINITION ---\n",
    "# The filter file path is already defined in a previous step, but included here for clarity\n",
    "filter_file = Path(\"data\") / f\"{job}_filter_keywords.csv\"\n",
    "\n",
    "# --- REQUIRED SUPPORT FUNCTION (Surgically Ported from botifython.py) ---\n",
    "\n",
    "def reorder_columns_surgical(df, priority_column, after_column):\n",
    "    \"\"\"\n",
    "    Surgical port of bf.reorder_columns: Moves a column immediately after a specified column.\n",
    "    \"\"\"\n",
    "    if priority_column in df.columns and after_column in df.columns:\n",
    "        columns = df.columns.drop(priority_column).tolist()\n",
    "        after_column_index = columns.index(after_column)\n",
    "        columns.insert(after_column_index + 1, priority_column)\n",
    "        return df[columns]\n",
    "    elif priority_column not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Warning: Priority column '{priority_column}' not found for reorder.\")\n",
    "    elif after_column not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Warning: After column '{after_column}' not found for reorder.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Merging Pivot Data with Aggregate Data...\")\n",
    "\n",
    "# 1. Merge Pivot Data (Keyword as index) with Aggregate Data (Keyword as index/column)\n",
    "pivotmerge_df = pd.merge(pivot_df.reset_index(), agg_df, on='Keyword', how='left')\n",
    "\n",
    "print(\"Pivot and Aggregate Data Joined.\\n\")\n",
    "rows, columns = pivotmerge_df.shape\n",
    "print(f\"Rows: {rows:,}\")\n",
    "print(f\"Cols: {columns:,}\")\n",
    "\n",
    "# --- FILTERING LOGIC ---\n",
    "\n",
    "print(\"\\nBrand and Negative Filters being applied...\")\n",
    "# 2. Optionally Filter Brand & Negative Keywords\n",
    "if filter_file.exists():\n",
    "    df_filter = pd.read_csv(filter_file, header=0)\n",
    "    \n",
    "    # Ensure all list items are strings before joining into a regex pattern\n",
    "    kw_filter = [str(f) for f in df_filter[\"Filter\"].dropna().tolist()]\n",
    "    \n",
    "    if kw_filter:\n",
    "        # Use re.escape to handle special characters in keywords and then join with '|' (OR)\n",
    "        pattern = '|'.join([re.escape(keyword) for keyword in kw_filter])\n",
    "        \n",
    "        # Apply the filter: keep rows where Keyword DOES NOT contain the pattern\n",
    "        filtered_df = pivotmerge_df[~pivotmerge_df[\"Keyword\"].str.contains(pattern, case=False, na=False)]\n",
    "        print(f\"‚úÖ Filter applied using {len(kw_filter)} terms from '{filter_file}'.\")\n",
    "    else:\n",
    "        filtered_df = pivotmerge_df\n",
    "        print(\"‚ö†Ô∏è Filter file exists but contains no terms. Skipping filter application.\")\n",
    "else:\n",
    "    filtered_df = pivotmerge_df\n",
    "    print(f\"‚òëÔ∏è No filter file found at '{filter_file}'. Skipping negative filtering.\")\n",
    "\n",
    "rows_filtered, columns_filtered = filtered_df.shape\n",
    "print(f\"Rows: {rows_filtered:,} ({rows:,} - {rows_filtered:,} = {rows - rows_filtered:,} rows removed)\")\n",
    "\n",
    "\n",
    "# --- REORDERING AND FINAL POLISH ---\n",
    "\n",
    "# 3. Apply Reordering Logic (Using the surgically defined function)\n",
    "# NOTE: The original logic chains reorders based on previously moved columns.\n",
    "temp_df = filtered_df.copy() # Use a temporary variable for clarity during chained operations\n",
    "\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Search Volume\", after_column=\"Keyword\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Number of Words\", after_column=\"CPC\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"CPC\", after_column=\"Number of Words\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Number of Results\", after_column=\"Position Type\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Timestamp\", after_column=\"Number of Results\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Competitor URL\", after_column=\"Client URL\")\n",
    "\n",
    "# 4. Final Arrange (Verbatim column ordering and sorting)\n",
    "# The manual reorder logic below overrides the custom function, but we include it verbatim:\n",
    "rest_of_columns = [col for col in temp_df.columns if col not in ['Keyword', 'Search Volume']]\n",
    "new_column_order = ['Keyword', 'Search Volume'] + rest_of_columns\n",
    "\n",
    "# The conditional block from the original (verbatim)\n",
    "if 'Keyword' in temp_df.columns:\n",
    "    temp_df = temp_df[['Keyword'] + ['Search Volume'] + [col for col in temp_df.columns if col not in ['Keyword', 'Search Volume']]]\n",
    "\n",
    "# Apply the intended final order\n",
    "filtered_df = temp_df[new_column_order]\n",
    "\n",
    "# Final sorting and column drops\n",
    "arranged_df = filtered_df.sort_values(by='Search Volume', ascending=False)\n",
    "arranged_df.drop(columns=[\"Previous position\", \"Traffic\", \"Traffic (%)\", \"Traffic Cost\"], inplace=True)\n",
    "\n",
    "print(\"\\nFinal Keyword Table Prepared.\")\n",
    "\n",
    "# Store the final result in the pipeline\n",
    "pip.set(job, 'filtered_gap_analysis_df_json', arranged_df.to_json(orient='records'))\n",
    "\n",
    "display(arranged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
