{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GAPalyzer üìê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from imports import gap_analyzer_sauce as secretsauce\n",
    "from pipulate import pip\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import _config as keys\n",
    "\n",
    "job = \"gapalyzer-11\" # Give your session a unique name\n",
    "print(f\"Cient: {keys.client_domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Set all your Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "botify_token = keys.botify\n",
    "ROW_LIMIT = 30000\n",
    "COMPETITOR_LIMIT = 100\n",
    "BROWSER_DOWNLOAD_PATH = None\n",
    "GLOBAL_WIDTH_ADJUSTMENT = 1.5\n",
    "ENABLE_CLUSTERING = False\n",
    "print(f'‚úÖ Configuration set: Final report will be limited to {ROW_LIMIT} rows.')\n",
    "if COMPETITOR_LIMIT:\n",
    "    print(f'‚úÖ Configuration set: Processing will be limited to the top {COMPETITOR_LIMIT} competitors.')\n",
    "else:\n",
    "    print(f'‚úÖ Configuration set: Processing all competitors.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "custom-filters-input"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Define Custom Excel Tab Filters --- \n",
    "# (This list is scrubbed by pip.nbup() and returned to this default)\n",
    "\n",
    "targeted_filters = [\n",
    "    (\"Gifts\", ['gift', 'gifts', 'idea', 'ideas', 'present', 'presents', 'give', 'giving', 'black friday', 'cyber monday', 'cyber week', 'bfcm', 'bf', 'cm', 'holiday', 'deals', 'sales', 'offer', 'discount', 'shopping']),\n",
    "    (\"Broad Questions\", '''am are can could did do does for from had has have how i is may might must shall should was were what when where which who whom whose why will with would'''.split()),\n",
    "    (\"Narrow Questions\", '''who whom whose what which where when why how'''.split()),\n",
    "    (\"Popular Modifiers\", ['how to', 'best', 'review', 'reviews']),\n",
    "    (\"Near Me\", ['near me', 'for sale', 'nearby', 'closest', 'near you', 'local'])\n",
    "]\n",
    "\n",
    "pip.set(job, 'targeted_filters', targeted_filters)\n",
    "print(f\"‚úÖ Stored {len(targeted_filters)} custom filter sets in pip state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. List all your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "# Enter one URL per line\n",
    "https://nixos.org/     # Linux\n",
    "https://jupyter.org/   # Python\n",
    "https://neovim.io/     # vim\n",
    "https://git-scm.com/   # git\n",
    "https://www.fastht.ml/ # FastHTML\n",
    "https://pipulate.com/  # AIE (Pronounced \"Ayyy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3. Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function from the sauce module\n",
    "# This performs the extraction, stores domains via pip.set, prints URLs,\n",
    "# and returns the domains list if needed elsewhere (though we primarily rely on pip state now).\n",
    "\n",
    "# Warning: If 2 competitors are subfolders of the same site, the filenames need to be made distinct.\n",
    "\n",
    "competitor_domains = secretsauce.extract_domains_and_print_urls(job)\n",
    "\n",
    "# Optional: You could add a pip.get here for verification if desired\n",
    "# stored_domains = pip.get(job, 'competitor_domains', [])\n",
    "# print(f\"\\nVerification: Retrieved {len(stored_domains)} domains from pip state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 4. Process the Rows\n",
    "\n",
    "## Verify Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function from the sauce module.\n",
    "# It handles moving files and storing relevant paths in pip state.\n",
    "# BROWSER_DOWNLOAD_PATH should be defined in a config cell near the top.\n",
    "semrush_dir, collected_files = secretsauce.collect_semrush_downloads(job, BROWSER_DOWNLOAD_PATH)\n",
    "\n",
    "# Optional verification (can be commented out for cleaner output)\n",
    "# if semrush_dir and collected_files:\n",
    "#    print(f\"\\nVerification: Files collected in '{pip.get(job, 'semrush_download_dir')}'\")\n",
    "#    print(f\"Files found/moved ({len(pip.get(job, 'collected_semrush_files'))}):\")\n",
    "#    # for f in pip.get(job, 'collected_semrush_files'): print(f\" - {Path(f).name}\") # Use Path for display if needed\n",
    "# elif semrush_dir:\n",
    "#    print(f\"\\nVerification: Destination directory '{pip.get(job, 'semrush_download_dir')}' confirmed, but no new files moved.\")\n",
    "# else:\n",
    "#    print(\"\\nVerification: File collection step encountered an error.\")\n",
    "\n",
    "# Call the function: It finds files, stores paths via pip.set, and returns Markdown summary\n",
    "# COMPETITOR_LIMIT should be defined in a config cell near the top\n",
    "markdown_summary = secretsauce.find_semrush_files_and_generate_summary(job, COMPETITOR_LIMIT)\n",
    "\n",
    "# Display the returned Markdown summary\n",
    "display(Markdown(markdown_summary))\n",
    "\n",
    "# Optional Verification (can be commented out)\n",
    "# stored_files = pip.get(job, 'collected_semrush_files', [])\n",
    "# print(f\"\\nVerification: Retrieved {len(stored_files)} file paths from pip state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Combine Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This one function now:\n",
    "# 1. Reads the file list from pip state.\n",
    "# 2. Loads and combines all SEMRush files into a master DataFrame.\n",
    "# 3. Applies the COMPETITOR_LIMIT.\n",
    "# 4. Stores the master DataFrame and competitor dictionary in pip state.\n",
    "# 5. Returns the master DataFrame (for the next step) and domain counts (for display).\n",
    "df2, domain_value_counts = secretsauce.load_and_combine_semrush_data(job, keys.client_domain, COMPETITOR_LIMIT)\n",
    "\n",
    "# Display the domain value counts for verification\n",
    "display(domain_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Make Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Pivoting df2 by Keyword/Domain.\n",
    "# 2. Calculating Competitors Positioning.\n",
    "# 3. Loading or creating the competitors_df and saving it to CSV.\n",
    "# 4. Printing summary statistics.\n",
    "# 5. Storing pivot_df and competitors_df in pip state.\n",
    "# It receives df2 directly from the previous cell's variable.\n",
    "pivot_df = secretsauce.pivot_semrush_data(job, df2, keys.client_domain)\n",
    "\n",
    "# Display the resulting pivot table\n",
    "display(pivot_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Pivot DF stored: {'keyword_pivot_df_json' in pip.read_state(job)}\")\n",
    "# print(f\"  Competitors DF stored: {'competitors_df_json' in pip.read_state(job)}\")\n",
    "# loaded_competitors = pd.read_json(pip.get(job, 'competitors_df_json', '[]'))\n",
    "# print(f\"  Competitors DF rows in state: {len(loaded_competitors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Filter Brand Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Loading competitors_df from pip state.\n",
    "# 2. Checking for and fetching missing homepage titles asynchronously.\n",
    "# 3. Updating competitors_df with new titles.\n",
    "# 4. Saving updated competitors_df to CSV and pip state.\n",
    "# 5. Generating the keyword filter list from domains and titles.\n",
    "# 6. Creating or updating the filter_keywords.csv file.\n",
    "# 7. Storing the filter keyword list in pip state.\n",
    "# It returns a status message.\n",
    "status_message = secretsauce.fetch_titles_and_create_filters(job)\n",
    "\n",
    "# Print the status message returned by the function\n",
    "print(status_message)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# updated_competitors_df = pd.read_json(StringIO(pip.get(job, 'competitors_df_json', '[]')))\n",
    "# print(f\"  Competitors DF rows in state: {len(updated_competitors_df)}\")\n",
    "# print(f\"  Example Title: {updated_competitors_df['Title'].iloc[0] if not updated_competitors_df.empty else 'N/A'}\")\n",
    "# filter_list = json.loads(pip.get(job, 'filter_keyword_list_json', '[]'))\n",
    "# print(f\"  Filter keywords stored: {len(filter_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Make Aggregate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Defining aggregation rules for each metric.\n",
    "# 2. Grouping df2 by Keyword and applying aggregations.\n",
    "# 3. Calculating 'Number of Words'.\n",
    "# 4. Dropping the aggregated 'Position' column.\n",
    "# 5. Storing the resulting agg_df in pip state.\n",
    "# 6. Returning agg_df for display and use in the next step.\n",
    "# It receives df2 directly from the previous cell's variable.\n",
    "agg_df = secretsauce.aggregate_semrush_metrics(job, df2)\n",
    "\n",
    "# Display the aggregated data\n",
    "display(agg_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Agg DF stored: {'keyword_aggregate_df_json' in pip.read_state(job)}\")\n",
    "# loaded_agg_df = pd.read_json(StringIO(pip.get(job, 'keyword_aggregate_df_json', '[]'))) # Use StringIO for verification\n",
    "# print(f\"  Agg DF rows in state: {len(loaded_agg_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Join Pivot & Aggregate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Merging pivot_df and agg_df.\n",
    "# 2. Reading the filter keyword list from the CSV file.\n",
    "# 3. Applying the brand/negative keyword filter.\n",
    "# 4. Reordering columns for readability.\n",
    "# 5. Dropping unnecessary columns (Traffic metrics, Previous position).\n",
    "# 6. Sorting the final DataFrame by Search Volume.\n",
    "# 7. Storing the final arranged_df in pip state.\n",
    "# It receives pivot_df and agg_df directly from previous cell variables.\n",
    "arranged_df = secretsauce.merge_filter_arrange_data(job, pivot_df, agg_df)\n",
    "\n",
    "# Display the final, arranged DataFrame\n",
    "display(arranged_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Arranged DF stored: {'filtered_gap_analysis_df_json' in pip.read_state(job)}\")\n",
    "# loaded_arranged_df = pd.read_json(StringIO(pip.get(job, 'filtered_gap_analysis_df_json', '[]'))) # Use StringIO\n",
    "# print(f\"  Final Arranged DF rows in state: {len(loaded_arranged_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Truncate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Truncate Data (Moved Upstream for Performance)\n",
    "#\n",
    "# We apply the ROW_LIMIT *before* merging with Botify data.\n",
    "# This speeds up the merge and all subsequent steps (clustering, Excel)\n",
    "# by operating on a much smaller, pre-filtered set of keywords.\n",
    "\n",
    "# %%\n",
    "# This function now handles:\n",
    "# 1. Iterating through volume cutoffs to find the best fit under ROW_LIMIT.\n",
    "# ... (comments) ...\n",
    "# 5. Returning the truncated DataFrame (aliased as 'df') for the next step.\n",
    "\n",
    "# It receives 'arranged_df' (the final_df from the previous step) and 'ROW_LIMIT' from config.\n",
    "# We will re-alias the output to 'arranged_df' so the next cell works.\n",
    "arranged_df = secretsauce.truncate_dataframe_by_volume(job, arranged_df, ROW_LIMIT)\n",
    "\n",
    "# Display the head of the final truncated DataFrame\n",
    "display(arranged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Download Botify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- START URGENT FIX: Bypassing stale kernel cache ---\n",
    "# We are redefining the function *locally* in this cell\n",
    "# to force the kernel to use the corrected version.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pipulate import pip # Make sure pip is imported\n",
    "import _config as keys # Make sure keys is imported\n",
    "\n",
    "# (Private helper functions _fetch_analysis_slugs, _export_data, etc., are assumed to be OK)\n",
    "# (If they are not, they would need to be pasted here too, but the error is in the main function)\n",
    "\n",
    "def fetch_botify_data_and_save(job: str, botify_token: str, botify_project_url: str):\n",
    "    \"\"\"\n",
    "    Orchestrates fetching data from the Botify API using pre-defined helpers,\n",
    "    handling slug detection, API calls with fallbacks, downloading, decompression,\n",
    "    and storing the final DataFrame in pip state.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Fetching data from Botify API...\")\n",
    "    report_name = None # Initialize report_name\n",
    "    csv_dir = None # Initialize csv_dir\n",
    "    botify_export_df = pd.DataFrame() # Initialize as empty DataFrame\n",
    "\n",
    "    # --- 1. Parse URL and get latest analysis slug ---\n",
    "    try:\n",
    "        cleaned_url = botify_project_url.rstrip('/')\n",
    "        url_parts = cleaned_url.split('/')\n",
    "        if len(url_parts) < 2:\n",
    "             raise ValueError(f\"Could not parse org/project from URL: {botify_project_url}\")\n",
    "\n",
    "        org = url_parts[-2]\n",
    "        project = url_parts[-1]\n",
    "        print(f\"  Parsed Org: {org}, Project: {project}\")\n",
    "\n",
    "        slugs = secretsauce._fetch_analysis_slugs(org, project, botify_token) # Call helper from module\n",
    "        if not slugs:\n",
    "            raise ValueError(\"Could not find any Botify analysis slugs for the provided project.\")\n",
    "        analysis = slugs[0] # Use the most recent analysis\n",
    "        print(f\"  ‚úÖ Found latest Analysis Slug: {analysis}\")\n",
    "\n",
    "    except (IndexError, ValueError, Exception) as e: \n",
    "        print(f\"  ‚ùå Critical Error during Botify setup: {e}\")\n",
    "        pip.set(job, 'botify_export_df_json', pd.DataFrame().to_json(orient='records')) # Use old key as fallback on error\n",
    "        return pd.DataFrame(), False, None, None \n",
    "\n",
    "    # --- 2. Define Paths and Payloads ---\n",
    "    try:\n",
    "        csv_dir = Path(\"data\") / f\"{job}_botify\"\n",
    "        csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "        report_name = csv_dir / \"botify_export.csv\"\n",
    "\n",
    "        payload_full = {\n",
    "            \"fields\": [\"url\", \"depth\", \"gsc_by_url.count_missed_clicks\", \"gsc_by_url.avg_ctr\", \"gsc_by_url.avg_position\", \"inlinks_internal.nb.unique\", \"internal_page_rank.value\", \"internal_page_rank.position\", \"internal_page_rank.raw\", \"gsc_by_url.count_impressions\", \"gsc_by_url.count_clicks\", \"gsc_by_url.count_keywords\", \"gsc_by_url.count_keywords_on_url_to_achieve_90pc_clicks\", \"metadata.title.content\", \"metadata.description.content\"],\n",
    "            \"sort\": []\n",
    "        }\n",
    "        payload_fallback = {\n",
    "            \"fields\": [\"url\", \"depth\", \"inlinks_internal.nb.unique\", \"internal_page_rank.value\", \"internal_page_rank.position\", \"internal_page_rank.raw\", \"metadata.title.content\", \"metadata.description.content\"],\n",
    "            \"sort\": []\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error defining paths/payloads: {e}\")\n",
    "        pip.set(job, 'botify_export_df_json', pd.DataFrame().to_json(orient='records')) # Use old key as fallback on error\n",
    "        return pd.DataFrame(), False, None, csv_dir \n",
    "\n",
    "    # --- 3. Main Logic: Check existing, call API with fallback ---\n",
    "    loaded_from_existing = False\n",
    "    if report_name.exists():\n",
    "        print(f\"  ‚òëÔ∏è Botify export file already exists at '{report_name}'. Reading from disk.\")\n",
    "        try:\n",
    "            botify_export_df = pd.read_csv(report_name, skiprows=1)\n",
    "            loaded_from_existing = True \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Could not read existing CSV file '{report_name}', will attempt to re-download. Error: {e}\")\n",
    "            botify_export_df = pd.DataFrame() \n",
    "\n",
    "    if not loaded_from_existing:\n",
    "        print(\"  Attempting download with Full GSC Payload...\")\n",
    "        status_code, _ = secretsauce._export_data('v1', org, project, payload_full, report_name, analysis=analysis)\n",
    "\n",
    "        if status_code not in [200, 201]: \n",
    "            print(\"    -> Full Payload failed. Attempting Fallback Payload (no GSC data)...\")\n",
    "            status_code, _ = secretsauce._export_data('v1', org, project, payload_fallback, report_name, analysis=analysis)\n",
    "\n",
    "        if report_name.exists():\n",
    "             try:\n",
    "                  botify_export_df = pd.read_csv(report_name, skiprows=1)\n",
    "                  print(\"  ‚úÖ Successfully downloaded and/or loaded Botify data.\")\n",
    "             except Exception as e:\n",
    "                  print(f\"  ‚ùå Download/decompression seemed successful, but failed to read the final CSV file '{report_name}'. Error: {e}\")\n",
    "                  botify_export_df = pd.DataFrame() \n",
    "        else:\n",
    "             print(\"  ‚ùå Botify export failed critically after both attempts, and no file exists.\")\n",
    "             botify_export_df = pd.DataFrame()\n",
    "\n",
    "    # --- 4. Store State and Return (THE FIX IS HERE) ---\n",
    "    has_botify = not botify_export_df.empty\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # We are storing the *path* to the CSV, not the *entire DataFrame*\n",
    "    # This avoids the TooBigError: string or blob too big\n",
    "    if has_botify:\n",
    "        pip.set(job, 'botify_export_csv_path', str(report_name.resolve()))\n",
    "        print(f\"üíæ Stored Botify CSV path in pip state for job '{job}': {report_name.resolve()}\")\n",
    "    else:\n",
    "        pip.set(job, 'botify_export_csv_path', None)\n",
    "        print(\"ü§∑ No Botify data loaded. Stored 'None' for path in pip state.\")\n",
    "    # --- END FIX ---\n",
    "\n",
    "    # Return necessary info for display logic in notebook\n",
    "    return botify_export_df, has_botify, report_name, csv_dir\n",
    "\n",
    "# --- END URGENT FIX ---\n",
    "\n",
    "\n",
    "# ... now your original cell content ...\n",
    "botify_export_df, has_botify, report_path, csv_dir_path = fetch_botify_data_and_save(\n",
    "    job,\n",
    "    keys.botify,\n",
    "    keys.botify_project_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Join Botify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Merging arranged_df with botify_export_df (if has_botify is True).\n",
    "# 2. Renaming Botify's 'url' to 'Full URL' for the merge.\n",
    "# 3. Inserting the new Botify columns neatly after the 'Competition' column.\n",
    "# 4. Cleaning up redundant URL columns used for the merge.\n",
    "# 5. Saving the intermediate 'unformatted.csv' file.\n",
    "# 6. Storing the final DataFrame in pip state ('final_working_df_json').\n",
    "# 7. Returning the final DataFrame (aliased as 'df') and a dict of data for display.\n",
    "\n",
    "# It receives arranged_df, botify_export_df, and has_botify from previous cells.\n",
    "df, display_data = secretsauce.merge_and_finalize_data(\n",
    "    job,\n",
    "    arranged_df,\n",
    "    botify_export_df,\n",
    "    has_botify\n",
    ")\n",
    "\n",
    "# --- Display Logic (Remains in Notebook, driven by return values) ---\n",
    "print(f\"Rows: {display_data['rows']:,}\")\n",
    "print(f\"Cols: {display_data['cols']:,}\")\n",
    "\n",
    "if display_data['has_botify'] and display_data['pagerank_counts'] is not None:\n",
    "    display(display_data['pagerank_counts'])\n",
    "elif display_data['has_botify']:\n",
    "    # This state means has_botify was true but 'Internal Pagerank' col was missing\n",
    "    print(\"‚ö†Ô∏è Botify data was merged, but 'Internal Pagerank' column not found for display.\")\n",
    "else:\n",
    "    # This state means has_botify was false\n",
    "    print(\"‚ÑπÔ∏è No Botify data was merged.\")\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Working DF stored: {'final_working_df_json' in pip.read_state(job)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Cluster Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This one function now handles the entire clustering and finalization process:\n",
    "# 1. Loads/tests clustering parameters from a JSON cache file.\n",
    "# 2. Runs iterative ML clustering (TF-IDF, SVD, k-means) to find the best fit.\n",
    "# 3. Names the resulting clusters using n-grams.\n",
    "# 4. Performs the final column reordering.\n",
    "# 5. Saves the final 'unformatted_csv'.\n",
    "# 6. Prints the final cluster counts.\n",
    "# 7. Stores the final DataFrame in pip state ('final_clustered_df_json').\n",
    "# 8. Returns the final DataFrame for display.\n",
    "\n",
    "# It receives 'df' (the truncated DF) and 'has_botify' from previous cells.\n",
    "df = secretsauce.cluster_and_finalize_dataframe(job, df, has_botify, enable_clustering=ENABLE_CLUSTERING)\n",
    "\n",
    "# Display the head of the final, clustered, and arranged DataFrame\n",
    "display(df.head())\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Clustered DF stored: {'final_clustered_df_json' in pip.read_state(job)}\")\n",
    "# loaded_clustered_df = pd.read_json(StringIO(pip.get(job, 'final_clustered_df_json', '[]')))\n",
    "# print(f\"  Clustered DF rows in state: {len(loaded_clustered_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Write Excel Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CONFIGURATION AND SETUP ---\n",
    "# 1. Define Output Path\n",
    "deliverables_dir = Path(\"deliverables\") / job\n",
    "deliverables_dir.mkdir(parents=True, exist_ok=True)\n",
    "semrush_lookup = secretsauce._extract_registered_domain(keys.client_domain)\n",
    "xl_filename = f\"{semrush_lookup.replace('.', '_').rstrip('_')}_GAPalyzer_{job}_V1.xlsx\"\n",
    "xl_file = deliverables_dir / xl_filename\n",
    "\n",
    "# 2. Get Canonical Competitors and Target Column\n",
    "# (This logic was previously inside create_deliverables_excel_and_button)\n",
    "clean_lookup_key = semrush_lookup.rstrip('/')\n",
    "TARGET_COMPETITOR_COL = None\n",
    "for col in df.columns:\n",
    "    if col.rstrip('/') == clean_lookup_key:\n",
    "        TARGET_COMPETITOR_COL = col\n",
    "        break\n",
    "if TARGET_COMPETITOR_COL is None:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not find canonical column for '{semrush_lookup}'. Using default.\")\n",
    "    TARGET_COMPETITOR_COL = semrush_lookup\n",
    "\n",
    "competitors_list_json = pip.get(job, 'competitors_list_json', '[]')\n",
    "import json\n",
    "competitors = json.loads(competitors_list_json)\n",
    "if not competitors:\n",
    "    competitors = [col for col in df.columns if col == semrush_lookup or '/' in col or '.com' in col]\n",
    "\n",
    "# 3. Create Button Widget\n",
    "import ipywidgets as widgets\n",
    "button = widgets.Button(\n",
    "    description=f\"üìÇ Open Deliverables Folder ({job})\",\n",
    "    tooltip=f\"Open {deliverables_dir.resolve()}\",\n",
    "    button_style='success'\n",
    ")\n",
    "def on_open_folder_click(b):\n",
    "    secretsauce._open_folder(str(deliverables_dir))\n",
    "button.on_click(on_open_folder_click)\n",
    "\n",
    "# --- EXECUTE BATCH WRITE ---\n",
    "button = secretsauce.add_filtered_excel_tabs(\n",
    "    job,\n",
    "    df,\n",
    "    semrush_lookup,\n",
    "    has_botify,\n",
    "    competitors,\n",
    "    xl_file,\n",
    "    TARGET_COMPETITOR_COL,\n",
    "    button,\n",
    "    custom_filters=targeted_filters,\n",
    "    width_adjustment=GLOBAL_WIDTH_ADJUSTMENT\n",
    ")\n",
    "\n",
    "# Display Result\n",
    "display(button)\n",
    "print(f\"üíæ Final File: {xl_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
