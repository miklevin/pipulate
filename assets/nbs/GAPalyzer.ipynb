{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import gap_analyzer_sauce\n",
    "from pipulate import pip\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import keys\n",
    "\n",
    "job = \"gapalyzer-03\" # Give your session a unique name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Set all your Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "botify_token = keys.botify\n",
    "ROW_LIMIT = 3000\n",
    "COMPETITOR_LIMIT = 3\n",
    "BROWSER_DOWNLOAD_PATH = None\n",
    "GLOBAL_WIDTH_ADJUSTMENT = 1.5\n",
    "print(f'✅ Configuration set: Final report will be limited to {ROW_LIMIT} rows.')\n",
    "if COMPETITOR_LIMIT:\n",
    "    print(f'✅ Configuration set: Processing will be limited to the top {COMPETITOR_LIMIT} competitors.')\n",
    "else:\n",
    "    print(f'✅ Configuration set: Processing all competitors.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. List all your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "https://nixos.org/    # Linux\n",
    "https://pypi.org/     # Python\n",
    "https://neovim.io/    # vim\n",
    "https://git-scm.com/  # git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3. Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gap_analyzer_sauce # Import the new module\n",
    "\n",
    "# Call the function from the sauce module\n",
    "# This performs the extraction, stores domains via pip.set, prints URLs,\n",
    "# and returns the domains list if needed elsewhere (though we primarily rely on pip state now).\n",
    "competitor_domains = gap_analyzer_sauce.extract_domains_and_print_urls(job)\n",
    "\n",
    "# Optional: You could add a pip.get here for verification if desired\n",
    "# stored_domains = pip.get(job, 'competitor_domains', [])\n",
    "# print(f\"\\nVerification: Retrieved {len(stored_domains)} domains from pip state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### 4. Process the Rows\n",
    "\n",
    "## Verify Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gap_analyzer_sauce # Ensure module is imported\n",
    "\n",
    "# Call the function from the sauce module.\n",
    "# It handles moving files and storing relevant paths in pip state.\n",
    "# BROWSER_DOWNLOAD_PATH should be defined in a config cell near the top.\n",
    "semrush_dir, collected_files = gap_analyzer_sauce.collect_semrush_downloads(job, BROWSER_DOWNLOAD_PATH)\n",
    "\n",
    "# Optional verification (can be commented out for cleaner output)\n",
    "# if semrush_dir and collected_files:\n",
    "#    print(f\"\\nVerification: Files collected in '{pip.get(job, 'semrush_download_dir')}'\")\n",
    "#    print(f\"Files found/moved ({len(pip.get(job, 'collected_semrush_files'))}):\")\n",
    "#    # for f in pip.get(job, 'collected_semrush_files'): print(f\" - {Path(f).name}\") # Use Path for display if needed\n",
    "# elif semrush_dir:\n",
    "#    print(f\"\\nVerification: Destination directory '{pip.get(job, 'semrush_download_dir')}' confirmed, but no new files moved.\")\n",
    "# else:\n",
    "#    print(\"\\nVerification: File collection step encountered an error.\")\n",
    "\n",
    "# Call the function: It finds files, stores paths via pip.set, and returns Markdown summary\n",
    "# COMPETITOR_LIMIT should be defined in a config cell near the top\n",
    "markdown_summary = gap_analyzer_sauce.find_semrush_files_and_generate_summary(job, COMPETITOR_LIMIT)\n",
    "\n",
    "# Display the returned Markdown summary\n",
    "display(Markdown(markdown_summary))\n",
    "\n",
    "# Optional Verification (can be commented out)\n",
    "# stored_files = pip.get(job, 'collected_semrush_files', [])\n",
    "# print(f\"\\nVerification: Retrieved {len(stored_files)} file paths from pip state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Combine Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This one function now:\n",
    "# 1. Reads the file list from pip state.\n",
    "# 2. Loads and combines all SEMRush files into a master DataFrame.\n",
    "# 3. Applies the COMPETITOR_LIMIT.\n",
    "# 4. Stores the master DataFrame and competitor dictionary in pip state.\n",
    "# 5. Returns the master DataFrame (for the next step) and domain counts (for display).\n",
    "df2, domain_value_counts = gap_analyzer_sauce.load_and_combine_semrush_data(job, keys.client_domain, COMPETITOR_LIMIT)\n",
    "\n",
    "# Display the domain value counts for verification\n",
    "display(domain_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Make Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Pivoting df2 by Keyword/Domain.\n",
    "# 2. Calculating Competitors Positioning.\n",
    "# 3. Loading or creating the competitors_df and saving it to CSV.\n",
    "# 4. Printing summary statistics.\n",
    "# 5. Storing pivot_df and competitors_df in pip state.\n",
    "# It receives df2 directly from the previous cell's variable.\n",
    "pivot_df = gap_analyzer_sauce.pivot_semrush_data(job, df2, keys.client_domain)\n",
    "\n",
    "# Display the resulting pivot table\n",
    "display(pivot_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Pivot DF stored: {'keyword_pivot_df_json' in pip.read_state(job)}\")\n",
    "# print(f\"  Competitors DF stored: {'competitors_df_json' in pip.read_state(job)}\")\n",
    "# loaded_competitors = pd.read_json(pip.get(job, 'competitors_df_json', '[]'))\n",
    "# print(f\"  Competitors DF rows in state: {len(loaded_competitors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Filter Brand Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Loading competitors_df from pip state.\n",
    "# 2. Checking for and fetching missing homepage titles asynchronously.\n",
    "# 3. Updating competitors_df with new titles.\n",
    "# 4. Saving updated competitors_df to CSV and pip state.\n",
    "# 5. Generating the keyword filter list from domains and titles.\n",
    "# 6. Creating or updating the filter_keywords.csv file.\n",
    "# 7. Storing the filter keyword list in pip state.\n",
    "# It returns a status message.\n",
    "status_message = gap_analyzer_sauce.fetch_titles_and_create_filters(job)\n",
    "\n",
    "# Print the status message returned by the function\n",
    "print(status_message)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# updated_competitors_df = pd.read_json(StringIO(pip.get(job, 'competitors_df_json', '[]')))\n",
    "# print(f\"  Competitors DF rows in state: {len(updated_competitors_df)}\")\n",
    "# print(f\"  Example Title: {updated_competitors_df['Title'].iloc[0] if not updated_competitors_df.empty else 'N/A'}\")\n",
    "# filter_list = json.loads(pip.get(job, 'filter_keyword_list_json', '[]'))\n",
    "# print(f\"  Filter keywords stored: {len(filter_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Make Aggregate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Defining aggregation rules for each metric.\n",
    "# 2. Grouping df2 by Keyword and applying aggregations.\n",
    "# 3. Calculating 'Number of Words'.\n",
    "# 4. Dropping the aggregated 'Position' column.\n",
    "# 5. Storing the resulting agg_df in pip state.\n",
    "# 6. Returning agg_df for display and use in the next step.\n",
    "# It receives df2 directly from the previous cell's variable.\n",
    "agg_df = gap_analyzer_sauce.aggregate_semrush_metrics(job, df2)\n",
    "\n",
    "# Display the aggregated data\n",
    "display(agg_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Agg DF stored: {'keyword_aggregate_df_json' in pip.read_state(job)}\")\n",
    "# loaded_agg_df = pd.read_json(StringIO(pip.get(job, 'keyword_aggregate_df_json', '[]'))) # Use StringIO for verification\n",
    "# print(f\"  Agg DF rows in state: {len(loaded_agg_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Join Pivot & Aggregate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Merging pivot_df and agg_df.\n",
    "# 2. Reading the filter keyword list from the CSV file.\n",
    "# 3. Applying the brand/negative keyword filter.\n",
    "# 4. Reordering columns for readability.\n",
    "# 5. Dropping unnecessary columns (Traffic metrics, Previous position).\n",
    "# 6. Sorting the final DataFrame by Search Volume.\n",
    "# 7. Storing the final arranged_df in pip state.\n",
    "# It receives pivot_df and agg_df directly from previous cell variables.\n",
    "arranged_df = gap_analyzer_sauce.merge_filter_arrange_data(job, pivot_df, agg_df)\n",
    "\n",
    "# Display the final, arranged DataFrame\n",
    "display(arranged_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Arranged DF stored: {'filtered_gap_analysis_df_json' in pip.read_state(job)}\")\n",
    "# loaded_arranged_df = pd.read_json(StringIO(pip.get(job, 'filtered_gap_analysis_df_json', '[]'))) # Use StringIO\n",
    "# print(f\"  Final Arranged DF rows in state: {len(loaded_arranged_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Download Botify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This one function now handles the entire Botify data fetching process...\n",
    "# (comments remain the same)\n",
    "botify_export_df, has_botify, report_path, csv_dir_path = gap_analyzer_sauce.fetch_botify_data_and_save(\n",
    "    job,\n",
    "    keys.botify,\n",
    "    keys.botify_project_url\n",
    ")\n",
    "\n",
    "# --- Display Logic (Remains in Notebook) ---\n",
    "# (Display logic remains the same)\n",
    "if has_botify:\n",
    "    print(\"\\n--- Botify Data Summary ---\")\n",
    "    if \"Internal Pagerank\" in botify_export_df.columns:\n",
    "        display(botify_export_df[\"Internal Pagerank\"].value_counts())\n",
    "    else:\n",
    "        print(\"  ⚠️ 'Internal Pagerank' column not found for display.\")\n",
    "    print(\"-------------------------\\n\")\n",
    "    if report_path:\n",
    "        print(f\"📁 Botify data saved to: {report_path.resolve()}\")\n",
    "    if csv_dir_path:\n",
    "        print(f\"📂 Containing folder: {csv_dir_path.resolve()}\")\n",
    "else:\n",
    "    print(\"\\nNo Botify data loaded or available to display summary.\")\n",
    "\n",
    "# Optional verification (using pip.get and StringIO)\n",
    "# from io import StringIO\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Botify DF stored: {'botify_export_df_json' in pip.read_state(job)}\")\n",
    "# loaded_botify_df = pd.read_json(StringIO(pip.get(job, 'botify_export_df_json', '[]')))\n",
    "# print(f\"  Botify DF rows in state: {len(loaded_botify_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Join Botify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Merging arranged_df with botify_export_df (if has_botify is True).\n",
    "# 2. Renaming Botify's 'url' to 'Full URL' for the merge.\n",
    "# 3. Inserting the new Botify columns neatly after the 'Competition' column.\n",
    "# 4. Cleaning up redundant URL columns used for the merge.\n",
    "# 5. Saving the intermediate 'unformatted.csv' file.\n",
    "# 6. Storing the final DataFrame in pip state ('final_working_df_json').\n",
    "# 7. Returning the final DataFrame (aliased as 'df') and a dict of data for display.\n",
    "\n",
    "# It receives arranged_df, botify_export_df, and has_botify from previous cells.\n",
    "df, display_data = gap_analyzer_sauce.merge_and_finalize_data(\n",
    "    job,\n",
    "    arranged_df,\n",
    "    botify_export_df,\n",
    "    has_botify\n",
    ")\n",
    "\n",
    "# --- Display Logic (Remains in Notebook, driven by return values) ---\n",
    "print(f\"Rows: {display_data['rows']:,}\")\n",
    "print(f\"Cols: {display_data['cols']:,}\")\n",
    "\n",
    "if display_data['has_botify'] and display_data['pagerank_counts'] is not None:\n",
    "    display(display_data['pagerank_counts'])\n",
    "elif display_data['has_botify']:\n",
    "    # This state means has_botify was true but 'Internal Pagerank' col was missing\n",
    "    print(\"⚠️ Botify data was merged, but 'Internal Pagerank' column not found for display.\")\n",
    "else:\n",
    "    # This state means has_botify was false\n",
    "    print(\"ℹ️ No Botify data was merged.\")\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Working DF stored: {'final_working_df_json' in pip.read_state(job)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Truncate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Iterating through volume cutoffs to find the best fit under ROW_LIMIT.\n",
    "# 2. Handling edge cases (e.g., if filtering removes all rows).\n",
    "# 3. Printing the truncation log.\n",
    "# 4. Storing the truncated DataFrame in pip state ('truncated_df_for_clustering_json').\n",
    "# 5. Returning the truncated DataFrame (aliased as 'df') for the next step.\n",
    "\n",
    "# It receives 'df' (the final_df from the previous step) and 'ROW_LIMIT' from config.\n",
    "df = gap_analyzer_sauce.truncate_dataframe_by_volume(job, df, ROW_LIMIT)\n",
    "\n",
    "# Display the head of the final truncated DataFrame\n",
    "display(df.head())\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Truncated DF stored: {'truncated_df_for_clustering_json' in pip.read_state(job)}\")\n",
    "# loaded_truncated_df = pd.read_json(StringIO(pip.get(job, 'truncated_df_for_clustering_json', '[]')))\n",
    "# print(f\"  Truncated DF rows in state: {len(loaded_truncated_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Cluster Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one function now handles the entire clustering and finalization process:\n",
    "# 1. Loads/tests clustering parameters from a JSON cache file.\n",
    "# 2. Runs iterative ML clustering (TF-IDF, SVD, k-means) to find the best fit.\n",
    "# 3. Names the resulting clusters using n-grams.\n",
    "# 4. Performs the final column reordering.\n",
    "# 5. Saves the final 'unformatted_csv'.\n",
    "# 6. Prints the final cluster counts.\n",
    "# 7. Stores the final DataFrame in pip state ('final_clustered_df_json').\n",
    "# 8. Returns the final DataFrame for display.\n",
    "\n",
    "# It receives 'df' (the truncated DF) and 'has_botify' from previous cells.\n",
    "df = gap_analyzer_sauce.cluster_and_finalize_dataframe(job, df, has_botify)\n",
    "\n",
    "# Display the head of the final, clustered, and arranged DataFrame\n",
    "display(df.head())\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Clustered DF stored: {'final_clustered_df_json' in pip.read_state(job)}\")\n",
    "# loaded_clustered_df = pd.read_json(StringIO(pip.get(job, 'final_clustered_df_json', '[]')))\n",
    "# print(f\"  Clustered DF rows in state: {len(loaded_clustered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Deriving competitor info (list, lookup col) from pip state/keys.\n",
    "# 2. Creating the 'deliverables' directory and Excel file path.\n",
    "# 3. Normalizing and scoring the data for the \"Gap Analysis\" tab.\n",
    "# 4. Writing this first tab to the Excel file.\n",
    "# 5. Creating the \"Open Deliverables Folder\" button.\n",
    "# 6. Storing all necessary paths and lists ('final_xl_file', 'loop_list', 'competitors_list', etc.) in pip state.\n",
    "# 7. Returning the button and key variables for the next step.\n",
    "\n",
    "# It receives 'df' and 'has_botify' from previous cells.\n",
    "(\n",
    "    button, \n",
    "    xl_file, \n",
    "    loop_list, \n",
    "    competitors, \n",
    "    semrush_lookup, \n",
    "    TARGET_COMPETITOR_COL, \n",
    "    has_botify\n",
    ") = gap_analyzer_sauce.create_deliverables_excel_and_button(\n",
    "    job,\n",
    "    df,\n",
    "    keys.client_domain, # Pass the clean domain\n",
    "    has_botify\n",
    ")\n",
    "\n",
    "# Display the button\n",
    "display(button)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final XL File stored: {pip.get(job, 'final_xl_file')}\")\n",
    "# print(f\"  Loop List stored: {pip.get(job, 'loop_list')}\")\n",
    "# print(f\"  Competitors stored: {pip.get(job, 'competitors_list')}\")\n",
    "# print(f\"  Target Col stored: {pip.get(job, 'target_competitor_col')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Defining and finding the canonical client competitor column (TARGET_COMPETITOR_COL).\n",
    "# 2. Defining helper functions for reading/filtering keywords.\n",
    "# 3. Looping through all filter definitions (\"Important Keywords\", \"Best Opportunities\", etc.).\n",
    "# 4. For each filter:\n",
    "#    - Slicing the main 'df'.\n",
    "#    - Normalizing and scoring the slice.\n",
    "#    - Sorting the slice.\n",
    "#    - Appending it as a new tab to the existing Excel file.\n",
    "# 5. Re-attaching the click handler to the button.\n",
    "# 6. Returning the button for re-display.\n",
    "\n",
    "# It receives all necessary objects from the previous cells.\n",
    "button = gap_analyzer_sauce.add_filtered_excel_tabs(\n",
    "    job,\n",
    "    df,\n",
    "    semrush_lookup,\n",
    "    has_botify,\n",
    "    competitors,\n",
    "    xl_file,\n",
    "    TARGET_COMPETITOR_COL, # This was returned by the previous function\n",
    "    button\n",
    ")\n",
    "\n",
    "# Re-display the button (its on_click handler is preserved)\n",
    "display(button)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# try:\n",
    "#     wb = openpyxl.load_workbook(xl_file)\n",
    "#     print(f\"  Excel sheets: {wb.sheetnames}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"  Could not read Excel file to verify sheets: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font, Alignment, Border, Side\n",
    "from openpyxl.formatting.rule import ColorScaleRule\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "from openpyxl.utils import get_column_letter\n",
    "import re # Needed for is_safe_url\n",
    "import validators # Need to import validators for URL check\n",
    "\n",
    "print(f\"🎨 Applying Excel Formatting to all data tabs in {xl_file.name} (third pass)...\")\n",
    "\n",
    "# NOTE: This cell assumes 'xl_file', 'competitors', 'semrush_lookup', 'has_botify'\n",
    "#       'TARGET_COMPETITOR_COL' (the verified column name) are defined in previous cells.\n",
    "\n",
    "# --- REQUIRED SUPPORT FUNCTIONS (Surgically Ported/Defined) ---\n",
    "\n",
    "def create_column_mapping(sheet):\n",
    "    \"\"\"Creates a dictionary mapping header names to column letters.\"\"\"\n",
    "    mapping = {}\n",
    "    for col_idx, column_cell in enumerate(sheet[1], 1): # Assumes headers are in row 1\n",
    "        column_letter = get_column_letter(col_idx)\n",
    "        mapping[str(column_cell.value)] = column_letter\n",
    "    return mapping\n",
    "\n",
    "def apply_fill_to_column_labels(sheet, column_mapping, columns_list, fill):\n",
    "    \"\"\"Applies a fill color to the header cells of specified columns.\"\"\"\n",
    "    for column_name in columns_list:\n",
    "        column_letter = column_mapping.get(column_name)\n",
    "        if column_letter:\n",
    "            cell = sheet[f\"{column_letter}1\"]\n",
    "            cell.fill = fill\n",
    "\n",
    "def find_last_data_row(sheet, keyword_column_letter):\n",
    "    \"\"\"Finds the last row containing data in a specific column (e.g., 'Keyword').\"\"\"\n",
    "    if not keyword_column_letter: # Handle case where keyword column might be missing\n",
    "        return sheet.max_row\n",
    "\n",
    "    last_row = sheet.max_row\n",
    "    # Iterate backwards from the max row\n",
    "    while last_row > 1 and sheet[f\"{keyword_column_letter}{last_row}\"].value in [None, \"\", \" \"]:\n",
    "        last_row -= 1\n",
    "    return last_row\n",
    "\n",
    "def apply_conditional_formatting(sheet, column_mapping, last_row, conditionals_descending, conditionals_ascending, rule_desc, rule_asc):\n",
    "    \"\"\"Applies color scale conditional formatting to specified columns.\"\"\"\n",
    "    for label in conditionals_descending + conditionals_ascending:\n",
    "        column_letter = column_mapping.get(label)\n",
    "        if column_letter and last_row > 1: # Ensure there is data to format\n",
    "            range_string = f'{column_letter}2:{column_letter}{last_row}'\n",
    "            rule = rule_desc if label in conditionals_descending else rule_asc\n",
    "            try:\n",
    "                sheet.conditional_formatting.add(range_string, rule)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to apply conditional formatting for {label}: {e}\")\n",
    "\n",
    "def is_safe_url(url):\n",
    "    \"\"\" Check if the given string is a valid URL using the validators library. \"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return False\n",
    "    # Use validators library for robust URL check\n",
    "    return validators.url(url)\n",
    "\n",
    "# Color schemes and patterns\n",
    "green = '33FF33'\n",
    "client_color = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid') # Yellow\n",
    "competitor_color = PatternFill(start_color='EEECE2', end_color='EEECE2', fill_type='solid') # Light Gray\n",
    "semrush_color = PatternFill(start_color='FAEADB', end_color='FAEADB', fill_type='solid') # Light Orange\n",
    "semrush_opportunity_color = PatternFill(start_color='F1C196', end_color='F1C196', fill_type='solid') # Darker Orange\n",
    "botify_color = PatternFill(start_color='EADFF2', end_color='EADFF2', fill_type='solid') # Light Purple\n",
    "botify_opportunity_color = PatternFill(start_color='AEA1C4', end_color='AEA1C4', fill_type='solid') # Darker Purple\n",
    "color_scale_rule_desc = ColorScaleRule(start_type='min', start_color='FFFFFF', end_type='max', end_color=green) # White to Green (Higher is Better)\n",
    "color_scale_rule_asc = ColorScaleRule(start_type='min', start_color=green, end_type='max', end_color='FFFFFF') # Green to White (Lower is Better)\n",
    "\n",
    "# Create a border style (Subtle hair lines, thin bottom for headers)\n",
    "thin_border = Border(left=Side(style='hair'), right=Side(style='hair'), top=Side(style='hair'), bottom=Side(style='thin'))\n",
    "\n",
    "# Commonly reused column widths\n",
    "tiny_width = 11\n",
    "small_width = 15\n",
    "medium_width = 20\n",
    "description_width = 50\n",
    "url_width = 70 # Adjusted slightly down from 100 for better viewability\n",
    "\n",
    "# Define column widths (Verbatim)\n",
    "column_widths = {\n",
    "    'Keyword': 40, 'Search Volume': small_width, 'Number of Words': tiny_width,\n",
    "    'Keyword Group (Experimental)': small_width, 'Competitors Positioning': tiny_width,\n",
    "    'CPC': tiny_width, 'Keyword Difficulty': tiny_width, 'Competition': tiny_width,\n",
    "    'Depth': tiny_width, 'No. of Keywords': tiny_width,\n",
    "    'No. of Impressions excluding anonymized queries': small_width,\n",
    "    'No. of Clicks excluding anonymized queries': small_width,\n",
    "    'No. of Missed Clicks excluding anonymized queries': small_width,\n",
    "    'Avg. URL CTR excluding anonymized queries': tiny_width,\n",
    "    'Avg. URL Position excluding anonymized queries': tiny_width,\n",
    "    'No. of Keywords for the URL To Achieve 90% Audience': tiny_width,\n",
    "    'Raw Internal Pagerank': small_width, 'Internal Pagerank': tiny_width,\n",
    "    'Internal Pagerank Position': tiny_width, 'No. of Unique Inlinks': tiny_width,\n",
    "    'Title': description_width, 'Meta Description': description_width,\n",
    "    'Timestamp': 12, 'SERP Features by Keyword': description_width,\n",
    "    'Keyword Intents': medium_width, 'Position Type': small_width,\n",
    "    'Number of Results': medium_width, 'Competitor URL': url_width,\n",
    "    'Client URL': url_width, # This gets renamed later\n",
    "    # Normalized/Score columns\n",
    "    'Normalized CPC': tiny_width, 'Normalized Keyword Difficulty': tiny_width,\n",
    "    'Normalized Raw Internal Pagerank': tiny_width, 'Normalized Search Volume': tiny_width,\n",
    "    'Normalized Search Position': tiny_width, 'Normalized Missed Clicks': tiny_width,\n",
    "    'Combined Score': tiny_width\n",
    "}\n",
    "\n",
    "# Commonly used number formats (Verbatim)\n",
    "int_fmt = '0'\n",
    "comma_fmt = '#,##0'\n",
    "pct_fmt = '0.00'\n",
    "date_fmt = 'yyyy-mm-dd' # Added for Timestamp clarity\n",
    "\n",
    "# Define number formats (Added Timestamp)\n",
    "number_formats = {\n",
    "    'Search Volume': comma_fmt, 'Number of Words': int_fmt, 'CPC': pct_fmt,\n",
    "    'Keyword Difficulty': int_fmt, 'Competition': pct_fmt, 'Depth': int_fmt,\n",
    "    'No. of Keywords': comma_fmt, 'No. of Impressions excluding anonymized queries': comma_fmt,\n",
    "    'No. of Clicks excluding anonymized queries': comma_fmt,\n",
    "    'No. of Missed Clicks excluding anonymized queries': comma_fmt,\n",
    "    'Avg. URL CTR excluding anonymized queries': pct_fmt,\n",
    "    'Avg. URL Position excluding anonymized queries': '0.0',\n",
    "    'No. of Keywords for the URL To Achieve 90% Audience': comma_fmt,\n",
    "    'Raw Internal Pagerank': '0.0000000', 'Internal Pagerank': pct_fmt,\n",
    "    'Internal Pagerank Position': int_fmt, 'No. of Unique Inlinks': comma_fmt,\n",
    "    'Number of Results': comma_fmt, 'Timestamp': date_fmt,\n",
    "    # Apply comma format to positioning and scores for consistency\n",
    "    'Competitors Positioning': int_fmt, 'Normalized CPC': pct_fmt,\n",
    "    'Normalized Keyword Difficulty': pct_fmt, 'Normalized Raw Internal Pagerank': pct_fmt,\n",
    "    'Normalized Search Volume': pct_fmt, 'Normalized Search Position': pct_fmt,\n",
    "    'Normalized Missed Clicks': pct_fmt, 'Combined Score': '0.00'\n",
    "}\n",
    "\n",
    "# --- DEFINE COLUMN GROUPS FOR COLORING (Verbatim, adapted for known columns) ---\n",
    "# Higher Numbers More Green (Descending is better)\n",
    "conditionals_descending = [\n",
    "    'Search Volume', 'CPC', 'Competition', # Removed Traffic metrics as they were dropped\n",
    "    'Avg. URL CTR excluding anonymized queries',\n",
    "    'No. of Missed Clicks excluding anonymized queries', 'Combined Score',\n",
    "    'No. of Unique Inlinks' # Added Inlinks (usually higher is better contextually)\n",
    "]\n",
    "# Lower Numbers More Green (Ascending is better)\n",
    "conditionals_ascending = [\n",
    "    'Keyword Difficulty', 'Raw Internal Pagerank', 'Internal Pagerank',\n",
    "    'Internal Pagerank Position', 'Avg. URL Position excluding anonymized queries', 'Depth',\n",
    "    TARGET_COMPETITOR_COL # Add the client's position column dynamically\n",
    "] + [col for col in competitors if col != TARGET_COMPETITOR_COL] # Add other competitor position columns\n",
    "\n",
    "# SEMRush Data Columns\n",
    "semrush_columns = [\n",
    "    'Keyword', 'Search Volume', 'CPC', 'Keyword Difficulty', 'Competition',\n",
    "    'SERP Features by Keyword', 'Keyword Intents', 'Position Type',\n",
    "    'Number of Results', 'Timestamp', 'Competitor URL', 'Client URL' # Includes Client URL before rename\n",
    "]\n",
    "# Botify Data Columns (Ensure these match final DataFrame after merge)\n",
    "botify_columns = [\n",
    "    'Depth', 'No. of Keywords', 'No. of Impressions excluding anonymized queries',\n",
    "    'No. of Clicks excluding anonymized queries', 'No. of Missed Clicks excluding anonymized queries',\n",
    "    'Avg. URL CTR excluding anonymized queries', 'Avg. URL Position excluding anonymized queries',\n",
    "    'No. of Keywords for the URL To Achieve 90% Audience', 'Raw Internal Pagerank',\n",
    "    'Internal Pagerank', 'Internal Pagerank Position', 'No. of Unique Inlinks',\n",
    "    'Title', 'Meta Description' # Changed from API name\n",
    "]\n",
    "# Columns which get bigger header fonts\n",
    "bigger_font_headers = [\n",
    "    \"Keyword\", \"Search Volume\", \"Title\", \"Meta Description\",\n",
    "    \"Competitor URL\", \"Client URL\", \"SERP Features by Keyword\"\n",
    "]\n",
    "# Columns which get darker Botify color\n",
    "botify_opportunity_columns = [\n",
    "    'Internal Pagerank', 'No. of Unique Inlinks',\n",
    "    'No. of Missed Clicks excluding anonymized queries',\n",
    "    'Normalized Raw Internal Pagerank', 'Normalized Missed Clicks'\n",
    "]\n",
    "# Columns which get darker SEMRush color\n",
    "semrush_opportunity_columns = [\n",
    "    'CPC', 'Keyword Difficulty', 'Normalized CPC', 'Normalized Keyword Difficulty',\n",
    "    'Normalized Search Volume', 'Normalized Search Position', 'Combined Score' # Added Combined Score here\n",
    "]\n",
    "\n",
    "\n",
    "# --- APPLY FORMATTING TO EXCEL FILE ---\n",
    "try:\n",
    "    wb = load_workbook(xl_file)\n",
    "\n",
    "    # --- UPDATED: Get all sheet names EXCEPT the diagnostics sheet ---\n",
    "    sheets_to_format = [name for name in wb.sheetnames if name != \"Filter Diagnostics\"]\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if not sheets_to_format:\n",
    "         print(\"⚠️ No data sheets found in the Excel file to format. Skipping formatting.\")\n",
    "\n",
    "    for sheet_name in sheets_to_format:\n",
    "        print(f\"- Formatting '{sheet_name}' tab...\")\n",
    "        sheet = wb[sheet_name]\n",
    "        column_mapping = create_column_mapping(sheet)\n",
    "\n",
    "        # Determine the last row with data based on the 'Keyword' column\n",
    "        keyword_col_letter = column_mapping.get(\"Keyword\")\n",
    "        # Add a check in case a sheet somehow doesn't have a Keyword column\n",
    "        if not keyword_col_letter:\n",
    "             print(f\"  Skipping sheet '{sheet_name}': Cannot find 'Keyword' column for formatting reference.\")\n",
    "             continue\n",
    "             \n",
    "        last_row = find_last_data_row(sheet, keyword_col_letter)\n",
    "        \n",
    "        # --- Apply Formatting ---\n",
    "\n",
    "        # 1. Fill client column (using TARGET_COMPETITOR_COL identified earlier)\n",
    "        client_column_letter = column_mapping.get(TARGET_COMPETITOR_COL)\n",
    "        if client_column_letter:\n",
    "            for row in range(1, last_row + 1):\n",
    "                cell = sheet[f\"{client_column_letter}{row}\"]\n",
    "                cell.fill = client_color\n",
    "                if row == 1: cell.font = Font(bold=True) # Bold header\n",
    "\n",
    "        # 2. Fill Header Backgrounds\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, semrush_columns, semrush_color)\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, botify_columns, botify_color)\n",
    "        # Apply competitor color only to competitor columns *present* in this sheet\n",
    "        present_competitors = [c for c in competitors if c in column_mapping and c != TARGET_COMPETITOR_COL]\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, present_competitors, competitor_color)\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, botify_opportunity_columns, botify_opportunity_color)\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, semrush_opportunity_columns, semrush_opportunity_color)\n",
    "\n",
    "        # 3. Header Styling (Alignment, Font, Border)\n",
    "        header_font = Font(bold=True)\n",
    "        header_align = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
    "        for header, col_letter in column_mapping.items():\n",
    "            cell = sheet[f\"{col_letter}1\"]\n",
    "            cell.alignment = header_align\n",
    "            cell.font = header_font\n",
    "            cell.border = thin_border # Apply border to header\n",
    "            if header in bigger_font_headers:\n",
    "                 cell.font = Font(size=14, bold=True) # Slightly smaller than original for balance\n",
    "\n",
    "        # 4. Hyperlinks (Competitor URL, Client URL)\n",
    "        for col_label in [\"Competitor URL\", \"Client URL\"]:\n",
    "            col_letter = column_mapping.get(col_label)\n",
    "            if col_letter:\n",
    "                for row in range(2, last_row + 1):\n",
    "                    cell = sheet[f\"{col_letter}{row}\"]\n",
    "                    url = cell.value\n",
    "                    if url and is_safe_url(url) and not str(url).startswith('=HYPERLINK'):\n",
    "                        # Truncate displayed URL if very long, keep full URL in link\n",
    "                        display_text = url if len(url) <= 80 else url[:77] + \"...\"\n",
    "                        cell.value = f'=HYPERLINK(\"{url}\", \"{display_text}\")'\n",
    "                        cell.font = Font(color=\"0000FF\", underline=\"single\")\n",
    "                        cell.alignment = Alignment(vertical='top', wrap_text=False) # Prevent wrap for URLs\n",
    "\n",
    "\n",
    "        # 5. Rotate Competitor Headers & Set Width\n",
    "        competitor_header_align = Alignment(vertical='bottom', textRotation=90, horizontal='center')\n",
    "        for competitor_col_name in competitors:\n",
    "            col_letter = column_mapping.get(competitor_col_name)\n",
    "            if col_letter:\n",
    "                cell = sheet[f\"{col_letter}1\"]\n",
    "                cell.alignment = competitor_header_align\n",
    "                sheet.column_dimensions[col_letter].width = 4\n",
    "\n",
    "        # 6. Apply Column Widths (with Global Adjustment)\n",
    "        for label, width in column_widths.items():\n",
    "            column_letter = column_mapping.get(label)\n",
    "            if column_letter:\n",
    "                # Apply the global width adjustment multiplier\n",
    "                sheet.column_dimensions[column_letter].width = width * GLOBAL_WIDTH_ADJUSTMENT\n",
    "\n",
    "        # 7. Apply Number Formats\n",
    "        for label, format_code in number_formats.items():\n",
    "            column_letter = column_mapping.get(label)\n",
    "            if column_letter:\n",
    "                for row in range(2, last_row + 1):\n",
    "                    cell = sheet[f\"{column_letter}{row}\"]\n",
    "                    # Apply only if cell is not empty, prevents formatting issues\n",
    "                    if cell.value is not None:\n",
    "                        cell.number_format = format_code\n",
    "\n",
    "        # 8. Apply Conditional Formatting (Using the combined rules)\n",
    "        apply_conditional_formatting(sheet, column_mapping, last_row, conditionals_descending, conditionals_ascending, color_scale_rule_desc, color_scale_rule_asc)\n",
    "\n",
    "        # 9. Rename 'Client URL' Header Dynamically\n",
    "        client_url_column_letter = column_mapping.get(\"Client URL\")\n",
    "        if client_url_column_letter:\n",
    "            header_cell = sheet[f\"{client_url_column_letter}1\"]\n",
    "            header_cell.value = f\"{TARGET_COMPETITOR_COL} URL\" # Use the canonical name\n",
    "\n",
    "        # 10. Data Cell Alignment (Wrap text, top align)\n",
    "        data_align = Alignment(wrap_text=False, vertical='top')\n",
    "        url_columns = [column_mapping.get(\"Competitor URL\"), column_mapping.get(\"Client URL\")] # Get letters before loop\n",
    "        for row_idx in range(2, last_row + 1):\n",
    "            for col_idx in range(1, sheet.max_column + 1):\n",
    "                cell = sheet.cell(row=row_idx, column=col_idx)\n",
    "                col_letter = get_column_letter(col_idx)\n",
    "                # Apply default alignment, skip URL columns handled earlier\n",
    "                if col_letter not in url_columns:\n",
    "                    cell.alignment = data_align\n",
    "\n",
    "\n",
    "        # 11. Header Row Height & Freeze Panes\n",
    "        # Use the explicit configuration variable for header height\n",
    "        sheet.row_dimensions[1].height = locals().get('max_length', 15) * 9 if 'max_length' in locals() else 100\n",
    "        sheet.freeze_panes = 'C2' # Freeze panes more appropriately after Keyword/Volume\n",
    "\n",
    "        # 12. Apply AutoFilter\n",
    "        max_col_letter = get_column_letter(sheet.max_column)\n",
    "        if last_row > 0: # Ensure there are rows to filter\n",
    "             sheet.auto_filter.ref = f\"A1:{max_col_letter}{last_row}\"\n",
    "\n",
    "        # 13. (Optional but recommended) Add Table for banded rows (replaces manual banding)\n",
    "        if last_row > 0: # Ensure there is data for the table\n",
    "            table_range = f\"A1:{max_col_letter}{last_row}\"\n",
    "            table_name = f\"DataTable_{re.sub(r'[^A-Za-z0-9_]', '', sheet_name)}\" # Sanitize sheet name for table name\n",
    "\n",
    "            # --- CORRECTED TABLE CHECK ---\n",
    "            # Defensively check if items in sheet._tables have a .name attribute\n",
    "            existing_table_names = [t.name for t in sheet._tables if hasattr(t, 'name')]\n",
    "            if table_name not in existing_table_names:\n",
    "            # --- END CORRECTION ---\n",
    "                 tab = Table(displayName=table_name, ref=table_range)\n",
    "                 style = TableStyleInfo(name=\"TableStyleMedium9\", showFirstColumn=False,\n",
    "                                       showLastColumn=False, showRowStripes=True, showColumnStripes=False)\n",
    "                 tab.tableStyleInfo = style\n",
    "                 try:\n",
    "                      sheet.add_table(tab)\n",
    "                 except ValueError as ve:\n",
    "                      print(f\"  Note: Could not add Excel Table '{table_name}' to sheet '{sheet_name}'. Maybe overlaps existing table? Error: {ve}\")\n",
    "            # Optional: Add an else here if you want to log that the table already exists\n",
    "            # else:\n",
    "            #    print(f\"  Skipping table creation: Table '{table_name}' already exists in sheet '{sheet_name}'.\")\n",
    "\n",
    "    # Save the workbook with all formatting applied\n",
    "    wb.save(xl_file)\n",
    "    print(f\"✅ Formatting applied to all data tabs and saved to {xl_file.name}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Excel file not found at {xl_file}. Cannot apply formatting.\")\n",
    "except KeyError as e:\n",
    "     print(f\"❌ Error during formatting: A required column key was not found: {e}. Check DataFrame structure.\")\n",
    "     # Safely attempt to print mapping if it exists\n",
    "     if 'column_mapping' in locals(): print(f\"   Column Mapping: {column_mapping}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An unexpected error occurred during Excel formatting: {e}\")\n",
    "# Use a lambda function to call the portable _open_folder function on click\n",
    "button.on_click(lambda b: _open_folder(str(deliverables_dir)))\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
