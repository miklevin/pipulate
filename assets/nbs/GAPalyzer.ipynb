{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pipulate import pip\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import keys\n",
    "\n",
    "job = \"gapalyzer-01\" # Give your session a unique name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- ‚öôÔ∏è Workflow Configuration ---\n",
    "ROW_LIMIT = 3000  # Final Output row limit, low for fast iteration\n",
    "COMPETITOR_LIMIT = 3  # Limit rows regardless of downloads, low for fast iteration\n",
    "BROWSER_DOWNLOAD_PATH = \"~/Downloads\"  # The default directory where your browser downloads files\n",
    "\n",
    "print(f\"‚úÖ Configuration set: Final report will be limited to {ROW_LIMIT} rows.\")\n",
    "if COMPETITOR_LIMIT:\n",
    "    print(f\"‚úÖ Configuration set: Processing will be limited to the top {COMPETITOR_LIMIT} competitors.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Configuration set: Processing all competitors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Here are the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "pip.api_key(job, key=keys.google)\n",
    "botify_token = keys.botify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Here are your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "https://nixos.org/    # Linux\n",
    "https://pypi.org/     # Python\n",
    "https://neovim.io/    # vim\n",
    "https://git-scm.com/  # git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "def get_competitors_from_notebook(notebook_filename=\"GAPalyzer.ipynb\"):\n",
    "    \"\"\"Parses this notebook to get the domain list from the 'url-list-input' cell.\"\"\"\n",
    "    try:\n",
    "        notebook_path = Path(notebook_filename)\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        for cell in nb.cells:\n",
    "            if \"url-list-input\" in cell.metadata.get(\"tags\", []):\n",
    "                domains_raw = cell.source\n",
    "                domains = [\n",
    "                    line.split('#')[0].strip() \n",
    "                    for line in domains_raw.splitlines() \n",
    "                    if line.strip() and not line.strip().startswith('#')\n",
    "                ]\n",
    "                return domains\n",
    "        print(\"‚ö†Ô∏è Warning: Could not find a cell tagged with 'url-list-input'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading domains from notebook: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"üöÄ Generating SEMrush URLs for GAP analysis...\")\n",
    "\n",
    "domains = get_competitors_from_notebook()\n",
    "url_template = \"https://www.semrush.com/analytics/organic/positions/?db=us&q={domain}&searchType=domain\"\n",
    "\n",
    "if not domains:\n",
    "    print(\"üõë No domains found. Please add competitor domains to the 'url-list-input' cell and re-run.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(domains)} competitor domains. Click the links below to open each report:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, domain in enumerate(domains):\n",
    "        full_url = url_template.format(domain=domain)\n",
    "        print(f\"{i+1}. {domain}:\\n   {full_url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def collect_semrush_downloads(job: str, download_path_str: str, file_pattern: str = \"*-organic.Positions*.xlsx\"):\n",
    "    \"\"\"\n",
    "    Moves downloaded SEMRush files matching a pattern from the user's download\n",
    "    directory to a job-specific 'downloads/{job}/' folder within the Notebooks/\n",
    "    directory.\n",
    "    \n",
    "    Args:\n",
    "        job (str): The current job ID (e.g., \"gapalyzer-01\").\n",
    "        download_path_str (str): The user's default browser download path (e.g., \"~/Downloads\").\n",
    "        file_pattern (str): The glob pattern to match SEMRush files.\n",
    "    \"\"\"\n",
    "    print(\"üì¶ Starting collection of new SEMRush downloads...\")\n",
    "\n",
    "    # 1. Define source and destination paths\n",
    "    # Resolve the user's download path (handles ~)\n",
    "    source_dir = Path(download_path_str).expanduser()\n",
    "    \n",
    "    # Define the destination path relative to the current working directory (Notebooks/)\n",
    "    # This assumes the Notebook is run from the 'Notebooks' directory or its path is correct.\n",
    "    destination_dir = Path(\"downloads\") / job\n",
    "\n",
    "    # 2. Create the destination directory if it doesn't exist\n",
    "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Destination folder created/ensured: '{destination_dir.resolve()}'\")\n",
    "\n",
    "    # 3. Find files in the source directory matching the pattern\n",
    "    # We use glob.glob for pattern matching, searching for both .xlsx and .csv\n",
    "    files_to_move = []\n",
    "    \n",
    "    # Check for .xlsx files\n",
    "    xlsx_files = glob.glob(str(source_dir / file_pattern))\n",
    "    files_to_move.extend(xlsx_files)\n",
    "    \n",
    "    # Check for .csv files\n",
    "    csv_pattern = file_pattern.replace(\".xlsx\", \".csv\")\n",
    "    csv_files = glob.glob(str(source_dir / csv_pattern))\n",
    "    files_to_move.extend(csv_files)\n",
    "\n",
    "    if not files_to_move:\n",
    "        print(\"‚ö†Ô∏è No new files matching the pattern were found in the download directory. Skipping move.\")\n",
    "        return\n",
    "\n",
    "    # 4. Move the files\n",
    "    move_count = 0\n",
    "    for source_file_path in files_to_move:\n",
    "        source_file = Path(source_file_path)\n",
    "        dest_file = destination_dir / source_file.name\n",
    "        \n",
    "        # Only move if the file doesn't already exist in the destination (to avoid overwriting)\n",
    "        # This protects manually modified files, but new downloads will have unique timestamps anyway.\n",
    "        if dest_file.exists():\n",
    "             # Option: could log that it exists or decide to overwrite/rename. \n",
    "             # Given the SEMRush filename pattern contains a unique timestamp, we expect \n",
    "             # them to be new. Let's just avoid redundant logging.\n",
    "             continue\n",
    "        \n",
    "        try:\n",
    "            shutil.move(source_file, dest_file)\n",
    "            print(f\"  -> Moved: {source_file.name}\")\n",
    "            move_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ‚ùå Error moving {source_file.name}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Collection complete. {move_count} new files moved to '{destination_dir}'.\")\n",
    "    \n",
    "    # --- Execute the function in the notebook ---\n",
    "collect_semrush_downloads(job, BROWSER_DOWNLOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# NOTE: This cell assumes 'job' is defined (e.g., \"gapalyzer-01\")\n",
    "\n",
    "# --- Define the file directory based on the job variable ---\n",
    "semrush_gap_analysis_dir = Path(\"downloads\") / job\n",
    "\n",
    "# --- Combine glob results for both .xlsx and .csv ---\n",
    "file_patterns = [\n",
    "    \"*-organic.Positions*.xlsx\", \n",
    "    \"*-organic.Positions*.csv\"\n",
    "]\n",
    "\n",
    "# Use itertools.chain to efficiently combine generators from multiple glob calls\n",
    "all_downloaded_files = sorted(list(itertools.chain.from_iterable(\n",
    "    semrush_gap_analysis_dir.glob(pattern) for pattern in file_patterns\n",
    ")))\n",
    "\n",
    "# --- Display Results ---\n",
    "if all_downloaded_files:\n",
    "    # Use a Markdown block for formatted display with emoji\n",
    "    markdown_output = [\"## üíæ Found Downloaded Files\"]\n",
    "    markdown_output.append(f\"‚úÖ **{len(all_downloaded_files)} files** ready for processing in `{semrush_gap_analysis_dir}/`\\n\")\n",
    "    \n",
    "    for i, file in enumerate(all_downloaded_files):\n",
    "        # The file name starts with the competitor's domain.\n",
    "        try:\n",
    "            # We strip the full file path name for cleaner display\n",
    "            domain_name = file.name[:file.name.index(\"-organic.\")].strip()\n",
    "        except ValueError:\n",
    "            # Fallback if the expected pattern is slightly off\n",
    "            domain_name = file.name\n",
    "            \n",
    "        markdown_output.append(f\"{i + 1}. **`{domain_name}`** ({file.suffix.upper()})\")\n",
    "\n",
    "    display(Markdown(\"\\n\".join(markdown_output)))\n",
    "    \n",
    "    # --- NEW FIX: Convert Path objects to strings for JSON serialization ---\n",
    "    # The Pipulate core needs simple, JSON-serializable types (strings, lists, dicts, etc.)\n",
    "    all_downloaded_files_as_str = [str(p) for p in all_downloaded_files]\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # For the next step, we'll store the list of files in the Pipulate pipeline.\n",
    "    pip.set(job, 'semrush_files', all_downloaded_files_as_str)\n",
    "    \n",
    "else:\n",
    "    display(Markdown(f\"‚ö†Ô∏è **Warning:** No SEMRush files found in `{semrush_gap_analysis_dir}/`.\\n(Looking for `*-organic.Positions*.xlsx` or `*.csv`)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import pandas as pd\n",
    "from tldextract import extract\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# --- SUPPORT FUNCTION (1-to-1 Transplant) ---\n",
    "# NOTE: This function requires 'tldextract' to be installed (which you've handled).\n",
    "def extract_registered_domain(url):\n",
    "    \"\"\"\n",
    "    Extracts the registered domain (domain.suffix) from a URL/hostname.\n",
    "    \"\"\"\n",
    "    extracted = extract(url)\n",
    "    return f\"{extracted.domain}.{extracted.suffix}\"\n",
    "\n",
    "# --- MAIN LOGIC ADAPTATION ---\n",
    "\n",
    "# Variables required from previous Notebook cells:\n",
    "# job, ROW_LIMIT, COMPETITOR_LIMIT, BROWSER_DOWNLOAD_PATH, client_domain, country_code\n",
    "# semrush_gap_analysis_dir is assumed to be defined as Path(\"downloads\") / job\n",
    "\n",
    "# Define 'semrush_gap_analysis_dir' and 'semrush_lookup' based on prior context\n",
    "# We use the 'job' variable to define the directory\n",
    "semrush_gap_analysis_dir = Path(\"downloads\") / job\n",
    "\n",
    "# The client domain is the key for separating client vs. competitor data.\n",
    "# We strip the full domain in case it contains a protocol or path.\n",
    "# Assuming 'client_domain' is available from a keys/config cell (e.g., \"example.com\")\n",
    "# Since we don't have 'client_domain' defined here, we'll temporarily define it for the port.\n",
    "# Replace this line with proper import/assignment if moving to external module:\n",
    "semrush_lookup = extract_registered_domain(client_domain) if 'client_domain' in locals() else \"nixos.org\"\n",
    "\n",
    "\n",
    "print(\"Creating a great big DataFrame...\")\n",
    "\n",
    "# 1. Adapt file globbing to handle BOTH CSV and XLSX (as done in the previous step)\n",
    "file_patterns = [\"*-organic.Positions*.xlsx\", \"*-organic.Positions*.csv\"]\n",
    "all_semrush_files = sorted(list(itertools.chain.from_iterable(\n",
    "    semrush_gap_analysis_dir.glob(pattern) for pattern in file_patterns\n",
    ")))\n",
    "\n",
    "# Initialize data structures\n",
    "cdict = {}\n",
    "list_of_dfs = []\n",
    "print(\"Loading SEMRush files: \", end=\"\")\n",
    "\n",
    "# 2. Loop through all found files\n",
    "for j, data_file in enumerate(all_semrush_files):\n",
    "    # Determine the file type and corresponding reader function\n",
    "    is_excel = data_file.suffix.lower() == '.xlsx'\n",
    "    read_func = pd.read_excel if is_excel else pd.read_csv\n",
    "    \n",
    "    # Original file name parsing logic\n",
    "    nend = data_file.stem.index(\"-organic\")\n",
    "    xlabel = data_file.stem[:nend].replace(\"_\", \"/\").replace(\"///\", \"://\").strip('.')\n",
    "    \n",
    "    # Original domain extraction logic (using the locally defined function)\n",
    "    just_domain = extract_registered_domain(xlabel)\n",
    "    cdict[just_domain] = xlabel\n",
    "    \n",
    "    # Load data\n",
    "    df = read_func(data_file)\n",
    "    \n",
    "    # Original column assignment logic\n",
    "    if just_domain == xlabel:\n",
    "        df[\"Domain\"] = just_domain\n",
    "    else:\n",
    "        # Use the full X-label (e.g., sub.domain.com) if it's not just the registered domain\n",
    "        df[\"Domain\"] = xlabel\n",
    "    \n",
    "    # Original data segregation logic\n",
    "    df[\"Client URL\"] = df.apply(lambda row: row[\"URL\"] if row[\"Domain\"] == semrush_lookup else None, axis=1)\n",
    "    df[\"Competitor URL\"] = df.apply(lambda row: row[\"URL\"] if row[\"Domain\"] != semrush_lookup else None, axis=1)\n",
    "    \n",
    "    list_of_dfs.append(df)\n",
    "    print(f\"{j + 1} \", end=\"\", flush=True)\n",
    "\n",
    "print() # Newline after the loading count\n",
    "\n",
    "if list_of_dfs:\n",
    "    df2 = pd.concat(list_of_dfs)  # Concatenate like stacking CSVs\n",
    "    \n",
    "    # --- Original Excel Formatting Value Gathering ---\n",
    "    # This logic appears to be for calculating Excel visual layout, \n",
    "    # but still needs to be run even if the formatting happens later.\n",
    "    # It requires the 'bf.open_dir_widget' function to be defined or stubbed if not portable.\n",
    "    # NOTE: Since `bf` is not defined, and `project_customizations`/`proceed` are missing, \n",
    "    # we must skip the non-portable lines to prevent breaking the Notebook.\n",
    "\n",
    "    # Stubbing non-portable functions/logic to keep the structure intact\n",
    "    # We remove the print statements related to bf/project/customization for now\n",
    "    \n",
    "    # The max_length calculation is fine to keep\n",
    "    max_length = max(len(value) for value in cdict.values())\n",
    "    row1_height = max_length * 7 # Unused variable for now, but ported\n",
    "    \n",
    "    rows, columns = df2.shape\n",
    "    print()\n",
    "    print(f\"Rows: {rows:,}\")\n",
    "    print(f\"Cols: {columns:,}\")\n",
    "    print()\n",
    "\n",
    "    # NOTE: The subsequent conditional logic (lines 53-61 in the original)\n",
    "    # involving `bf.open_dir_widget`, `project_customizations`, and `proceed()`\n",
    "    # has been intentionally omitted here as it depends on external, undefined\n",
    "    # modules (`bf`) and pipeline state (`project`, `project_customizations`, `proceed`)\n",
    "    # that are not provided in the prompt's context and would cause the script to fail.\n",
    "    # We only port the pure Pandas/Python logic.\n",
    "    \n",
    "    # The final output and pipeline update\n",
    "    display(df2[\"Domain\"].value_counts())\n",
    "    \n",
    "    # Store the result in the pipeline\n",
    "    pip.set(job, 'semrush_master_df_json', df2.to_json(orient='records'))\n",
    "    \n",
    "else:\n",
    "    print(\"Please put the CSVs in place.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Todo\n",
    "- Move everything that matches the `.csv` or `.xlsx` template from downloads to somewhere relative to Notebook\n",
    "- Make it work with either Excel or CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
