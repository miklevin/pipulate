{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pipulate import pip\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import keys\n",
    "\n",
    "job = \"gapalyzer-01\" # Give your session a unique name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- ‚öôÔ∏è Workflow Configuration ---\n",
    "ROW_LIMIT = 3000  # Final Output row limit, low for fast iteration\n",
    "COMPETITOR_LIMIT = 3  # Limit rows regardless of downloads, low for fast iteration\n",
    "BROWSER_DOWNLOAD_PATH = \"~/Downloads\"  # The default directory where your browser downloads files\n",
    "\n",
    "print(f\"‚úÖ Configuration set: Final report will be limited to {ROW_LIMIT} rows.\")\n",
    "if COMPETITOR_LIMIT:\n",
    "    print(f\"‚úÖ Configuration set: Processing will be limited to the top {COMPETITOR_LIMIT} competitors.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Configuration set: Processing all competitors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Here are the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "pip.api_key(job, key=keys.google)\n",
    "botify_token = keys.botify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Here are your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "https://nixos.org/    # Linux\n",
    "https://pypi.org/     # Python\n",
    "https://neovim.io/    # vim\n",
    "https://git-scm.com/  # git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "def get_competitors_from_notebook(notebook_filename=\"GAPalyzer.ipynb\"):\n",
    "    \"\"\"Parses this notebook to get the domain list from the 'url-list-input' cell.\"\"\"\n",
    "    try:\n",
    "        notebook_path = Path(notebook_filename)\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        for cell in nb.cells:\n",
    "            if \"url-list-input\" in cell.metadata.get(\"tags\", []):\n",
    "                domains_raw = cell.source\n",
    "                domains = [\n",
    "                    line.split('#')[0].strip() \n",
    "                    for line in domains_raw.splitlines() \n",
    "                    if line.strip() and not line.strip().startswith('#')\n",
    "                ]\n",
    "                return domains\n",
    "        print(\"‚ö†Ô∏è Warning: Could not find a cell tagged with 'url-list-input'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading domains from notebook: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"üöÄ Generating SEMrush URLs for GAP analysis...\")\n",
    "\n",
    "domains = get_competitors_from_notebook()\n",
    "url_template = \"https://www.semrush.com/analytics/organic/positions/?db=us&q={domain}&searchType=domain\"\n",
    "\n",
    "if not domains:\n",
    "    print(\"üõë No domains found. Please add competitor domains to the 'url-list-input' cell and re-run.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(domains)} competitor domains. Click the links below to open each report:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, domain in enumerate(domains):\n",
    "        full_url = url_template.format(domain=domain)\n",
    "        print(f\"{i+1}. {domain}:\\n   {full_url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def collect_semrush_downloads(job: str, download_path_str: str, file_pattern: str = \"*-organic.Positions*.xlsx\"):\n",
    "    \"\"\"\n",
    "    Moves downloaded SEMRush files matching a pattern from the user's download\n",
    "    directory to a job-specific 'downloads/{job}/' folder within the Notebooks/\n",
    "    directory.\n",
    "    \n",
    "    Args:\n",
    "        job (str): The current job ID (e.g., \"gapalyzer-01\").\n",
    "        download_path_str (str): The user's default browser download path (e.g., \"~/Downloads\").\n",
    "        file_pattern (str): The glob pattern to match SEMRush files.\n",
    "    \"\"\"\n",
    "    print(\"üì¶ Starting collection of new SEMRush downloads...\")\n",
    "\n",
    "    # 1. Define source and destination paths\n",
    "    # Resolve the user's download path (handles ~)\n",
    "    source_dir = Path(download_path_str).expanduser()\n",
    "    \n",
    "    # Define the destination path relative to the current working directory (Notebooks/)\n",
    "    # This assumes the Notebook is run from the 'Notebooks' directory or its path is correct.\n",
    "    destination_dir = Path(\"downloads\") / job\n",
    "\n",
    "    # 2. Create the destination directory if it doesn't exist\n",
    "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Destination folder created/ensured: '{destination_dir.resolve()}'\")\n",
    "\n",
    "    # 3. Find files in the source directory matching the pattern\n",
    "    # We use glob.glob for pattern matching, searching for both .xlsx and .csv\n",
    "    files_to_move = []\n",
    "    \n",
    "    # Check for .xlsx files\n",
    "    xlsx_files = glob.glob(str(source_dir / file_pattern))\n",
    "    files_to_move.extend(xlsx_files)\n",
    "    \n",
    "    # Check for .csv files\n",
    "    csv_pattern = file_pattern.replace(\".xlsx\", \".csv\")\n",
    "    csv_files = glob.glob(str(source_dir / csv_pattern))\n",
    "    files_to_move.extend(csv_files)\n",
    "\n",
    "    if not files_to_move:\n",
    "        print(\"‚ö†Ô∏è No new files matching the pattern were found in the download directory. Skipping move.\")\n",
    "        return\n",
    "\n",
    "    # 4. Move the files\n",
    "    move_count = 0\n",
    "    for source_file_path in files_to_move:\n",
    "        source_file = Path(source_file_path)\n",
    "        dest_file = destination_dir / source_file.name\n",
    "        \n",
    "        # Only move if the file doesn't already exist in the destination (to avoid overwriting)\n",
    "        # This protects manually modified files, but new downloads will have unique timestamps anyway.\n",
    "        if dest_file.exists():\n",
    "             # Option: could log that it exists or decide to overwrite/rename. \n",
    "             # Given the SEMRush filename pattern contains a unique timestamp, we expect \n",
    "             # them to be new. Let's just avoid redundant logging.\n",
    "             continue\n",
    "        \n",
    "        try:\n",
    "            shutil.move(source_file, dest_file)\n",
    "            print(f\"  -> Moved: {source_file.name}\")\n",
    "            move_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ‚ùå Error moving {source_file.name}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Collection complete. {move_count} new files moved to '{destination_dir}'.\")\n",
    "    \n",
    "    # --- Execute the function in the notebook ---\n",
    "collect_semrush_downloads(job, BROWSER_DOWNLOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# NOTE: This cell assumes 'job' is defined (e.g., \"gapalyzer-01\")\n",
    "\n",
    "# --- Define the file directory based on the job variable ---\n",
    "semrush_gap_analysis_dir = Path(\"downloads\") / job\n",
    "\n",
    "# --- Combine glob results for both .xlsx and .csv ---\n",
    "file_patterns = [\n",
    "    \"*-organic.Positions*.xlsx\", \n",
    "    \"*-organic.Positions*.csv\"\n",
    "]\n",
    "\n",
    "# Use itertools.chain to efficiently combine generators from multiple glob calls\n",
    "all_downloaded_files = sorted(list(itertools.chain.from_iterable(\n",
    "    semrush_gap_analysis_dir.glob(pattern) for pattern in file_patterns\n",
    ")))\n",
    "\n",
    "# --- Display Results ---\n",
    "if all_downloaded_files:\n",
    "    # Use a Markdown block for formatted display with emoji\n",
    "    markdown_output = [\"## üíæ Found Downloaded Files\"]\n",
    "    markdown_output.append(f\"‚úÖ **{len(all_downloaded_files)} files** ready for processing in `{semrush_gap_analysis_dir}/`\\n\")\n",
    "    \n",
    "    for i, file in enumerate(all_downloaded_files):\n",
    "        # The file name starts with the competitor's domain.\n",
    "        try:\n",
    "            # We strip the full file path name for cleaner display\n",
    "            domain_name = file.name[:file.name.index(\"-organic.\")].strip()\n",
    "        except ValueError:\n",
    "            # Fallback if the expected pattern is slightly off\n",
    "            domain_name = file.name\n",
    "            \n",
    "        markdown_output.append(f\"{i + 1}. **`{domain_name}`** ({file.suffix.upper()})\")\n",
    "\n",
    "    display(Markdown(\"\\n\".join(markdown_output)))\n",
    "    \n",
    "    # --- NEW FIX: Convert Path objects to strings for JSON serialization ---\n",
    "    # The Pipulate core needs simple, JSON-serializable types (strings, lists, dicts, etc.)\n",
    "    all_downloaded_files_as_str = [str(p) for p in all_downloaded_files]\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # For the next step, we'll store the list of files in the Pipulate pipeline.\n",
    "    pip.set(job, 'semrush_files', all_downloaded_files_as_str)\n",
    "    \n",
    "else:\n",
    "    display(Markdown(f\"‚ö†Ô∏è **Warning:** No SEMRush files found in `{semrush_gap_analysis_dir}/`.\\n(Looking for `*-organic.Positions*.xlsx` or `*.csv`)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Todo\n",
    "- Move everything that matches the `.csv` or `.xlsx` template from downloads to somewhere relative to Notebook\n",
    "- Make it work with either Excel or CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
