{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "1. Fix where the temporary files are being stored.\n",
    "2. Test with different sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pipulate import pip\n",
    "import gap_analyzer_sauce\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import keys\n",
    "\n",
    "job = \"gapalyzer-02\" # Give your session a unique name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- ‚öôÔ∏è Workflow Configuration ---\n",
    "ROW_LIMIT = 3000  # Final Output row limit, low for fast iteration\n",
    "COMPETITOR_LIMIT = 3  # Limit rows regardless of downloads, low for fast iteration\n",
    "BROWSER_DOWNLOAD_PATH = \"~/Downloads\"  # The default directory where your browser downloads files\n",
    "GLOBAL_WIDTH_ADJUSTMENT = 1.5  #Multiplier to globally adjust column widths (1.0 = no change, 1.2 = 20% wider)\n",
    "\n",
    "print(f\"‚úÖ Configuration set: Final report will be limited to {ROW_LIMIT} rows.\")\n",
    "if COMPETITOR_LIMIT:\n",
    "    print(f\"‚úÖ Configuration set: Processing will be limited to the top {COMPETITOR_LIMIT} competitors.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Configuration set: Processing all competitors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Here are the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "pip.api_key(job, key=keys.google)\n",
    "botify_token = keys.botify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Here are your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "https://nixos.org/    # Linux\n",
    "https://pypi.org/     # Python\n",
    "https://neovim.io/    # vim\n",
    "https://git-scm.com/  # git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gap_analyzer_sauce # Import the new module\n",
    "\n",
    "# Call the function from the sauce module\n",
    "# This performs the extraction, stores domains via pip.set, prints URLs,\n",
    "# and returns the domains list if needed elsewhere (though we primarily rely on pip state now).\n",
    "competitor_domains = gap_analyzer_sauce.extract_domains_and_print_urls(job)\n",
    "\n",
    "# Optional: You could add a pip.get here for verification if desired\n",
    "# stored_domains = pip.get(job, 'competitor_domains', [])\n",
    "# print(f\"\\nVerification: Retrieved {len(stored_domains)} domains from pip state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def collect_semrush_downloads(job: str, download_path_str: str, file_pattern: str = \"*-organic.Positions*.xlsx\"):\n",
    "    \"\"\"\n",
    "    Moves downloaded SEMRush files matching a pattern from the user's download\n",
    "    directory to a job-specific 'downloads/{job}/' folder within the Notebooks/\n",
    "    directory.\n",
    "    \n",
    "    Args:\n",
    "        job (str): The current job ID (e.g., \"gapalyzer-01\").\n",
    "        download_path_str (str): The user's default browser download path (e.g., \"~/Downloads\").\n",
    "        file_pattern (str): The glob pattern to match SEMRush files.\n",
    "    \"\"\"\n",
    "    print(\"üì¶ Starting collection of new SEMRush downloads...\")\n",
    "\n",
    "    # 1. Define source and destination paths\n",
    "    # Resolve the user's download path (handles ~)\n",
    "    source_dir = Path(download_path_str).expanduser()\n",
    "    \n",
    "    # Define the destination path relative to the current working directory (Notebooks/)\n",
    "    # This assumes the Notebook is run from the 'Notebooks' directory or its path is correct.\n",
    "    destination_dir = Path(\"downloads\") / job\n",
    "\n",
    "    # 2. Create the destination directory if it doesn't exist\n",
    "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Destination folder created/ensured: '{destination_dir.resolve()}'\")\n",
    "\n",
    "    # 3. Find files in the source directory matching the pattern\n",
    "    # We use glob.glob for pattern matching, searching for both .xlsx and .csv\n",
    "    files_to_move = []\n",
    "    \n",
    "    # Check for .xlsx files\n",
    "    xlsx_files = glob.glob(str(source_dir / file_pattern))\n",
    "    files_to_move.extend(xlsx_files)\n",
    "    \n",
    "    # Check for .csv files\n",
    "    csv_pattern = file_pattern.replace(\".xlsx\", \".csv\")\n",
    "    csv_files = glob.glob(str(source_dir / csv_pattern))\n",
    "    files_to_move.extend(csv_files)\n",
    "\n",
    "    if not files_to_move:\n",
    "        print(\"‚ö†Ô∏è No new files matching the pattern were found in the download directory. Skipping move.\")\n",
    "        return\n",
    "\n",
    "    # 4. Move the files\n",
    "    move_count = 0\n",
    "    for source_file_path in files_to_move:\n",
    "        source_file = Path(source_file_path)\n",
    "        dest_file = destination_dir / source_file.name\n",
    "        \n",
    "        # Only move if the file doesn't already exist in the destination (to avoid overwriting)\n",
    "        # This protects manually modified files, but new downloads will have unique timestamps anyway.\n",
    "        if dest_file.exists():\n",
    "             # Option: could log that it exists or decide to overwrite/rename. \n",
    "             # Given the SEMRush filename pattern contains a unique timestamp, we expect \n",
    "             # them to be new. Let's just avoid redundant logging.\n",
    "             continue\n",
    "        \n",
    "        try:\n",
    "            shutil.move(source_file, dest_file)\n",
    "            print(f\"  -> Moved: {source_file.name}\")\n",
    "            move_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ‚ùå Error moving {source_file.name}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Collection complete. {move_count} new files moved to '{destination_dir}'.\")\n",
    "    \n",
    "    # --- Execute the function in the notebook ---\n",
    "collect_semrush_downloads(job, BROWSER_DOWNLOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# NOTE: This cell assumes 'job' is defined (e.g., \"gapalyzer-01\")\n",
    "\n",
    "# --- Define the file directory based on the job variable ---\n",
    "semrush_gap_analysis_dir = Path(\"downloads\") / job\n",
    "\n",
    "# --- Combine glob results for both .xlsx and .csv ---\n",
    "file_patterns = [\n",
    "    \"*-organic.Positions*.xlsx\", \n",
    "    \"*-organic.Positions*.csv\"\n",
    "]\n",
    "\n",
    "# Use itertools.chain to efficiently combine generators from multiple glob calls\n",
    "all_downloaded_files = sorted(list(itertools.chain.from_iterable(\n",
    "    semrush_gap_analysis_dir.glob(pattern) for pattern in file_patterns\n",
    ")))\n",
    "\n",
    "# --- Display Results ---\n",
    "if all_downloaded_files:\n",
    "    # Use a Markdown block for formatted display with emoji\n",
    "    markdown_output = [\"## üíæ Found Downloaded Files\"]\n",
    "    markdown_output.append(f\"‚úÖ **{len(all_downloaded_files)} files** ready for processing in `{semrush_gap_analysis_dir}/`\\n\")\n",
    "    \n",
    "    for i, file in enumerate(all_downloaded_files):\n",
    "        # The file name starts with the competitor's domain.\n",
    "        try:\n",
    "            # We strip the full file path name for cleaner display\n",
    "            domain_name = file.name[:file.name.index(\"-organic.\")].strip()\n",
    "        except ValueError:\n",
    "            # Fallback if the expected pattern is slightly off\n",
    "            domain_name = file.name\n",
    "            \n",
    "        markdown_output.append(f\"{i + 1}. **`{domain_name}`** ({file.suffix.upper()})\")\n",
    "\n",
    "    display(Markdown(\"\\n\".join(markdown_output)))\n",
    "    \n",
    "    # --- NEW FIX: Convert Path objects to strings for JSON serialization ---\n",
    "    # The Pipulate core needs simple, JSON-serializable types (strings, lists, dicts, etc.)\n",
    "    all_downloaded_files_as_str = [str(p) for p in all_downloaded_files]\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # For the next step, we'll store the list of files in the Pipulate pipeline.\n",
    "    pip.set(job, 'semrush_files', all_downloaded_files_as_str)\n",
    "    \n",
    "else:\n",
    "    display(Markdown(f\"‚ö†Ô∏è **Warning:** No SEMRush files found in `{semrush_gap_analysis_dir}/`.\\n(Looking for `*-organic.Positions*.xlsx` or `*.csv`)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tldextract import extract\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# --- SUPPORT FUNCTION (1-to-1 Transplant) ---\n",
    "# NOTE: This function requires 'tldextract' to be installed (which you've handled).\n",
    "def extract_registered_domain(url):\n",
    "    \"\"\"\n",
    "    Extracts the registered domain (domain.suffix) from a URL/hostname.\n",
    "    \"\"\"\n",
    "    extracted = extract(url)\n",
    "    return f\"{extracted.domain}.{extracted.suffix}\"\n",
    "\n",
    "# --- MAIN LOGIC ADAPTATION ---\n",
    "\n",
    "# Variables required from previous Notebook cells:\n",
    "# job, ROW_LIMIT, COMPETITOR_LIMIT, BROWSER_DOWNLOAD_PATH, client_domain, country_code\n",
    "# semrush_gap_analysis_dir is assumed to be defined as Path(\"downloads\") / job\n",
    "\n",
    "# Define 'semrush_gap_analysis_dir' and 'semrush_lookup' based on prior context\n",
    "# We use the 'job' variable to define the directory\n",
    "semrush_gap_analysis_dir = Path(\"downloads\") / job\n",
    "\n",
    "# The client domain is the key for separating client vs. competitor data.\n",
    "# We strip the full domain in case it contains a protocol or path.\n",
    "# Assuming 'client_domain' is available from a keys/config cell (e.g., \"example.com\")\n",
    "# Since we don't have 'client_domain' defined here, we'll temporarily define it for the port.\n",
    "# Replace this line with proper import/assignment if moving to external module:\n",
    "semrush_lookup = extract_registered_domain(keys.client_domain)\n",
    "\n",
    "print(f\"Creating a great big DataFrame for {semrush_lookup}...\")\n",
    "\n",
    "# 1. Adapt file globbing to handle BOTH CSV and XLSX (as done in the previous step)\n",
    "file_patterns = [\"*-organic.Positions*.xlsx\", \"*-organic.Positions*.csv\"]\n",
    "all_semrush_files = sorted(list(itertools.chain.from_iterable(\n",
    "    semrush_gap_analysis_dir.glob(pattern) for pattern in file_patterns\n",
    ")))\n",
    "\n",
    "# Initialize data structures\n",
    "cdict = {}\n",
    "list_of_dfs = []\n",
    "print(\"Loading SEMRush files: \", end=\"\")\n",
    "\n",
    "# 2. Loop through all found files\n",
    "for j, data_file in enumerate(all_semrush_files):\n",
    "    # Determine the file type and corresponding reader function\n",
    "    is_excel = data_file.suffix.lower() == '.xlsx'\n",
    "    read_func = pd.read_excel if is_excel else pd.read_csv\n",
    "    \n",
    "    # Original file name parsing logic\n",
    "    nend = data_file.stem.index(\"-organic\")\n",
    "    xlabel = data_file.stem[:nend].replace(\"_\", \"/\").replace(\"///\", \"://\").strip('.')\n",
    "    \n",
    "    # Original domain extraction logic (using the locally defined function)\n",
    "    just_domain = extract_registered_domain(xlabel)\n",
    "    cdict[just_domain] = xlabel\n",
    "    \n",
    "    # Load data\n",
    "    df = read_func(data_file)\n",
    "    \n",
    "    # Original column assignment logic\n",
    "    if just_domain == xlabel:\n",
    "        df[\"Domain\"] = just_domain\n",
    "    else:\n",
    "        # Use the full X-label (e.g., sub.domain.com) if it's not just the registered domain\n",
    "        df[\"Domain\"] = xlabel\n",
    "    \n",
    "    # Original data segregation logic\n",
    "    df[\"Client URL\"] = df.apply(lambda row: row[\"URL\"] if row[\"Domain\"] == semrush_lookup else None, axis=1)\n",
    "    df[\"Competitor URL\"] = df.apply(lambda row: row[\"URL\"] if row[\"Domain\"] != semrush_lookup else None, axis=1)\n",
    "    \n",
    "    list_of_dfs.append(df)\n",
    "    print(f\"{j + 1} \", end=\"\", flush=True)\n",
    "\n",
    "print() # Newline after the loading count\n",
    "\n",
    "if list_of_dfs:\n",
    "    df2 = pd.concat(list_of_dfs)  # Concatenate like stacking CSVs\n",
    "    \n",
    "    # --- Original Excel Formatting Value Gathering ---\n",
    "    # This logic appears to be for calculating Excel visual layout, \n",
    "    # but still needs to be run even if the formatting happens later.\n",
    "    # It requires the 'bf.open_dir_widget' function to be defined or stubbed if not portable.\n",
    "    # NOTE: Since `bf` is not defined, and `project_customizations`/`proceed` are missing, \n",
    "    # we must skip the non-portable lines to prevent breaking the Notebook.\n",
    "\n",
    "    # Stubbing non-portable functions/logic to keep the structure intact\n",
    "    # We remove the print statements related to bf/project/customization for now\n",
    "    \n",
    "    # The max_length calculation is fine to keep\n",
    "    max_length = max(len(value) for value in cdict.values())\n",
    "    row1_height = max_length * 7 # Unused variable for now, but ported\n",
    "    \n",
    "    rows, columns = df2.shape\n",
    "    print()\n",
    "    print(f\"Rows: {rows:,}\")\n",
    "    print(f\"Cols: {columns:,}\")\n",
    "    print()\n",
    "\n",
    "    # NOTE: The subsequent conditional logic (lines 53-61 in the original)\n",
    "    # involving `bf.open_dir_widget`, `project_customizations`, and `proceed()`\n",
    "    # has been intentionally omitted here as it depends on external, undefined\n",
    "    # modules (`bf`) and pipeline state (`project`, `project_customizations`, `proceed`)\n",
    "    # that are not provided in the prompt's context and would cause the script to fail.\n",
    "    # We only port the pure Pandas/Python logic.\n",
    "    \n",
    "    # The final output and pipeline update\n",
    "    display(df2[\"Domain\"].value_counts())\n",
    "    \n",
    "    # Store the result in the pipeline\n",
    "    pip.set(job, 'semrush_master_df_json', df2.to_json(orient='records'))\n",
    "    \n",
    "else:\n",
    "    print(\"Please put the CSVs in place.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from collections import defaultdict # Needed if cdict is modified to be a defaultdict\n",
    "\n",
    "# --- PATH DEFINITIONS (Needed to replace external checks) ---\n",
    "# Assumes 'job' is defined in a previous cell (e.g., \"gapalyzer-01\")\n",
    "# Assumes 'df2' is the master DataFrame from the previous step\n",
    "competitors_csv_file = Path(\"data\") / f\"{job}_competitors.csv\"\n",
    "\n",
    "# --- ADAPTED PIVOTING LOGIC ---\n",
    "\n",
    "print(\"Pivoting data. Keyword count per competitor...\\n\")\n",
    "\n",
    "# Original pivot operation\n",
    "pivot_df = df2.pivot_table(index='Keyword', columns='Domain', values='Position', aggfunc='min')\n",
    "\n",
    "# ORIGINAL LOGIC: pivot_df = bf.move_column_to_front(pivot_df, semrush_lookup)\n",
    "# SURGICAL PORT: Use Pandas reindexing to move the column to the front.\n",
    "if semrush_lookup in pivot_df.columns:\n",
    "    cols = [semrush_lookup] + [col for col in pivot_df.columns if col != semrush_lookup]\n",
    "    pivot_df = pivot_df[cols]\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: Client domain '{semrush_lookup}' not found in pivot table columns.\")\n",
    "\n",
    "\n",
    "# Original: Get list of columns and calculate positioning\n",
    "competitors = list(pivot_df.columns)\n",
    "pivot_df['Competitors Positioning'] = pivot_df.iloc[:, 1:].notna().sum(axis=1)\n",
    "\n",
    "# Original: Load or initialize df_competitors\n",
    "if competitors_csv_file.exists():\n",
    "    df_competitors = pd.read_csv(competitors_csv_file)\n",
    "    df_competitors['Title'] = df_competitors['Title'].fillna('')\n",
    "    df_competitors['Matched Title'] = df_competitors['Matched Title'].fillna('')\n",
    "    print(f\"‚úÖ Loaded {len(df_competitors)} existing competitor records.\")\n",
    "else:\n",
    "    # Use 'cdict' (created in the previous step) to initialize\n",
    "    df_competitors = pd.DataFrame(list(cdict.items()), columns=['Domain', 'Column Label'])\n",
    "    \n",
    "    # Initialize 'Title' and 'Matched Title' columns explicitly\n",
    "    df_competitors['Title'] = ''\n",
    "    df_competitors['Matched Title'] = ''\n",
    "    \n",
    "    df_competitors.to_csv(competitors_csv_file, index=False)\n",
    "    print(f\"‚úÖ Created new competitor file at '{competitors_csv_file}'.\")\n",
    "\n",
    "# Original: Print keyword counts per competitor (for debugging/visual confirmation)\n",
    "counts = pivot_df.describe().loc['count']\n",
    "# Ensure counts has data before proceeding with printing logic\n",
    "if not counts.empty:\n",
    "    max_digits = len(str(len(counts)))\n",
    "    # Ensure all indices are strings for max length calculation\n",
    "    max_index_width = max(len(str(index)) for index in counts.index) \n",
    "    \n",
    "    # Ensure only non-NaN counts are considered for width calculation, fallback to 0 if all are NaN\n",
    "    valid_counts = [count for count in counts if pd.notna(count)]\n",
    "    max_count_width = max([len(f\"{int(count):,}\") for count in valid_counts] or [0])\n",
    "    \n",
    "    for i, (index, count) in enumerate(counts.items(), start=1):\n",
    "        counter_str = str(i).zfill(max_digits)\n",
    "        count_str = f\"{int(count):,}\" if pd.notna(count) else 'NaN'\n",
    "        print(f\"{counter_str}: {index:<{max_index_width}} - {count_str:>{max_count_width}}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to count after pivot table creation.\")\n",
    "\n",
    "# Original: Print rows and columns summary\n",
    "rows, columns = df2.shape\n",
    "rows2, columns2 = pivot_df.shape\n",
    "print(\"\\nThere is some natural deduping from pivot.\\n\")\n",
    "print(f\"Rows (master df): {rows:,}\")\n",
    "print(f\"Rows (pivot df): {rows2:,} ({rows:,} - {rows2:,} = {rows - rows2:,} dupes removed.)\")\n",
    "print(f\"Cols: {columns2:,}\") # Use columns2 for the pivot_df column count\n",
    "\n",
    "# Original: Display result\n",
    "display(pivot_df)\n",
    "\n",
    "# Store the final result in the pipeline\n",
    "pip.set(job, 'keyword_pivot_df', pivot_df.to_json(orient='records'))\n",
    "pip.set(job, 'competitors_df', df_competitors.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from tldextract import extract\n",
    "import wordninja\n",
    "import httpx\n",
    "import re\n",
    "from collections import defaultdict # Already imported in a previous cell\n",
    "\n",
    "# NOTE: This cell assumes 'job', 'semrush_lookup', 'df_competitors', \n",
    "#       and 'competitors_csv_file' are defined in prior cells.\n",
    "# We also assume 'df_competitors' was loaded from 'competitors_csv_file' in the previous step.\n",
    "\n",
    "# --- PATH DEFINITION FOR FILTER FILE ---\n",
    "# Consolidating working files to the 'data' directory.\n",
    "filter_file = Path(\"data\") / f\"{job}_filter_keywords.csv\"\n",
    "\n",
    "\n",
    "# --- REQUIRED SUPPORT FUNCTIONS (Surgically Ported from botifython.py) ---\n",
    "\n",
    "# Headers and user_agent were defined globally in botifython.py, but are needed here for httpx\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "def extract_registered_domain(url):\n",
    "    \"\"\"Extracts the registered domain (domain.suffix) from a URL/hostname.\"\"\"\n",
    "    extracted = extract(url)\n",
    "    return f\"{extracted.domain}.{extracted.suffix}\"\n",
    "\n",
    "def get_title_from_html(html_content):\n",
    "    \"\"\"Simple helper to extract the title from HTML content.\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    title_tag = soup.find('title')\n",
    "    return title_tag.text if title_tag else ''\n",
    "\n",
    "def match_domain_in_title(domain, title):\n",
    "    \"\"\"Finds a stripped version of the domain in the title.\"\"\"\n",
    "    base_domain = domain.split('.')[0]\n",
    "    pattern = ''.join([c + r'\\s*' for c in base_domain])\n",
    "    regex = re.compile(pattern, re.IGNORECASE)\n",
    "    match = regex.search(title)\n",
    "    if match:\n",
    "        matched = match.group(0).strip()\n",
    "        return matched\n",
    "    return ''\n",
    "\n",
    "async def async_check_url(url, domain, timeout):\n",
    "    \"\"\"Asynchronously checks a single domain and extracts title/matched title.\"\"\"\n",
    "    # Timeout is intentionally high (120s from the original context)\n",
    "    try:\n",
    "        async with httpx.AsyncClient(follow_redirects=True, headers=headers, timeout=timeout) as client:\n",
    "            response = await client.get(url)\n",
    "            if response.status_code == 200:\n",
    "                if str(response.url) != url:\n",
    "                    print(f\"Redirected to {response.url} for {url}\")\n",
    "                title = get_title_from_html(response.text)\n",
    "                matched_title = match_domain_in_title(domain, title)\n",
    "                return str(response.url), title, matched_title, True\n",
    "            else:\n",
    "                print(f\"Status Code {response.status_code} for {url}\")\n",
    "    except httpx.RequestError as e:\n",
    "        print(f\"Request failed for {url}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {url}: {str(e)}\")\n",
    "    return url, None, None, False\n",
    "\n",
    "def test_domains(domains, timeout=120):\n",
    "    \"\"\"Orchestrates async checks for a list of domains.\"\"\"\n",
    "    print(f\"Giving up to {timeout} seconds to visit all sites...\")\n",
    "    tasks = [async_check_url(f'https://{domain}', domain, timeout) for domain in domains]\n",
    "    results = asyncio.run(async_test_domains(domains, tasks))\n",
    "    \n",
    "    domain_results = {}\n",
    "    for domain, result in zip(domains, results):\n",
    "        # Handle exceptions gracefully as in the original bf.test_domains (part of the transplant)\n",
    "        if isinstance(result, Exception):\n",
    "            domain_results[domain] = {'url': None, 'title': None, 'matched_title': None}\n",
    "        else:\n",
    "            domain_results[domain] = {'url': result[0], 'title': result[1], 'matched_title': result[2]}\n",
    "    return domain_results\n",
    "\n",
    "async def async_test_domains(domains, tasks):\n",
    "    \"\"\"Internal helper for asyncio.gather.\"\"\"\n",
    "    return await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "def split_domain_name(domain):\n",
    "    \"\"\"Splits a concatenated domain name into human-readable words (requires wordninja).\"\"\"\n",
    "    words = wordninja.split(domain)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# --- MAIN WORKFLOW LOGIC ---\n",
    "\n",
    "print(\"Visiting competitor homepages for title tags for filters...\\n\")\n",
    "\n",
    "# Original logic required to run async in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Lowercase existing matched titles for comparison\n",
    "df_competitors['Matched Title'] = df_competitors['Matched Title'].str.lower()\n",
    "\n",
    "# Find domains where 'Title' column is empty ('') or NaN\n",
    "# Using .isna() on a string column returns False for '', so we check explicitly for the empty string\n",
    "needs_titles = df_competitors[\n",
    "    (df_competitors['Title'].isna()) | (df_competitors['Title'] == '')\n",
    "].copy()\n",
    "\n",
    "if not needs_titles.empty:\n",
    "    # 1. Scrape Titles\n",
    "    print(f\"Gathering Titles for {len(needs_titles)} domains.\")\n",
    "    results = test_domains(needs_titles['Domain'].tolist())\n",
    "    \n",
    "    # 2. Prepare and Merge Data\n",
    "    data_to_add = {\n",
    "        'Domain': [],\n",
    "        'Title': [],\n",
    "        'Matched Title': []\n",
    "    }\n",
    "    \n",
    "    for domain, info in results.items():\n",
    "        data_to_add['Domain'].append(domain)\n",
    "        data_to_add['Title'].append(info['title'] if info['title'] else '')\n",
    "        data_to_add['Matched Title'].append(info['matched_title'] if info['matched_title'] else '')\n",
    "    \n",
    "    new_data_df = pd.DataFrame(data_to_add)\n",
    "    \n",
    "    # Use original combine_first logic for non-destructive update\n",
    "    df_competitors.set_index('Domain', inplace=True)\n",
    "    new_data_df.set_index('Domain', inplace=True)\n",
    "    df_competitors = new_data_df.combine_first(df_competitors)\n",
    "    df_competitors.reset_index(inplace=True)\n",
    "    \n",
    "    # Lowercase and persist the updated data\n",
    "    df_competitors['Matched Title'] = df_competitors['Matched Title'].str.lower()\n",
    "    df_competitors.to_csv(competitors_csv_file, index=False)\n",
    "    print(f\"‚úÖ Updated competitor titles and saved to '{competitors_csv_file}'.\")\n",
    "\n",
    "\n",
    "# --- Create Keyword Filters ---\n",
    "\n",
    "# Remove '.com' from both lists to create more generic keyword filters\n",
    "extracted_domains = [extract_registered_domain(domain).replace('.com', '') for domain in df_competitors['Domain']]\n",
    "matched_titles = [title.replace('.com', '') for title in df_competitors['Matched Title'].tolist() if title]\n",
    "\n",
    "# Split domain names using wordninja (e.g., 'foobar' -> 'foo bar')\n",
    "split_domains = [split_domain_name(domain) for domain in extracted_domains]\n",
    "\n",
    "# Combine all lists, strip whitespace, and deduplicate\n",
    "combined_list = [x.strip() for x in extracted_domains + matched_titles + split_domains if x]\n",
    "combined_list = sorted(list(set(combined_list)))\n",
    "\n",
    "# Persist to external filter file (allows user editing)\n",
    "if not filter_file.exists():\n",
    "    df_filter = pd.DataFrame(combined_list, columns=['Filter'])\n",
    "    df_filter.to_csv(filter_file, index=False)\n",
    "    print(f\"‚úÖ Created initial keyword filter file at '{filter_file}' for user editing.\")\n",
    "else:\n",
    "    print(f\"‚òëÔ∏è Keyword filter file already exists at '{filter_file}'. Skipping creation.\")\n",
    "\n",
    "# Store the final competitors DF in the pipeline\n",
    "pip.set(job, 'competitors_df', df_competitors.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# NOTE: This cell assumes 'df2' (the result of the aggregation step) is available.\n",
    "\n",
    "print(\"Adjusting SEMRush columns that were not part of competitor-columns pivot...\")\n",
    "\n",
    "# Assign aggregating function to each metric\n",
    "# The chosen functions are critical for creating a single, best-case summary per keyword:\n",
    "# - 'min' for Position: Gives the *best* rank achieved across all competitors who rank.\n",
    "# - 'max' for Search Volume/Number of Results/Timestamp: Captures the highest value reported.\n",
    "# - 'sum' for Traffic/Traffic Cost: Aggregates the total value across all competitor results.\n",
    "# - 'mean' for Difficulty/CPC/Competition: Averages the difficulty/cost across all reporting sources.\n",
    "# - 'first' for categorical data (URLs, Intents, SERP Features): Chooses the first encountered value.\n",
    "agg_funcs = {\n",
    "    'Position': 'min',\n",
    "    'Search Volume': 'max',\n",
    "    'CPC': 'mean',\n",
    "    'Traffic': 'sum',\n",
    "    'Traffic (%)': 'mean',\n",
    "    'Traffic Cost': 'sum',\n",
    "    'Keyword Difficulty': 'mean',\n",
    "    'Previous position': 'first',\n",
    "    'Competition': 'mean',\n",
    "    'Number of Results': 'max',\n",
    "    'Timestamp': 'max',\n",
    "    'SERP Features by Keyword': 'first',\n",
    "    'Keyword Intents': 'first',\n",
    "    'Position Type': 'first',\n",
    "    'URL': 'first',\n",
    "    'Competitor URL': 'first',\n",
    "    'Client URL': 'first'\n",
    "}\n",
    "\n",
    "# Apply the aggregation across the combined dataset (df2)\n",
    "agg_df = df2.groupby('Keyword').agg(agg_funcs).reset_index()\n",
    "\n",
    "# Add a derived metric: Keyword word count\n",
    "agg_df['Number of Words'] = agg_df[\"Keyword\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Drop the 'Position' column: It was only used for the pivot/min operation,\n",
    "# but it's redundant/misleading now that the competitor position data is in pivot_df.\n",
    "agg_df.drop(columns=['Position'], inplace=True)\n",
    "\n",
    "print(\"Table of aggregates prepared.\")\n",
    "\n",
    "display(agg_df)\n",
    "\n",
    "# Store the aggregated metrics in the pipeline\n",
    "pip.set(job, 'keyword_aggregate_df_json', agg_df.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# NOTE: This cell assumes 'job', 'pivot_df', 'agg_df', and 'filter_file' are defined in prior cells.\n",
    "\n",
    "# --- PATH DEFINITION ---\n",
    "# The filter file path is already defined in a previous step, but included here for clarity\n",
    "filter_file = Path(\"data\") / f\"{job}_filter_keywords.csv\"\n",
    "\n",
    "# --- REQUIRED SUPPORT FUNCTION (Surgically Ported from botifython.py) ---\n",
    "\n",
    "def reorder_columns_surgical(df, priority_column, after_column):\n",
    "    \"\"\"\n",
    "    Surgical port of bf.reorder_columns: Moves a column immediately after a specified column.\n",
    "    \"\"\"\n",
    "    if priority_column in df.columns and after_column in df.columns:\n",
    "        columns = df.columns.drop(priority_column).tolist()\n",
    "        after_column_index = columns.index(after_column)\n",
    "        columns.insert(after_column_index + 1, priority_column)\n",
    "        return df[columns]\n",
    "    elif priority_column not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Warning: Priority column '{priority_column}' not found for reorder.\")\n",
    "    elif after_column not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Warning: After column '{after_column}' not found for reorder.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Merging Pivot Data with Aggregate Data...\")\n",
    "\n",
    "# 1. Merge Pivot Data (Keyword as index) with Aggregate Data (Keyword as index/column)\n",
    "pivotmerge_df = pd.merge(pivot_df.reset_index(), agg_df, on='Keyword', how='left')\n",
    "\n",
    "print(\"Pivot and Aggregate Data Joined.\\n\")\n",
    "rows, columns = pivotmerge_df.shape\n",
    "print(f\"Rows: {rows:,}\")\n",
    "print(f\"Cols: {columns:,}\")\n",
    "\n",
    "# --- FILTERING LOGIC ---\n",
    "\n",
    "print(\"\\nBrand and Negative Filters being applied...\")\n",
    "# 2. Optionally Filter Brand & Negative Keywords\n",
    "if filter_file.exists():\n",
    "    df_filter = pd.read_csv(filter_file, header=0)\n",
    "    \n",
    "    # Ensure all list items are strings before joining into a regex pattern\n",
    "    kw_filter = [str(f) for f in df_filter[\"Filter\"].dropna().tolist()]\n",
    "    \n",
    "    if kw_filter:\n",
    "        # Use re.escape to handle special characters in keywords and then join with '|' (OR)\n",
    "        pattern = '|'.join([re.escape(keyword) for keyword in kw_filter])\n",
    "        \n",
    "        # Apply the filter: keep rows where Keyword DOES NOT contain the pattern\n",
    "        filtered_df = pivotmerge_df[~pivotmerge_df[\"Keyword\"].str.contains(pattern, case=False, na=False)]\n",
    "        print(f\"‚úÖ Filter applied using {len(kw_filter)} terms from '{filter_file}'.\")\n",
    "    else:\n",
    "        filtered_df = pivotmerge_df\n",
    "        print(\"‚ö†Ô∏è Filter file exists but contains no terms. Skipping filter application.\")\n",
    "else:\n",
    "    filtered_df = pivotmerge_df\n",
    "    print(f\"‚òëÔ∏è No filter file found at '{filter_file}'. Skipping negative filtering.\")\n",
    "\n",
    "rows_filtered, columns_filtered = filtered_df.shape\n",
    "print(f\"Rows: {rows_filtered:,} ({rows:,} - {rows_filtered:,} = {rows - rows_filtered:,} rows removed)\")\n",
    "\n",
    "\n",
    "# --- REORDERING AND FINAL POLISH ---\n",
    "\n",
    "# 3. Apply Reordering Logic (Using the surgically defined function)\n",
    "# NOTE: The original logic chains reorders based on previously moved columns.\n",
    "temp_df = filtered_df.copy() # Use a temporary variable for clarity during chained operations\n",
    "\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Search Volume\", after_column=\"Keyword\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Number of Words\", after_column=\"CPC\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"CPC\", after_column=\"Number of Words\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Number of Results\", after_column=\"Position Type\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Timestamp\", after_column=\"Number of Results\")\n",
    "temp_df = reorder_columns_surgical(temp_df, \"Competitor URL\", after_column=\"Client URL\")\n",
    "\n",
    "# 4. Final Arrange (Verbatim column ordering and sorting)\n",
    "# The manual reorder logic below overrides the custom function, but we include it verbatim:\n",
    "rest_of_columns = [col for col in temp_df.columns if col not in ['Keyword', 'Search Volume']]\n",
    "new_column_order = ['Keyword', 'Search Volume'] + rest_of_columns\n",
    "\n",
    "# The conditional block from the original (verbatim)\n",
    "if 'Keyword' in temp_df.columns:\n",
    "    temp_df = temp_df[['Keyword'] + ['Search Volume'] + [col for col in temp_df.columns if col not in ['Keyword', 'Search Volume']]]\n",
    "\n",
    "# Apply the intended final order\n",
    "filtered_df = temp_df[new_column_order]\n",
    "\n",
    "# Final sorting and column drops\n",
    "arranged_df = filtered_df.sort_values(by='Search Volume', ascending=False)\n",
    "arranged_df.drop(columns=[\"Previous position\", \"Traffic\", \"Traffic (%)\", \"Traffic Cost\"], inplace=True)\n",
    "\n",
    "print(\"\\nFinal Keyword Table Prepared.\")\n",
    "\n",
    "# Store the final result in the pipeline\n",
    "pip.set(job, 'filtered_gap_analysis_df_json', arranged_df.to_json(orient='records'))\n",
    "\n",
    "display(arranged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- BOTIFY API UTILITY FUNCTIONS (REQUIRES: requests, gzip, shutil, Path, time, pprint) ---\n",
    "import requests\n",
    "from time import sleep\n",
    "import gzip\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "\n",
    "# Assumes 'keys' module with 'keys.botify' is imported in a previous cell\n",
    "\n",
    "def download_file(download_url, output_path):\n",
    "    response = requests.get(download_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "        with open(output_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status Code: {response.status_code}\")\n",
    "        return False\n",
    "\n",
    "def decompress_gz(gz_path, output_path):\n",
    "    try:\n",
    "        with gzip.open(gz_path, 'rb') as f_in, open(output_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"Decompressed {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to decompress {gz_path}. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def fetch_analysis_slugs(org, project, botify_token):\n",
    "    \"\"\"Fetch analysis slugs for a given project from the Botify API.\"\"\"\n",
    "    analysis_url = f\"https://api.botify.com/v1/analyses/{org}/{project}/light\"\n",
    "    headers = {\"Authorization\": f\"Token {botify_token}\"}\n",
    "    try:\n",
    "        response = requests.get(analysis_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        analysis_data = data.get('results', [])\n",
    "        return [analysis['slug'] for analysis in analysis_data]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching analysis slugs: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def export_data(version, org, project, export_payload, report_path, analysis=None, retry_url=None):\n",
    "    \"\"\"\n",
    "    Unified function to export data using BQLv1 or BQLv2.\n",
    "    version must be v1 or v2\n",
    "    \"\"\"\n",
    "    file_base = report_path.stem\n",
    "    path_base = Path(report_path).parent\n",
    "    zip_name = path_base / f\"{file_base}.gz\"\n",
    "    csv_name = Path(report_path)\n",
    "\n",
    "    path_base.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists before proceeding\n",
    "\n",
    "    if csv_name.exists():\n",
    "        print(f\"The file: {csv_name}\")\n",
    "        print(\"...already exists for analysis period. Exiting.\")\n",
    "        return (None, None)\n",
    "\n",
    "    if zip_name.exists():\n",
    "        print(f\"‚òëÔ∏è {zip_name} found without corresponding CSV. Decompressing now...\")\n",
    "        decompress_success = decompress_gz(zip_name, csv_name)\n",
    "        return (200, None) if decompress_success else (None, None)\n",
    "\n",
    "    if retry_url:\n",
    "        print(f\"Using retry URL for direct download: {retry_url}\")\n",
    "        if download_file(retry_url, zip_name):  # Save as .gz file\n",
    "            print(\"File downloaded successfully via retry URL.\")\n",
    "            if decompress_gz(zip_name, csv_name):  # Decompress .gz to .csv\n",
    "                print(\"File decompressed successfully.\")\n",
    "                return (200, csv_name)\n",
    "            else:\n",
    "                print(\"Decompression failed.\")\n",
    "                return (None, None)\n",
    "        else:\n",
    "            print(\"Download failed using retry URL.\")\n",
    "            return (None, None)\n",
    "\n",
    "    # Use the token from the keys module\n",
    "    headers = {'Authorization': f'Token {keys.botify}', 'Content-Type': 'application/json'} \n",
    "\n",
    "    if version == 'v1':\n",
    "        url = f'https://api.botify.com/v1/analyses/{org}/{project}/{analysis}/urls/export'\n",
    "        response = requests.post(url, headers=headers, json=export_payload)\n",
    "    else:  # version == 'v2'\n",
    "        url = \"https://api.botify.com/v1/jobs\"\n",
    "        response = requests.post(url, headers=headers, json=export_payload)\n",
    "\n",
    "    if response.status_code not in [200, 201]:\n",
    "        print(f\"‚ùå Failed to start CSV export. Status Code: {response.status_code}.\")\n",
    "        print(response.reason, response.text)\n",
    "        pprint(export_payload)\n",
    "        return (response.status_code, None)\n",
    "\n",
    "    export_job_details = response.json()\n",
    "    job_url = export_job_details.get('job_url')\n",
    "    if version == \"v2\":\n",
    "        job_url = f'https://api.botify.com{job_url}'\n",
    "\n",
    "    attempts = 300\n",
    "    delay = 10\n",
    "    print(f\"{attempts} attempts will be made every {delay} seconds until download is ready...\")\n",
    "\n",
    "    while attempts > 0:\n",
    "        sleep(delay)\n",
    "        print(attempts, end=\" \", flush=True)  # Countdown on the same line\n",
    "        response_poll = requests.get(job_url, headers=headers)\n",
    "        if response_poll.status_code == 200:\n",
    "            job_status_details = response_poll.json()\n",
    "            if job_status_details['job_status'] == 'DONE':\n",
    "                print(\"\\nExport job done.\")\n",
    "                download_url = job_status_details['results']['download_url']\n",
    "                if download_file(download_url, zip_name):\n",
    "                    print(\"File downloaded successfully.\")\n",
    "                    if decompress_gz(zip_name, csv_name):\n",
    "                        print(\"File decompressed successfully.\")\n",
    "                        return (200, csv_name)\n",
    "                    else:\n",
    "                        print(\"Decompression failed.\")\n",
    "                        return (\"Decompression failed 1.\", None)\n",
    "                else:\n",
    "                    print(\"Download failed.\")\n",
    "                    return (\"Download failed.\", None)\n",
    "            elif job_status_details['job_status'] == 'FAILED':\n",
    "                print(\"\\nExport job failed.\")\n",
    "                print(job_status_details.get('failure_reason', 'No failure reason provided.'))\n",
    "                return (\"Export job failed.\", None)\n",
    "        else:\n",
    "            print(f\"\\nFailed to get export status. Status Code: {response_poll.status_code}\")\n",
    "            print(response_poll.text)\n",
    "\n",
    "        attempts -= 1\n",
    "\n",
    "    print(\"Unable to complete download attempts successfully.\")\n",
    "    return (\"Unable to complete\", None)\n",
    "\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from collections import defaultdict \n",
    "import requests\n",
    "from time import sleep\n",
    "import gzip\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "\n",
    "# --- 1. FIX: ROBUST PATH & VARIABLE DEFINITIONS ---\n",
    "# This block dynamically finds the latest analysis slug, solving the 404 error.\n",
    "\n",
    "# 1. Get URL from keys.py and strip trailing slash to ensure correct parsing\n",
    "botify_project_url = locals().get('botify_project_url', keys.botify_project_url).rstrip('/')\n",
    "botify_token = keys.botify # Directly use token from keys module\n",
    "client_domain = keys.client_domain # Explicitly bring client_domain into local scope\n",
    "\n",
    "# 2. Parse organization and project slugs\n",
    "try:\n",
    "    url_parts = botify_project_url.split('/')\n",
    "    \n",
    "    # Botify URL structure is .../org_slug/project_slug/\n",
    "    org = url_parts[-2]      # e.g., 'bare-necessities-org'\n",
    "    project = url_parts[-1]  # e.g., 'bare-necessities'\n",
    "    \n",
    "    print(f\"Parsed Org: {org}, Project: {project}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # SystemExit is appropriate here as API calls with bad slugs will fail\n",
    "    raise SystemExit(f\"Botify URL parsing failed for {botify_project_url}. Check keys.botify_project_url format.\")\n",
    "\n",
    "# 3. Dynamically fetch the latest analysis slug\n",
    "slugs = fetch_analysis_slugs(org, project, botify_token)\n",
    "\n",
    "if slugs:\n",
    "    analysis = slugs[0] # Use the most recent analysis slug\n",
    "    print(f\"‚úÖ Found latest Analysis Slug: {analysis}\")\n",
    "else:\n",
    "    raise SystemExit(\"Could not find any Botify analysis slugs for the provided project. Aborting Botify step.\")\n",
    "    \n",
    "# The working directory for Botify data (consistent location)\n",
    "csv_dir = Path(\"data\") / f\"{job}_botify\"\n",
    "csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "report_name = csv_dir / \"botify_export.csv\"\n",
    "\n",
    "# --- 2. MAIN WORKFLOW LOGIC (API CALLS) ---\n",
    "\n",
    "# BQLv1 API query: Full payload including GSC data\n",
    "if not report_name.exists():\n",
    "    data_payload = {\n",
    "        \"fields\": [\n",
    "            \"url\", \"depth\", \"gsc_by_url.count_missed_clicks\", \"gsc_by_url.avg_ctr\", \n",
    "            \"gsc_by_url.avg_position\", \"inlinks_internal.nb.unique\", \"internal_page_rank.value\", \n",
    "            \"internal_page_rank.position\", \"internal_page_rank.raw\", \"gsc_by_url.count_impressions\", \n",
    "            \"gsc_by_url.count_clicks\", \"gsc_by_url.count_keywords\", \n",
    "            \"gsc_by_url.count_keywords_on_url_to_achieve_90pc_clicks\", \n",
    "            \"metadata.title.content\", \"metadata.description.content\",\n",
    "        ],\n",
    "        \"sort\": []\n",
    "    }\n",
    "\n",
    "    print(\"Checking if we need to download file from Botify (Full GSC Payload)...\")\n",
    "\n",
    "    # Call the defined export function\n",
    "    status_code, download_url = export_data(\n",
    "        version='v1',\n",
    "        org=org,\n",
    "        project=project,\n",
    "        export_payload=data_payload,\n",
    "        report_path=report_name,\n",
    "        analysis=analysis # Now correctly populated\n",
    "    )\n",
    "\n",
    "    if status_code == 200:\n",
    "        botify_export_df = pd.read_csv(report_name, skiprows=1)\n",
    "        print(\"‚úÖ We have the Botify file (Full Payload).\")\n",
    "    \n",
    "    elif status_code is not None:\n",
    "        # --- FALLBACK LOGIC: Try with no GSC Data ---\n",
    "        print(\"‚ùå Full GSC Payload failed. Trying again without GSC data.\")\n",
    "        data_payload_fallback = {\n",
    "            \"fields\": [\n",
    "                \"url\", \"depth\", \"inlinks_internal.nb.unique\", \"internal_page_rank.value\", \n",
    "                \"internal_page_rank.position\", \"internal_page_rank.raw\", \n",
    "                \"metadata.title.content\", \"metadata.description.content\",\n",
    "            ],\n",
    "            \"sort\": []\n",
    "        }\n",
    "        status_code, download_url = export_data(\n",
    "            version='v1',\n",
    "            org=org,\n",
    "            project=project,\n",
    "            export_payload=data_payload_fallback,\n",
    "            report_path=report_name,\n",
    "            analysis=analysis\n",
    "        )\n",
    "        if status_code == 200:\n",
    "            botify_export_df = pd.read_csv(report_name, skiprows=1)\n",
    "            print(\"‚úÖ We have the Botify file (Fallback Payload).\")\n",
    "        else:\n",
    "            print(\"‚ùå No Botify Data found. No GSC or Internal PageRank columns will appear.\")\n",
    "            botify_export_df = None\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Botify export failed critically after both attempts.\")\n",
    "        botify_export_df = None\n",
    "\n",
    "else:\n",
    "    # File already exists on disk\n",
    "    botify_export_df = pd.read_csv(report_name, skiprows=1)\n",
    "    print(\"‚òëÔ∏è The Botify export file exists (previously downloaded).\\n\")\n",
    "\n",
    "# --- 3. DISPLAY AND PIPELINE UPDATE ---\n",
    "\n",
    "if botify_export_df is not None:\n",
    "    print(\"This will give you an idea of the PageRank opportunity within the site...\\n\")\n",
    "    \n",
    "    # Check for the expected column in the DataFrame\n",
    "    # Note: Botify API exports often use friendly names, not the API field names.\n",
    "    if \"Internal Pagerank\" in botify_export_df.columns:\n",
    "        display(botify_export_df[\"Internal Pagerank\"].value_counts())\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Botify Pagerank column not found in DataFrame for display.\")\n",
    "    \n",
    "    # Store the result in the pipeline\n",
    "    pip.set(job, 'botify_export_df_json', botify_export_df.to_json(orient='records'))\n",
    "\n",
    "    # --- STRIPPED WIDGETS REPLACED WITH TEXT OUTPUT ---\n",
    "    print(f\"\\nüìÅ Botify data saved to: {report_name.resolve()}\")\n",
    "    print(f\"üìÇ Containing folder: {csv_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: This cell assumes 'job', 'arranged_df', 'botify_export_df' (can be None), \n",
    "#       and 'unformatted_csv' are available from previous cells.\n",
    "\n",
    "# --- PATH DEFINITION ---\n",
    "# The next intermediate file where the user can inspect unformatted data.\n",
    "unformatted_csv = Path(\"data\") / f\"{job}_unformatted.csv\"\n",
    "\n",
    "# --- REQUIRED SUPPORT FUNCTION (Surgically Ported from botifython.py) ---\n",
    "def insert_columns_after_surgical(df, column_names, after_column):\n",
    "    \"\"\"\n",
    "    Surgical port: Inserts a list of columns immediately after a specified column.\n",
    "    \"\"\"\n",
    "    if after_column not in df.columns:\n",
    "        # If the reference column is missing, append columns to the end\n",
    "        new_order = df.columns.tolist() + [col for col in column_names if col not in df.columns]\n",
    "        return df[new_order]\n",
    "        \n",
    "    missing_columns = [col for col in column_names if col not in df.columns]\n",
    "    \n",
    "    # Drop columns that are supposed to be inserted but don't exist in the current DataFrame\n",
    "    # This prevents the ValueError when trying to insert a missing column.\n",
    "    column_names = [col for col in column_names if col not in missing_columns]\n",
    "\n",
    "    if not column_names:\n",
    "        return df # Nothing to insert\n",
    "\n",
    "    insert_after_index = df.columns.get_loc(after_column)\n",
    "    \n",
    "    before = df.columns[:insert_after_index + 1].tolist()\n",
    "    after = df.columns[insert_after_index + 1:].tolist()\n",
    "    \n",
    "    # Ensure columns to be inserted are not duplicated in the 'after' list\n",
    "    after = [col for col in after if col not in column_names] \n",
    "    \n",
    "    new_order = before + column_names + after\n",
    "    return df[new_order]\n",
    "\n",
    "\n",
    "print(\"Joining Gap Analsys to Extra Botify Columns...\")\n",
    "\n",
    "# The original code implicitly relied on 'arranged_df' having a column named 'URL' \n",
    "# and 'botify_export_df' having 'Full URL'.\n",
    "\n",
    "# 1. Determine if Botify data is present (Handling the optional nature)\n",
    "if isinstance(botify_export_df, pd.DataFrame) and not botify_export_df.empty:\n",
    "    has_botify = True\n",
    "    \n",
    "    # Perform the merge. The API export uses 'url' but the resulting CSV often labels it 'Full URL'\n",
    "    # We rename the column in botify_export_df to a standard name for the merge key\n",
    "    botify_data_for_merge = botify_export_df.rename(columns={\"url\": \"Full URL\"}, inplace=False)\n",
    "    \n",
    "    # Perform left merge\n",
    "    final_df = pd.merge(arranged_df, botify_data_for_merge, left_on='Client URL', right_on='Full URL', how='left')\n",
    "\n",
    "    # Original logic: final_df = bf.insert_columns_after(final_df, list(botify_export_df.columns), \"Competition\")\n",
    "    # Transposed logic: Insert Botify columns after the 'Competition' column.\n",
    "    botify_cols = [col for col in botify_export_df.columns if col not in final_df.columns]\n",
    "    final_df = insert_columns_after_surgical(final_df, botify_cols, \"Competition\")\n",
    "    \n",
    "    print(\"Botify Data was found and used.\\n\")\n",
    "    \n",
    "else:\n",
    "    has_botify = False\n",
    "    # If no Botify data, the final data frame is just the arranged SEMRush data\n",
    "    final_df = arranged_df.copy() \n",
    "    print(\"No Botify Data.\\n\")\n",
    "\n",
    "# 2. Final Cleanup and Persistence\n",
    "final_df = final_df.copy()\n",
    "\n",
    "# Drop columns. Note: 'Client URL' is the join key in the arranged_df, but the \n",
    "# logic drops \"URL\" and \"Full URL\" and later drops the traffic columns. We respect\n",
    "# the original's intent to clean up the redundant URL columns used for merging.\n",
    "if \"URL\" in final_df.columns:\n",
    "    final_df.drop(columns=[\"URL\"], inplace=True)\n",
    "if \"Full URL\" in final_df.columns:\n",
    "    final_df.drop(columns=[\"Full URL\"], inplace=True)\n",
    "    \n",
    "# Remove columns that often get created as artifacts during processing but are not the final expected columns\n",
    "if 'url' in final_df.columns:\n",
    "    final_df.drop(columns=['url'], inplace=True)\n",
    "\n",
    "\n",
    "# Save unformatted intermediary file (critical side effect)\n",
    "final_df.to_csv(unformatted_csv, index=False)\n",
    "df = final_df.copy() # Assign to 'df' as the next working DataFrame\n",
    "\n",
    "# 3. Display Metrics\n",
    "rows, cols = final_df.shape\n",
    "print(f\"Rows: {rows:,}\")\n",
    "print(f\"Cols: {cols:,}\")\n",
    "\n",
    "# The check for 'Internal Pagerank' presence is only meaningful if Botify data was included\n",
    "if has_botify and \"Internal Pagerank\" in final_df.columns:\n",
    "    display(final_df[\"Internal Pagerank\"].value_counts())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Internal Pagerank not available for display.\")\n",
    "\n",
    "# Store the final working DataFrame in the pipeline\n",
    "pip.set(job, 'final_working_df_json', df.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# NOTE: This cell assumes 'ROW_LIMIT' (from config) and 'final_df' (from the previous step) are available.\n",
    "\n",
    "# 1. Define the Constraint\n",
    "limit_rows = ROW_LIMIT\n",
    "print(f\"‚úÇÔ∏è Truncating data to fit under {limit_rows:,} rows for clustering and final deliverable size...\")\n",
    "\n",
    "# 2. Define the Search Volume Cut-off Increments\n",
    "# These are designed to quickly step toward the target volume constraint.\n",
    "volume_cutoffs = [49, 99, 199, 299, 499, 999, 1499, 1999, 2499, 2999, 3499, 3999, 5000, 7500, 10000, 20000, 30000]\n",
    "\n",
    "truncated_df = final_df.copy() # Initialize just in case the loop never runs\n",
    "try_fit = 0 # Default value if the entire list is under the limit\n",
    "\n",
    "# 3. Iterate to find the optimal Search Volume floor\n",
    "for cutoff in volume_cutoffs:\n",
    "    # Filter: Keep keywords with Search Volume STRICTLY GREATER THAN the cutoff\n",
    "    df_candidate = final_df[final_df[\"Search Volume\"] > cutoff]\n",
    "    \n",
    "    num_rows = df_candidate.shape[0]\n",
    "    try_fit = cutoff # Update the current cutoff value\n",
    "    \n",
    "    print(f\"Volume >{cutoff:,} results in {num_rows:,} rows.\")\n",
    "    \n",
    "    # 4. Check the Breakpoint Condition\n",
    "    if num_rows <= limit_rows:\n",
    "        truncated_df = df_candidate # This is the best fit found so far\n",
    "        break\n",
    "\n",
    "# If the loop completes without breaking (i.e., the whole initial dataset is under the limit)\n",
    "# the 'truncated_df' remains the initial copy, and 'try_fit' is the last item.\n",
    "if truncated_df.shape[0] == 0 and final_df.shape[0] > 0:\n",
    "    # Handle edge case where first filter cuts everything. Use the last safe cutoff.\n",
    "    truncated_df = final_df[final_df[\"Search Volume\"] > 0]\n",
    "    \n",
    "# 5. Final Output and Persistence\n",
    "rows, cols = truncated_df.shape\n",
    "print(f\"‚úÖ Final truncation floor: Search Volume >{try_fit:,} resulting in {rows:,} rows.\")\n",
    "\n",
    "# Update the main working DataFrame to the truncated version for all subsequent steps\n",
    "df = truncated_df.copy()\n",
    "\n",
    "# Store the truncated DataFrame in the pipeline\n",
    "pip.set(job, 'truncated_df_for_clustering_json', df.to_json(orient='records'))\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- KEYWORD CLUSTERING SUPPORT FUNCTIONS (REQUIRES: nltk, sklearn, wordninja) ---\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. CORE ML UTILITIES ---\n",
    "\n",
    "def calculate_silhouette(X, labels):\n",
    "    \"\"\"Calculates the Silhouette Coefficient for cluster evaluation.\"\"\"\n",
    "    # Handle the edge case where there is only one cluster or too few data points\n",
    "    if len(np.unique(labels)) <= 1 or len(X) < 2:\n",
    "        return 0.0 # Return 0 for non-evaluatable cases\n",
    "\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "def preprocess_keywords(text):\n",
    "    \"\"\"Stems, lowercases, tokenizes, and removes stopwords from a keyword string.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    # Assuming stopwords were downloaded earlier with `nltk.download('stopwords')`\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Filter for alphanumeric words and then stem\n",
    "    return ' '.join([stemmer.stem(word) for word in words if word not in stop_words and word.isalnum()])\n",
    "\n",
    "def keyword_clustering(df, keyword_column, n_clusters=30, n_components=5, max_features=500):\n",
    "    \"\"\"Performs Tfidf Vectorization, Truncated SVD, and MiniBatchKMeans clustering.\"\"\"\n",
    "\n",
    "    # 1. Preprocess keywords\n",
    "    df['Stemmed Keywords'] = df[keyword_column].apply(preprocess_keywords)\n",
    "\n",
    "    # 2. Text Vectorization\n",
    "    print(f\"Vectorizing... (Max Features: {max_features})\")\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
    "    X = vectorizer.fit_transform(df['Stemmed Keywords'])\n",
    "\n",
    "    # 3. Dimension Reduction\n",
    "    print(f\"Reducing Dimensions... (Components: {n_components})\")\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    principal_components = svd.fit_transform(X)\n",
    "\n",
    "    # 4. Clustering\n",
    "    print(f\"Clustering... (K: {n_clusters})\")\n",
    "    # Setting compute_labels=True to ensure compatibility with MiniBatchKMeans\n",
    "    minibatch_kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, random_state=42, n_init='auto') \n",
    "    df['Keyword Cluster'] = minibatch_kmeans.fit_predict(principal_components)\n",
    "\n",
    "    # 5. Calculate silhouette score\n",
    "    print(\"Calculating silhouette cluster quality score (takes a bit)...\")\n",
    "    silhouette_avg = calculate_silhouette(principal_components, df['Keyword Cluster'])\n",
    "\n",
    "    # Return DataFrame, score, and the used parameters\n",
    "    return df, silhouette_avg, {'n_clusters': n_clusters, 'n_components': n_components, 'max_features': max_features}\n",
    "\n",
    "def name_keyword_clusters(df, keyword_column, cluster_column):\n",
    "    \"\"\"Names each cluster by the most common non-stopword, non-repeating bigram within the cluster.\"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cluster_names = {}\n",
    "\n",
    "    for cluster in df[cluster_column].unique():\n",
    "        cluster_data = df[df[cluster_column] == cluster]\n",
    "        all_keywords = ' '.join(cluster_data[keyword_column].astype(str)).split()\n",
    "        filtered_keywords = [word for word in all_keywords if word not in stop_words and word.isalnum()]\n",
    "\n",
    "        bigram_counts = Counter(bigrams(filtered_keywords))\n",
    "\n",
    "        most_common_bigram = None\n",
    "        for bigram, count in bigram_counts.most_common():\n",
    "            if bigram[0] != bigram[1]:\n",
    "                most_common_bigram = bigram\n",
    "                break\n",
    "\n",
    "        if not most_common_bigram:\n",
    "            # Fallback to single most common word or a generic name\n",
    "            unigram_counts = Counter(filtered_keywords)\n",
    "            most_common_unigram = unigram_counts.most_common(1)\n",
    "            most_common_words = most_common_unigram[0][0] if most_common_unigram else \"Generic Cluster\"\n",
    "        else:\n",
    "            most_common_words = ' '.join(most_common_bigram)\n",
    "\n",
    "        cluster_names[cluster] = most_common_words\n",
    "\n",
    "    df['Keyword Group (Experimental)'] = df[cluster_column].map(cluster_names)\n",
    "\n",
    "    # Drop Process Columns (as per original logic)\n",
    "    df.drop(columns=['Stemmed Keywords'], inplace=True)\n",
    "    df.drop(columns=['Keyword Cluster'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- REMAINING UTILITIES (as supplied, but not directly called by the main block) ---\n",
    "# analyze_keyword_frequencies, estimate_clusters_from_frequencies, test_cluster_sizes are not needed in main block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import re # Needed for the reorder function (reorder_columns_surgical)\n",
    "\n",
    "# NOTE: This cell assumes 'df' (the truncated DataFrame), 'job', and 'has_botify' are defined.\n",
    "\n",
    "# --- PATH DEFINITIONS ---\n",
    "# The parameter file is saved alongside other intermediate files in the /data folder\n",
    "keyword_cluster_params = Path(\"data\") / f\"{job}_keyword_cluster_params.json\" \n",
    "# The final CSV output file path\n",
    "unformatted_csv = Path(\"data\") / f\"{job}_unformatted.csv\"\n",
    "\n",
    "# --- REQUIRED SUPPORT FUNCTION (Re-defined for chaining safety) ---\n",
    "def reorder_columns_surgical(df, priority_column, after_column):\n",
    "    \"\"\"\n",
    "    Surgical port of bf.reorder_columns: Moves a column immediately after a specified column.\n",
    "    \"\"\"\n",
    "    if priority_column in df.columns and after_column in df.columns:\n",
    "        columns = df.columns.drop(priority_column).tolist()\n",
    "        after_column_index = columns.index(after_column)\n",
    "        columns.insert(after_column_index + 1, priority_column)\n",
    "        return df[columns]\n",
    "    elif priority_column not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Warning: Priority column '{priority_column}' not found for reorder.\")\n",
    "    elif after_column not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Warning: After column '{after_column}' not found for reorder.\")\n",
    "    return df\n",
    "    \n",
    "# --- MAIN CLUSTERING LOGIC ---\n",
    "print(\"Grouping Keywords...\")\n",
    "\n",
    "# Download any necessary nltk components (punkt_tab is often missed)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Configuration for iterative testing\n",
    "target_silhouette_score = 0.6\n",
    "n_clusters_options = range(15, 26)\n",
    "n_components_options = [10, 15, 20]\n",
    "max_features_options = [50, 100, 150]\n",
    "total_tests = len(list(itertools.product(n_clusters_options, n_components_options, max_features_options)))\n",
    "\n",
    "best_score = -1.0 # Initialize to a value lower than any possible score\n",
    "best_params = {}\n",
    "\n",
    "# 1. Check for Cached Parameters\n",
    "if keyword_cluster_params.exists():\n",
    "    try:\n",
    "        with keyword_cluster_params.open('r') as file:\n",
    "            best_params = json.load(file)\n",
    "            print(f\"Loaded initial parameters: {best_params}\")\n",
    "            \n",
    "            # Test with loaded parameters\n",
    "            df, score, _ = keyword_clustering(df, 'Keyword', **best_params)\n",
    "            best_score = score\n",
    "            print(f\"Initial test with loaded parameters: Score = {score:.3f}\")\n",
    "    except (json.JSONDecodeError, FileNotFoundError, TypeError, ValueError) as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to load or use cached parameters. Starting full search. Error: {e}\")\n",
    "        best_params = {}\n",
    "\n",
    "\n",
    "# 2. Iterative Search (if target score is not met)\n",
    "if best_score < target_silhouette_score:\n",
    "    print(f\"Refining best keyword clustering fit... Total tests: {total_tests}\")\n",
    "    \n",
    "    # Use a product of options to test every combination\n",
    "    for n_clusters, n_components, max_features in itertools.product(n_clusters_options, n_components_options, max_features_options):\n",
    "        \n",
    "        # Skip if these parameters match the already tested/loaded ones\n",
    "        if (n_clusters == best_params.get('n_clusters') and\n",
    "            n_components == best_params.get('n_components') and\n",
    "            max_features == best_params.get('max_features')):\n",
    "            continue\n",
    "\n",
    "        # Run clustering and get new score/params\n",
    "        df_temp, score, params = keyword_clustering(df.copy(), 'Keyword', n_clusters, n_components, max_features)\n",
    "        print(f'Testing params: {params}, Score: {score:.3f}')\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "            df = df_temp.copy() # Keep the DataFrame with the better cluster labels\n",
    "\n",
    "            # Check for early stop condition\n",
    "            if best_score >= target_silhouette_score:\n",
    "                print(f'‚úÖ Good enough score found: {best_score:.3f} with params {best_params}')\n",
    "                with keyword_cluster_params.open('w') as file:\n",
    "                    json.dump(best_params, file)\n",
    "                print(f'Saved best parameters: {best_params}')\n",
    "                break\n",
    "    \n",
    "    # If the search finished without hitting the target, save the highest score achieved\n",
    "    if best_score < target_silhouette_score and best_params:\n",
    "        print(f'Highest score reached: {best_score:.3f}. Saving best parameters found.')\n",
    "        with keyword_cluster_params.open('w') as file:\n",
    "            json.dump(best_params, file)\n",
    "            \n",
    "# 3. Finalize Clustering (Ensures the final best DataFrame is used if no search was run)\n",
    "# This step is crucial if the initial loaded parameters were already above the target, \n",
    "# as the 'df' used inside the loop might be a temporary copy. Re-cluster with the best params found.\n",
    "if 'Keyword Cluster' not in df.columns: # If the original clustering didn't run or was skipped\n",
    "    df, _, _ = keyword_clustering(df, 'Keyword', **best_params)\n",
    "\n",
    "\n",
    "# 4. Naming clusters and saving the result\n",
    "print(\"\\nNaming clusters...\")\n",
    "# Original logic: df = bf.name_keyword_clusters(df, 'Keyword', 'Keyword Cluster')\n",
    "df = name_keyword_clusters(df, 'Keyword', 'Keyword Cluster')\n",
    "\n",
    "\n",
    "# --- FINAL REORDERING (Using surgical replacement of bf.reorder_columns) ---\n",
    "df = reorder_columns_surgical(df, 'CPC', after_column='Keyword Difficulty')\n",
    "df = reorder_columns_surgical(df, 'Keyword Group (Experimental)', after_column='Number of Words')\n",
    "df = reorder_columns_surgical(df, 'CPC', after_column='Number of Words') # This reorder is duplicated/redundant with the previous one, but ported verbatim\n",
    "\n",
    "# Conditional column reordering based on data presence\n",
    "if has_botify:\n",
    "    # The diagnostic showed the column is 'Meta Description'. We use this.\n",
    "    df = reorder_columns_surgical(df, 'Client URL', after_column='Meta Description')\n",
    "else:\n",
    "    # Fallback remains unchanged (no Meta Description column to place Client URL after)\n",
    "    df = reorder_columns_surgical(df, 'Client URL', after_column='Competition')\n",
    "    \n",
    "# Original: df = bf.reorder_columns(df, 'Competitor URL', after_column='Client URL')\n",
    "df = reorder_columns_surgical(df, 'Competitor URL', after_column='Client URL')\n",
    "\n",
    "# Final file persistence\n",
    "df.to_csv(unformatted_csv, index=False)\n",
    "\n",
    "\n",
    "# --- DISPLAY FINAL CLUSTER COUNTS ---\n",
    "print(\"\\nFinal Keyword Group Counts:\")\n",
    "value_counts = df[\"Keyword Group (Experimental)\"].value_counts()\n",
    "if not value_counts.empty:\n",
    "    max_digits = len(str(len(value_counts)))\n",
    "    max_index_width = max(len(str(index)) for index in value_counts.index)\n",
    "    max_count_width = max(len(f\"{count:,}\") for count in value_counts)\n",
    "    \n",
    "    for i, (index, count) in enumerate(value_counts.items(), start=1):\n",
    "        counter_str = str(i).zfill(max_digits)\n",
    "        count_str = f\"{count:,}\"\n",
    "        print(f\"{counter_str}: {index:<{max_index_width}} - {count_str:>{max_count_width}}\")\n",
    "else:\n",
    "    print(\"‚ùå No keyword groups were created.\")\n",
    "\n",
    "# Store final DataFrame in the pipeline\n",
    "pip.set(job, 'final_clustered_df_json', df.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CORE SUPPORT FUNCTIONS ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Surgical port of _open_folder from FAQuilizer, necessary for the widget to work\n",
    "def _open_folder(path_str: str = \".\"):\n",
    "    \"\"\"Opens the specified folder in the system's default file explorer.\"\"\"\n",
    "    folder_path = Path(path_str).resolve()\n",
    "    \n",
    "    if not folder_path.exists() or not folder_path.is_dir():\n",
    "        print(f\"‚ùå Error: Path is not a valid directory: {folder_path}\")\n",
    "        return\n",
    "\n",
    "    system = platform.system()\n",
    "    try:\n",
    "        if system == \"Windows\":\n",
    "            os.startfile(folder_path)\n",
    "        elif system == \"Darwin\":  # macOS\n",
    "            subprocess.run([\"open\", folder_path])\n",
    "        else:  # Linux (xdg-open covers most desktop environments)\n",
    "            subprocess.run([\"xdg-open\", folder_path])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to open folder. Error: {e}\")\n",
    "\n",
    "# This utility must be defined for normalize_and_score to work\n",
    "def safe_normalize(series):\n",
    "    \"\"\" Normalize the series safely to avoid divide by zero and handle NaN values. \"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    range_val = max_val - min_val\n",
    "    if range_val == 0:\n",
    "        # Avoid division by zero by returning zero array if no range\n",
    "        return np.zeros_like(series)\n",
    "    else:\n",
    "        # Normalize and fill NaN values that might result from empty/NaN series\n",
    "        return (series - min_val).div(range_val).fillna(0)\n",
    "\n",
    "# Surgical port of bf.reorder_columns\n",
    "def reorder_columns_surgical(df, priority_column, after_column):\n",
    "    if priority_column in df.columns:\n",
    "        columns = list(df.columns.drop(priority_column))\n",
    "        # Handle cases where the after_column may have been dropped earlier\n",
    "        if after_column not in columns:\n",
    "            print(f\"‚ö†Ô∏è Reorder Error: Target column '{after_column}' not found. Skipping reorder of '{priority_column}'.\")\n",
    "            return df\n",
    "            \n",
    "        after_column_index = columns.index(after_column)\n",
    "        columns.insert(after_column_index + 1, priority_column)\n",
    "        df = df[columns]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Reorder Error: Column '{priority_column}' not found in DataFrame.\")\n",
    "    return df\n",
    "\n",
    "# Surgical port of bf.normalize_and_score - WITH CRITICAL FIX\n",
    "def normalize_and_score_surgical(df, registered_domain, has_botify_data, after_col, reorder):\n",
    "    \n",
    "    # Rename original column fields to match expected names in the dataframe\n",
    "    if 'internal_page_rank.raw' in df.columns:\n",
    "        df = df.rename(columns={'internal_page_rank.raw': 'Raw Internal Pagerank'}, inplace=False)\n",
    "    \n",
    "    # --- CRITICAL FIX FOR KEYERROR / TRAILING SLASHES ---\n",
    "    # The lookup key (e.g., 'nixos.org') must be matched against the DataFrame column (e.g., 'nixos.org/').\n",
    "    # We clean both for comparison to find the unique canonical key, but use the original column name.\n",
    "    \n",
    "    # Clean the lookup domain (assuming the input `registered_domain` might be missing the slash)\n",
    "    clean_lookup_key = registered_domain.rstrip('/')\n",
    "    target_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Find the column whose stripped name matches the stripped lookup key.\n",
    "        if col.rstrip('/') == clean_lookup_key:\n",
    "            target_col = col\n",
    "            break\n",
    "            \n",
    "    if target_col is None:\n",
    "        raise KeyError(f\"Could not find client domain column for '{registered_domain}' in DataFrame. Available columns: {df.columns.tolist()}\")\n",
    "    # --- END CRITICAL FIX ---\n",
    "    \n",
    "    # Normalize metrics that are always included\n",
    "    df['Normalized Search Volume'] = safe_normalize(df['Search Volume'])\n",
    "    df['Normalized Search Position'] = safe_normalize(df[target_col]) # <-- USES THE FOUND, CANONICAL COLUMN NAME\n",
    "    df['Normalized Keyword Difficulty'] = safe_normalize(df['Keyword Difficulty'])\n",
    "    df['Normalized CPC'] = safe_normalize(df['CPC'])\n",
    "\n",
    "    # Always include CPC and Keyword Difficulty in the combined score\n",
    "    combined_score = df['Normalized CPC'] - df['Normalized Keyword Difficulty']\n",
    "\n",
    "    if has_botify_data:\n",
    "        # Normalize additional Botify metrics if available\n",
    "        if 'Raw Internal Pagerank' in df.columns:\n",
    "            df['Normalized Raw Internal Pagerank'] = safe_normalize(df['Raw Internal Pagerank'])\n",
    "        else:\n",
    "            df['Normalized Raw Internal Pagerank'] = 0\n",
    "\n",
    "        if \"No. of Missed Clicks excluding anonymized queries\" in df.columns:\n",
    "            df['Normalized Missed Clicks'] = safe_normalize(df[\"No. of Missed Clicks excluding anonymized queries\"])\n",
    "            combined_score += df['Normalized Missed Clicks']\n",
    "        else:\n",
    "            df['Normalized Missed Clicks'] = 0\n",
    "\n",
    "        # Add Botify metrics to the combined score\n",
    "        combined_score += (-1 * df['Normalized Raw Internal Pagerank'] +\n",
    "                           df['Normalized Search Volume'] +\n",
    "                           df['Normalized Search Position'])\n",
    "\n",
    "    # Apply the combined score to the DataFrame\n",
    "    df['Combined Score'] = combined_score\n",
    "\n",
    "    if reorder:\n",
    "        # Reorder columns if required (using the surgically ported reorder function)\n",
    "        df = reorder_columns_surgical(df, \"CPC\", after_col)\n",
    "        df = reorder_columns_surgical(df, \"Keyword Difficulty\", \"CPC\")\n",
    "        if has_botify_data:\n",
    "            df = reorder_columns_surgical(df, \"Internal Pagerank\", \"Keyword Difficulty\")\n",
    "            df = reorder_columns_surgical(df, \"No. of Unique Inlinks\", \"Internal Pagerank\")\n",
    "            if \"No. of Missed Clicks excluding anonymized queries\" in df.columns:\n",
    "                df = reorder_columns_surgical(df, \"No. of Missed Clicks excluding anonymized queries\", \"No. of Unique Inlinks\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "# NOTE: This cell assumes 'job', 'df', 'competitors', 'semrush_lookup', 'has_botify' are defined.\n",
    "\n",
    "# --- 1. DEFINE SECURE OUTPUT PATHS ---\n",
    "# Create the secure, client-specific deliverables folder: Notebooks/deliverables/{job}/\n",
    "deliverables_dir = Path(\"deliverables\") / job\n",
    "deliverables_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the final Excel file path (using a standardized, clean name for the file)\n",
    "# We ensure the filename is clean by stripping the trailing slash/underscore that might be left by SEMRush input.\n",
    "xl_filename = f\"{semrush_lookup.replace('.', '_').rstrip('_')}_GAPalyzer_{job}_V1.xlsx\"\n",
    "xl_file = deliverables_dir / xl_filename\n",
    "\n",
    "# --- 2. EXECUTE CORE LOGIC ---\n",
    "print(f\"- Writing Gap Analysis tab to {xl_file.name} (first pass)...\")\n",
    "\n",
    "# Initialize Loop List (verbatim)\n",
    "loop_list = [\"Gap Analysis\"]\n",
    "last_competitor = competitors[-1]\n",
    "\n",
    "# Apply the normalization and scoring logic\n",
    "# The target_col lookup inside this function will now succeed.\n",
    "df_tab = normalize_and_score_surgical(df.copy(), semrush_lookup, has_botify, last_competitor, False)\n",
    "\n",
    "# Save Initial Excel Sheet with xlsxwriter (Engine Kwargs included verbatim)\n",
    "arg_dict = {'options': {'strings_to_urls': False}}\n",
    "try:\n",
    "    with pd.ExcelWriter(xl_file, engine=\"xlsxwriter\", engine_kwargs=arg_dict, mode='w') as writer:\n",
    "        # NOTE: Using df_tab here, which has the Normalized columns\n",
    "        df_tab.to_excel(writer, sheet_name='Gap Analysis', index=False)\n",
    "    \n",
    "    print(\"‚úÖ Gap Analysis tab written (Unformatted Pass 1).\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error writing Excel file: {e}\")\n",
    "    \n",
    "# --- 3. DISPLAY SECURE EGRESS BUTTON ---\n",
    "\n",
    "# The button description clearly indicates the file and the security of the folder.\n",
    "button = widgets.Button(\n",
    "    description=f\"üìÇ Open Deliverables Folder ({job})\",\n",
    "    tooltip=f\"Open {deliverables_dir.resolve()}\",\n",
    "    button_style='success'\n",
    ")\n",
    "# Use a lambda function to call the portable _open_folder function on click\n",
    "button.on_click(lambda b: _open_folder(str(deliverables_dir)))\n",
    "display(button)\n",
    "\n",
    "# Store final file path in pipeline\n",
    "pip.set(job, 'final_xl_file', str(xl_file))\n",
    "pip.set(job, 'deliverables_folder', str(deliverables_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"- Adding filter tabs to {xl_file.name} (second pass)...\")\n",
    "\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: This cell assumes 'job', 'df', 'semrush_lookup', 'has_botify', \n",
    "#       'xl_file', 'competitors', and helper functions are defined.\n",
    "\n",
    "# --- PATH DEFINITION ---\n",
    "important_keywords_file = Path(\"data\") / f\"{job}_important_keywords.txt\" \n",
    "\n",
    "# --- CRITICAL FIX START: ENFORCE CANONICAL KEY ---\n",
    "# The lookup key (semrush_lookup) might be clean ('nixos.org'), while the column name is dirty ('nixos.org/').\n",
    "# We find the actual dirty column name once, and use that variable (TARGET_COMPETITOR_COL) everywhere.\n",
    "# NOTE: This logic must match the internal search in normalize_and_score_surgical.\n",
    "\n",
    "clean_lookup_key = semrush_lookup.rstrip('/')\n",
    "TARGET_COMPETITOR_COL = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if col.rstrip('/') == clean_lookup_key:\n",
    "        TARGET_COMPETITOR_COL = col\n",
    "        break\n",
    "\n",
    "if TARGET_COMPETITOR_COL is None:\n",
    "    # If the lookup fails here, the process MUST fail gracefully to prevent subsequent runtime errors.\n",
    "    raise KeyError(f\"CRITICAL ERROR: Could not find canonical competitor column for '{semrush_lookup}' in DataFrame. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"‚úÖ Canonical Competitor Column Identified: '{TARGET_COMPETITOR_COL}'\")\n",
    "# --- CRITICAL FIX END ---\n",
    "\n",
    "\n",
    "# --- REQUIRED SUPPORT FUNCTIONS (No changes needed, but must be present) ---\n",
    "# ... (All helper functions: reorder_columns_surgical, safe_normalize, normalize_and_score_surgical) ...\n",
    "# Assuming the necessary helper functions are either imported or defined above this block.\n",
    "# We trust that the version of normalize_and_score_surgical already has the internal fix.\n",
    "\n",
    "\n",
    "def read_keywords(file_path):\n",
    "    \"\"\"Function to read keywords from a file.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Warning: Keywords file not found at {file_path}. Skipping file-based filter.\")\n",
    "        return []\n",
    "    with open(file_path, 'r') as file:\n",
    "        important_keywords_list = [line.strip() for line in file.readlines()]\n",
    "    return important_keywords_list\n",
    "    \n",
    "def filter_df_by_keywords(df, keywords):\n",
    "    \"\"\"Function to filter dataframe based on an exact list of keywords.\"\"\"\n",
    "    return df[df[\"Keyword\"].isin(keywords)]\n",
    "\n",
    "\n",
    "# --- MAIN TAB GENERATION LOGIC ---\n",
    "\n",
    "print(\"Starting subsequent Excel tab generation (Appending via openpyxl)...\")\n",
    "\n",
    "# --- 1. Filter: Important Keywords (File-Based Filter) ---\n",
    "filter_name = \"Important Keywords\"\n",
    "\n",
    "if important_keywords_file.exists():\n",
    "    important_keywords_list = read_keywords(important_keywords_file)\n",
    "    if important_keywords_list:\n",
    "        print(f\"- Writing {filter_name} tab (via file list)...\")\n",
    "        \n",
    "        df_filtered = filter_df_by_keywords(df.copy(), important_keywords_list)\n",
    "        \n",
    "        df_filtered = normalize_and_score_surgical(df_filtered, semrush_lookup, has_botify, competitors[-1], True)\n",
    "        df_filtered.sort_values(by='Combined Score', ascending=False, inplace=True)\n",
    "        \n",
    "        with pd.ExcelWriter(xl_file, engine=\"openpyxl\", mode='a', if_sheet_exists='replace') as writer:\n",
    "            df_filtered.to_excel(writer, sheet_name=filter_name, index=False)\n",
    "    else:\n",
    "        print(f\"‚òëÔ∏è Skipping '{filter_name}': Keyword file is empty.\")\n",
    "else:\n",
    "    print(f\"‚òëÔ∏è Skipping '{filter_name}': Keywords file does not exist.\")\n",
    "\n",
    "\n",
    "# --- 2. Filter: Best Opportunities / Striking Distance ---\n",
    "filter_name = \"Best Opportunities\"\n",
    "striking_lower = 100\n",
    "\n",
    "df_tab = df.copy()\n",
    "\n",
    "if has_botify:\n",
    "    try:\n",
    "        # FIXED LINE: Uses TARGET_COMPETITOR_COL for direct DataFrame indexing\n",
    "        df_tab = df_tab[(df_tab[\"No. of Impressions excluding anonymized queries\"] > 0) & (df_tab[TARGET_COMPETITOR_COL] > 3)].copy()\n",
    "        print(f\"- Writing {filter_name} tab (Botify/GSC Striking Distance)...\")\n",
    "    except KeyError:\n",
    "        # Fallback uses TARGET_COMPETITOR_COL\n",
    "        # FIXED LINE: Uses TARGET_COMPETITOR_COL for fallback indexing\n",
    "        df_tab = df[(df[TARGET_COMPETITOR_COL] >= 4) & (df[TARGET_COMPETITOR_COL] <= striking_lower)].copy()\n",
    "        print(f\"- Writing {filter_name} tab (SEMRush Striking Distance fallback)...\")\n",
    "else:\n",
    "    # SEMRush-only logic uses TARGET_COMPETITOR_COL\n",
    "    # FIXED LINE: Uses TARGET_COMPETITOR_COL\n",
    "    df_tab = df[(df[TARGET_COMPETITOR_COL] >= 4) & (df[TARGET_COMPETITOR_COL] <= striking_lower)].copy()\n",
    "    print(f\"- Writing {filter_name} tab (SEMRush Striking Distance)...\")\n",
    "    \n",
    "# Apply scoring, reorder, and sort\n",
    "df_tab = normalize_and_score_surgical(df_tab.copy(), semrush_lookup, has_botify, competitors[-1], True)\n",
    "df_tab.sort_values(by='Combined Score', ascending=False, inplace=True)\n",
    "\n",
    "# Write out the tab\n",
    "with pd.ExcelWriter(xl_file, engine=\"openpyxl\", mode='a', if_sheet_exists='replace') as writer:\n",
    "    df_tab.to_excel(writer, sheet_name=filter_name, index=False)\n",
    "\n",
    "\n",
    "# --- 3. Filter: Important Keywords disable (Client Ranking Sort) ---\n",
    "filter_name = \"Important Keywords disable\"\n",
    "\n",
    "# Filter: Keywords where the client is ranking (position is not null)\n",
    "# FIXED LINE: Uses TARGET_COMPETITOR_COL\n",
    "df_tab = df[df[TARGET_COMPETITOR_COL].notnull()].sort_values(by=[TARGET_COMPETITOR_COL, 'Search Volume'], ascending=[True, False]).copy()\n",
    "    \n",
    "print(f\"- Writing {filter_name} tab (Client Rank Sort)...\")\n",
    "# Write out the tab\n",
    "with pd.ExcelWriter(xl_file, engine=\"openpyxl\", mode='a', if_sheet_exists='replace') as writer:\n",
    "    df_tab.to_excel(writer, sheet_name=filter_name, index=False)\n",
    "\n",
    "\n",
    "# --- 4. Loop: Targeted Keyword Filters (Gifts, Questions, Near Me) ---\n",
    "\n",
    "targeted_filters = [\n",
    "    (\"Gifts\", [\n",
    "        'gift', 'gifts', 'idea', 'ideas', 'present', 'presents', 'give', 'giving', \n",
    "        'black friday', 'cyber monday', 'cyber week', 'bfcm', 'bf', 'cm', \n",
    "        'holiday', 'deals', 'sales', 'offer', 'discount', 'shopping'\n",
    "    ]),\n",
    "    (\"Broad Questions\", '''\n",
    "        am are can could did do does for from had has have how i is may might must shall should was were what when where which who whom whose why will with would\n",
    "        '''.split()),\n",
    "    (\"Narrow Questions\", '''\n",
    "        who whom whose what which where when why how\n",
    "        '''.split()),\n",
    "    (\"Popular Modifiers\", [\n",
    "        'how to', 'best', 'review', 'reviews'\n",
    "    ]),\n",
    "    (\"Near Me\", ['near me', 'for sale', 'nearby', 'closest', 'near you', 'local'])\n",
    "]\n",
    "\n",
    "for filter_name, keywords in targeted_filters:\n",
    "    print(f\"- Writing {filter_name} tab...\")\n",
    "    \n",
    "    # Construct the RegEx pattern using word boundaries for precise matching\n",
    "    pattern = r'\\b(?:' + '|'.join([re.escape(k) for k in keywords]) + r')\\b'\n",
    "    \n",
    "    # Filter 1: Find keywords matching the pattern (e.g., 'gift', 'how', 'near me')\n",
    "    df_tab = df[df[\"Keyword\"].str.contains(pattern, case=False, na=False)].copy()\n",
    "    \n",
    "    # Filter 2: Apply Striking Distance Logic \n",
    "    # Keep keywords where client is ranking (pos >= 4) OR client is not ranking at all (NaN)\n",
    "    # FIXED LINE: Uses TARGET_COMPETITOR_COL\n",
    "    # df_tab = df_tab[(df_tab[TARGET_COMPETITOR_COL] >= 4) | (df_tab[TARGET_COMPETITOR_COL].isna())]\n",
    "    \n",
    "    # Apply scoring, reorder, and sort\n",
    "    df_tab = normalize_and_score_surgical(df_tab.copy(), semrush_lookup, has_botify, competitors[-1], True)\n",
    "    df_tab.sort_values(by='Combined Score', ascending=False, inplace=True)\n",
    "    \n",
    "    # Write out the tab\n",
    "    with pd.ExcelWriter(xl_file, engine=\"openpyxl\", mode='a', if_sheet_exists='replace') as writer:\n",
    "        df_tab.to_excel(writer, sheet_name=filter_name, index=False)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Done writing all supplementary Excel tabs.\")\n",
    "# Use a lambda function to call the portable _open_folder function on click\n",
    "button.on_click(lambda b: _open_folder(str(deliverables_dir)))\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font, Alignment, Border, Side\n",
    "from openpyxl.formatting.rule import ColorScaleRule\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "from openpyxl.utils import get_column_letter\n",
    "import re # Needed for is_safe_url\n",
    "import validators # Need to import validators for URL check\n",
    "\n",
    "print(f\"üé® Applying Excel Formatting to all data tabs in {xl_file.name} (third pass)...\")\n",
    "\n",
    "# NOTE: This cell assumes 'xl_file', 'competitors', 'semrush_lookup', 'has_botify'\n",
    "#       'TARGET_COMPETITOR_COL' (the verified column name) are defined in previous cells.\n",
    "\n",
    "# --- REQUIRED SUPPORT FUNCTIONS (Surgically Ported/Defined) ---\n",
    "\n",
    "def create_column_mapping(sheet):\n",
    "    \"\"\"Creates a dictionary mapping header names to column letters.\"\"\"\n",
    "    mapping = {}\n",
    "    for col_idx, column_cell in enumerate(sheet[1], 1): # Assumes headers are in row 1\n",
    "        column_letter = get_column_letter(col_idx)\n",
    "        mapping[str(column_cell.value)] = column_letter\n",
    "    return mapping\n",
    "\n",
    "def apply_fill_to_column_labels(sheet, column_mapping, columns_list, fill):\n",
    "    \"\"\"Applies a fill color to the header cells of specified columns.\"\"\"\n",
    "    for column_name in columns_list:\n",
    "        column_letter = column_mapping.get(column_name)\n",
    "        if column_letter:\n",
    "            cell = sheet[f\"{column_letter}1\"]\n",
    "            cell.fill = fill\n",
    "\n",
    "def find_last_data_row(sheet, keyword_column_letter):\n",
    "    \"\"\"Finds the last row containing data in a specific column (e.g., 'Keyword').\"\"\"\n",
    "    if not keyword_column_letter: # Handle case where keyword column might be missing\n",
    "        return sheet.max_row\n",
    "\n",
    "    last_row = sheet.max_row\n",
    "    # Iterate backwards from the max row\n",
    "    while last_row > 1 and sheet[f\"{keyword_column_letter}{last_row}\"].value in [None, \"\", \" \"]:\n",
    "        last_row -= 1\n",
    "    return last_row\n",
    "\n",
    "def apply_conditional_formatting(sheet, column_mapping, last_row, conditionals_descending, conditionals_ascending, rule_desc, rule_asc):\n",
    "    \"\"\"Applies color scale conditional formatting to specified columns.\"\"\"\n",
    "    for label in conditionals_descending + conditionals_ascending:\n",
    "        column_letter = column_mapping.get(label)\n",
    "        if column_letter and last_row > 1: # Ensure there is data to format\n",
    "            range_string = f'{column_letter}2:{column_letter}{last_row}'\n",
    "            rule = rule_desc if label in conditionals_descending else rule_asc\n",
    "            try:\n",
    "                sheet.conditional_formatting.add(range_string, rule)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to apply conditional formatting for {label}: {e}\")\n",
    "\n",
    "def is_safe_url(url):\n",
    "    \"\"\" Check if the given string is a valid URL using the validators library. \"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return False\n",
    "    # Use validators library for robust URL check\n",
    "    return validators.url(url)\n",
    "\n",
    "# Color schemes and patterns\n",
    "green = '33FF33'\n",
    "client_color = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid') # Yellow\n",
    "competitor_color = PatternFill(start_color='EEECE2', end_color='EEECE2', fill_type='solid') # Light Gray\n",
    "semrush_color = PatternFill(start_color='FAEADB', end_color='FAEADB', fill_type='solid') # Light Orange\n",
    "semrush_opportunity_color = PatternFill(start_color='F1C196', end_color='F1C196', fill_type='solid') # Darker Orange\n",
    "botify_color = PatternFill(start_color='EADFF2', end_color='EADFF2', fill_type='solid') # Light Purple\n",
    "botify_opportunity_color = PatternFill(start_color='AEA1C4', end_color='AEA1C4', fill_type='solid') # Darker Purple\n",
    "color_scale_rule_desc = ColorScaleRule(start_type='min', start_color='FFFFFF', end_type='max', end_color=green) # White to Green (Higher is Better)\n",
    "color_scale_rule_asc = ColorScaleRule(start_type='min', start_color=green, end_type='max', end_color='FFFFFF') # Green to White (Lower is Better)\n",
    "\n",
    "# Create a border style (Subtle hair lines, thin bottom for headers)\n",
    "thin_border = Border(left=Side(style='hair'), right=Side(style='hair'), top=Side(style='hair'), bottom=Side(style='thin'))\n",
    "\n",
    "# Commonly reused column widths\n",
    "tiny_width = 11\n",
    "small_width = 15\n",
    "medium_width = 20\n",
    "description_width = 50\n",
    "url_width = 70 # Adjusted slightly down from 100 for better viewability\n",
    "\n",
    "# Define column widths (Verbatim)\n",
    "column_widths = {\n",
    "    'Keyword': 40, 'Search Volume': small_width, 'Number of Words': tiny_width,\n",
    "    'Keyword Group (Experimental)': small_width, 'Competitors Positioning': tiny_width,\n",
    "    'CPC': tiny_width, 'Keyword Difficulty': tiny_width, 'Competition': tiny_width,\n",
    "    'Depth': tiny_width, 'No. of Keywords': tiny_width,\n",
    "    'No. of Impressions excluding anonymized queries': small_width,\n",
    "    'No. of Clicks excluding anonymized queries': small_width,\n",
    "    'No. of Missed Clicks excluding anonymized queries': small_width,\n",
    "    'Avg. URL CTR excluding anonymized queries': tiny_width,\n",
    "    'Avg. URL Position excluding anonymized queries': tiny_width,\n",
    "    'No. of Keywords for the URL To Achieve 90% Audience': tiny_width,\n",
    "    'Raw Internal Pagerank': small_width, 'Internal Pagerank': tiny_width,\n",
    "    'Internal Pagerank Position': tiny_width, 'No. of Unique Inlinks': tiny_width,\n",
    "    'Title': description_width, 'Meta Description': description_width,\n",
    "    'Timestamp': 12, 'SERP Features by Keyword': description_width,\n",
    "    'Keyword Intents': medium_width, 'Position Type': small_width,\n",
    "    'Number of Results': medium_width, 'Competitor URL': url_width,\n",
    "    'Client URL': url_width, # This gets renamed later\n",
    "    # Normalized/Score columns\n",
    "    'Normalized CPC': tiny_width, 'Normalized Keyword Difficulty': tiny_width,\n",
    "    'Normalized Raw Internal Pagerank': tiny_width, 'Normalized Search Volume': tiny_width,\n",
    "    'Normalized Search Position': tiny_width, 'Normalized Missed Clicks': tiny_width,\n",
    "    'Combined Score': tiny_width\n",
    "}\n",
    "\n",
    "# Commonly used number formats (Verbatim)\n",
    "int_fmt = '0'\n",
    "comma_fmt = '#,##0'\n",
    "pct_fmt = '0.00'\n",
    "date_fmt = 'yyyy-mm-dd' # Added for Timestamp clarity\n",
    "\n",
    "# Define number formats (Added Timestamp)\n",
    "number_formats = {\n",
    "    'Search Volume': comma_fmt, 'Number of Words': int_fmt, 'CPC': pct_fmt,\n",
    "    'Keyword Difficulty': int_fmt, 'Competition': pct_fmt, 'Depth': int_fmt,\n",
    "    'No. of Keywords': comma_fmt, 'No. of Impressions excluding anonymized queries': comma_fmt,\n",
    "    'No. of Clicks excluding anonymized queries': comma_fmt,\n",
    "    'No. of Missed Clicks excluding anonymized queries': comma_fmt,\n",
    "    'Avg. URL CTR excluding anonymized queries': pct_fmt,\n",
    "    'Avg. URL Position excluding anonymized queries': '0.0',\n",
    "    'No. of Keywords for the URL To Achieve 90% Audience': comma_fmt,\n",
    "    'Raw Internal Pagerank': '0.0000000', 'Internal Pagerank': pct_fmt,\n",
    "    'Internal Pagerank Position': int_fmt, 'No. of Unique Inlinks': comma_fmt,\n",
    "    'Number of Results': comma_fmt, 'Timestamp': date_fmt,\n",
    "    # Apply comma format to positioning and scores for consistency\n",
    "    'Competitors Positioning': int_fmt, 'Normalized CPC': pct_fmt,\n",
    "    'Normalized Keyword Difficulty': pct_fmt, 'Normalized Raw Internal Pagerank': pct_fmt,\n",
    "    'Normalized Search Volume': pct_fmt, 'Normalized Search Position': pct_fmt,\n",
    "    'Normalized Missed Clicks': pct_fmt, 'Combined Score': '0.00'\n",
    "}\n",
    "\n",
    "# --- DEFINE COLUMN GROUPS FOR COLORING (Verbatim, adapted for known columns) ---\n",
    "# Higher Numbers More Green (Descending is better)\n",
    "conditionals_descending = [\n",
    "    'Search Volume', 'CPC', 'Competition', # Removed Traffic metrics as they were dropped\n",
    "    'Avg. URL CTR excluding anonymized queries',\n",
    "    'No. of Missed Clicks excluding anonymized queries', 'Combined Score',\n",
    "    'No. of Unique Inlinks' # Added Inlinks (usually higher is better contextually)\n",
    "]\n",
    "# Lower Numbers More Green (Ascending is better)\n",
    "conditionals_ascending = [\n",
    "    'Keyword Difficulty', 'Raw Internal Pagerank', 'Internal Pagerank',\n",
    "    'Internal Pagerank Position', 'Avg. URL Position excluding anonymized queries', 'Depth',\n",
    "    TARGET_COMPETITOR_COL # Add the client's position column dynamically\n",
    "] + [col for col in competitors if col != TARGET_COMPETITOR_COL] # Add other competitor position columns\n",
    "\n",
    "# SEMRush Data Columns\n",
    "semrush_columns = [\n",
    "    'Keyword', 'Search Volume', 'CPC', 'Keyword Difficulty', 'Competition',\n",
    "    'SERP Features by Keyword', 'Keyword Intents', 'Position Type',\n",
    "    'Number of Results', 'Timestamp', 'Competitor URL', 'Client URL' # Includes Client URL before rename\n",
    "]\n",
    "# Botify Data Columns (Ensure these match final DataFrame after merge)\n",
    "botify_columns = [\n",
    "    'Depth', 'No. of Keywords', 'No. of Impressions excluding anonymized queries',\n",
    "    'No. of Clicks excluding anonymized queries', 'No. of Missed Clicks excluding anonymized queries',\n",
    "    'Avg. URL CTR excluding anonymized queries', 'Avg. URL Position excluding anonymized queries',\n",
    "    'No. of Keywords for the URL To Achieve 90% Audience', 'Raw Internal Pagerank',\n",
    "    'Internal Pagerank', 'Internal Pagerank Position', 'No. of Unique Inlinks',\n",
    "    'Title', 'Meta Description' # Changed from API name\n",
    "]\n",
    "# Columns which get bigger header fonts\n",
    "bigger_font_headers = [\n",
    "    \"Keyword\", \"Search Volume\", \"Title\", \"Meta Description\",\n",
    "    \"Competitor URL\", \"Client URL\", \"SERP Features by Keyword\"\n",
    "]\n",
    "# Columns which get darker Botify color\n",
    "botify_opportunity_columns = [\n",
    "    'Internal Pagerank', 'No. of Unique Inlinks',\n",
    "    'No. of Missed Clicks excluding anonymized queries',\n",
    "    'Normalized Raw Internal Pagerank', 'Normalized Missed Clicks'\n",
    "]\n",
    "# Columns which get darker SEMRush color\n",
    "semrush_opportunity_columns = [\n",
    "    'CPC', 'Keyword Difficulty', 'Normalized CPC', 'Normalized Keyword Difficulty',\n",
    "    'Normalized Search Volume', 'Normalized Search Position', 'Combined Score' # Added Combined Score here\n",
    "]\n",
    "\n",
    "\n",
    "# --- APPLY FORMATTING TO EXCEL FILE ---\n",
    "try:\n",
    "    wb = load_workbook(xl_file)\n",
    "\n",
    "    # --- UPDATED: Get all sheet names EXCEPT the diagnostics sheet ---\n",
    "    sheets_to_format = [name for name in wb.sheetnames if name != \"Filter Diagnostics\"]\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if not sheets_to_format:\n",
    "         print(\"‚ö†Ô∏è No data sheets found in the Excel file to format. Skipping formatting.\")\n",
    "\n",
    "    for sheet_name in sheets_to_format:\n",
    "        print(f\"- Formatting '{sheet_name}' tab...\")\n",
    "        sheet = wb[sheet_name]\n",
    "        column_mapping = create_column_mapping(sheet)\n",
    "\n",
    "        # Determine the last row with data based on the 'Keyword' column\n",
    "        keyword_col_letter = column_mapping.get(\"Keyword\")\n",
    "        # Add a check in case a sheet somehow doesn't have a Keyword column\n",
    "        if not keyword_col_letter:\n",
    "             print(f\"  Skipping sheet '{sheet_name}': Cannot find 'Keyword' column for formatting reference.\")\n",
    "             continue\n",
    "             \n",
    "        last_row = find_last_data_row(sheet, keyword_col_letter)\n",
    "        \n",
    "        # --- Apply Formatting ---\n",
    "\n",
    "        # 1. Fill client column (using TARGET_COMPETITOR_COL identified earlier)\n",
    "        client_column_letter = column_mapping.get(TARGET_COMPETITOR_COL)\n",
    "        if client_column_letter:\n",
    "            for row in range(1, last_row + 1):\n",
    "                cell = sheet[f\"{client_column_letter}{row}\"]\n",
    "                cell.fill = client_color\n",
    "                if row == 1: cell.font = Font(bold=True) # Bold header\n",
    "\n",
    "        # 2. Fill Header Backgrounds\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, semrush_columns, semrush_color)\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, botify_columns, botify_color)\n",
    "        # Apply competitor color only to competitor columns *present* in this sheet\n",
    "        present_competitors = [c for c in competitors if c in column_mapping and c != TARGET_COMPETITOR_COL]\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, present_competitors, competitor_color)\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, botify_opportunity_columns, botify_opportunity_color)\n",
    "        apply_fill_to_column_labels(sheet, column_mapping, semrush_opportunity_columns, semrush_opportunity_color)\n",
    "\n",
    "        # 3. Header Styling (Alignment, Font, Border)\n",
    "        header_font = Font(bold=True)\n",
    "        header_align = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
    "        for header, col_letter in column_mapping.items():\n",
    "            cell = sheet[f\"{col_letter}1\"]\n",
    "            cell.alignment = header_align\n",
    "            cell.font = header_font\n",
    "            cell.border = thin_border # Apply border to header\n",
    "            if header in bigger_font_headers:\n",
    "                 cell.font = Font(size=14, bold=True) # Slightly smaller than original for balance\n",
    "\n",
    "        # 4. Hyperlinks (Competitor URL, Client URL)\n",
    "        for col_label in [\"Competitor URL\", \"Client URL\"]:\n",
    "            col_letter = column_mapping.get(col_label)\n",
    "            if col_letter:\n",
    "                for row in range(2, last_row + 1):\n",
    "                    cell = sheet[f\"{col_letter}{row}\"]\n",
    "                    url = cell.value\n",
    "                    if url and is_safe_url(url) and not str(url).startswith('=HYPERLINK'):\n",
    "                        # Truncate displayed URL if very long, keep full URL in link\n",
    "                        display_text = url if len(url) <= 80 else url[:77] + \"...\"\n",
    "                        cell.value = f'=HYPERLINK(\"{url}\", \"{display_text}\")'\n",
    "                        cell.font = Font(color=\"0000FF\", underline=\"single\")\n",
    "                        cell.alignment = Alignment(vertical='top', wrap_text=False) # Prevent wrap for URLs\n",
    "\n",
    "\n",
    "        # 5. Rotate Competitor Headers & Set Width\n",
    "        competitor_header_align = Alignment(vertical='bottom', textRotation=90, horizontal='center')\n",
    "        for competitor_col_name in competitors:\n",
    "            col_letter = column_mapping.get(competitor_col_name)\n",
    "            if col_letter:\n",
    "                cell = sheet[f\"{col_letter}1\"]\n",
    "                cell.alignment = competitor_header_align\n",
    "                sheet.column_dimensions[col_letter].width = 4\n",
    "\n",
    "        # 6. Apply Column Widths (with Global Adjustment)\n",
    "        for label, width in column_widths.items():\n",
    "            column_letter = column_mapping.get(label)\n",
    "            if column_letter:\n",
    "                # Apply the global width adjustment multiplier\n",
    "                sheet.column_dimensions[column_letter].width = width * GLOBAL_WIDTH_ADJUSTMENT\n",
    "\n",
    "        # 7. Apply Number Formats\n",
    "        for label, format_code in number_formats.items():\n",
    "            column_letter = column_mapping.get(label)\n",
    "            if column_letter:\n",
    "                for row in range(2, last_row + 1):\n",
    "                    cell = sheet[f\"{column_letter}{row}\"]\n",
    "                    # Apply only if cell is not empty, prevents formatting issues\n",
    "                    if cell.value is not None:\n",
    "                        cell.number_format = format_code\n",
    "\n",
    "        # 8. Apply Conditional Formatting (Using the combined rules)\n",
    "        apply_conditional_formatting(sheet, column_mapping, last_row, conditionals_descending, conditionals_ascending, color_scale_rule_desc, color_scale_rule_asc)\n",
    "\n",
    "        # 9. Rename 'Client URL' Header Dynamically\n",
    "        client_url_column_letter = column_mapping.get(\"Client URL\")\n",
    "        if client_url_column_letter:\n",
    "            header_cell = sheet[f\"{client_url_column_letter}1\"]\n",
    "            header_cell.value = f\"{TARGET_COMPETITOR_COL} URL\" # Use the canonical name\n",
    "\n",
    "        # 10. Data Cell Alignment (Wrap text, top align)\n",
    "        data_align = Alignment(wrap_text=False, vertical='top')\n",
    "        url_columns = [column_mapping.get(\"Competitor URL\"), column_mapping.get(\"Client URL\")] # Get letters before loop\n",
    "        for row_idx in range(2, last_row + 1):\n",
    "            for col_idx in range(1, sheet.max_column + 1):\n",
    "                cell = sheet.cell(row=row_idx, column=col_idx)\n",
    "                col_letter = get_column_letter(col_idx)\n",
    "                # Apply default alignment, skip URL columns handled earlier\n",
    "                if col_letter not in url_columns:\n",
    "                    cell.alignment = data_align\n",
    "\n",
    "\n",
    "        # 11. Header Row Height & Freeze Panes\n",
    "        # Use the explicit configuration variable for header height\n",
    "        sheet.row_dimensions[1].height = locals().get('max_length', 15) * 9 if 'max_length' in locals() else 100\n",
    "        sheet.freeze_panes = 'C2' # Freeze panes more appropriately after Keyword/Volume\n",
    "\n",
    "        # 12. Apply AutoFilter\n",
    "        max_col_letter = get_column_letter(sheet.max_column)\n",
    "        if last_row > 0: # Ensure there are rows to filter\n",
    "             sheet.auto_filter.ref = f\"A1:{max_col_letter}{last_row}\"\n",
    "\n",
    "        # 13. (Optional but recommended) Add Table for banded rows (replaces manual banding)\n",
    "        if last_row > 0: # Ensure there is data for the table\n",
    "            table_range = f\"A1:{max_col_letter}{last_row}\"\n",
    "            table_name = f\"DataTable_{re.sub(r'[^A-Za-z0-9_]', '', sheet_name)}\" # Sanitize sheet name for table name\n",
    "\n",
    "            # --- CORRECTED TABLE CHECK ---\n",
    "            # Defensively check if items in sheet._tables have a .name attribute\n",
    "            existing_table_names = [t.name for t in sheet._tables if hasattr(t, 'name')]\n",
    "            if table_name not in existing_table_names:\n",
    "            # --- END CORRECTION ---\n",
    "                 tab = Table(displayName=table_name, ref=table_range)\n",
    "                 style = TableStyleInfo(name=\"TableStyleMedium9\", showFirstColumn=False,\n",
    "                                       showLastColumn=False, showRowStripes=True, showColumnStripes=False)\n",
    "                 tab.tableStyleInfo = style\n",
    "                 try:\n",
    "                      sheet.add_table(tab)\n",
    "                 except ValueError as ve:\n",
    "                      print(f\"  Note: Could not add Excel Table '{table_name}' to sheet '{sheet_name}'. Maybe overlaps existing table? Error: {ve}\")\n",
    "            # Optional: Add an else here if you want to log that the table already exists\n",
    "            # else:\n",
    "            #    print(f\"  Skipping table creation: Table '{table_name}' already exists in sheet '{sheet_name}'.\")\n",
    "\n",
    "    # Save the workbook with all formatting applied\n",
    "    wb.save(xl_file)\n",
    "    print(f\"‚úÖ Formatting applied to all data tabs and saved to {xl_file.name}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Excel file not found at {xl_file}. Cannot apply formatting.\")\n",
    "except KeyError as e:\n",
    "     print(f\"‚ùå Error during formatting: A required column key was not found: {e}. Check DataFrame structure.\")\n",
    "     # Safely attempt to print mapping if it exists\n",
    "     if 'column_mapping' in locals(): print(f\"   Column Mapping: {column_mapping}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An unexpected error occurred during Excel formatting: {e}\")\n",
    "# Use a lambda function to call the portable _open_folder function on click\n",
    "button.on_click(lambda b: _open_folder(str(deliverables_dir)))\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
