{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pipulate import pip\n",
    "import secretsauce\n",
    "import nest_asyncio\n",
    "import keys\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- ⚙️ Workflow Configuration ---\n",
    "\n",
    "# Set the maximum number of rows for the final Excel export.\n",
    "# A smaller number (like 1000) is great for fast testing.\n",
    "ROW_LIMIT = 3000 \n",
    "\n",
    "# --- NEW LOGIC ---\n",
    "# Set the maximum number of *competitors* to process (in addition to your_domain)\n",
    "# Set to 10 for rapid testing, or -1 to process all competitors.\n",
    "COMPETITOR_LIMIT = 3\n",
    "# --- END NEW LOGIC ---\n",
    "\n",
    "print(f\"✅ Configuration set: Final report will be limited to {ROW_LIMIT} rows.\")\n",
    "if COMPETITOR_LIMIT != -1:\n",
    "    print(f\"✅ Configuration set: Processing will be limited to the top {COMPETITOR_LIMIT} competitors.\")\n",
    "else:\n",
    "    print(f\"✅ Configuration set: Processing all competitors.\")\n",
    "    \n",
    "job = \"gapalyzer-01\" # Give your session a unique name \n",
    "\n",
    "print(f\"✅ Configuration set: Final report will be limited to {ROW_LIMIT} rows.\")\n",
    "\n",
    "job = \"gapalyzer-01\" # Give your session a unique name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Here are the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "pip.api_key(job, key=keys.google)\n",
    "botify_token = keys.botify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Here are your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "https://nixos.org/    # Linux\n",
    "https://pypi.org/     # Python\n",
    "https://neovim.io/    # vim\n",
    "https://git-scm.com/  # git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "def get_competitors_from_notebook(notebook_filename=\"GAPalyzer.ipynb\"):\n",
    "    \"\"\"Parses this notebook to get the domain list from the 'url-list-input' cell.\"\"\"\n",
    "    try:\n",
    "        notebook_path = Path(notebook_filename)\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        for cell in nb.cells:\n",
    "            if \"url-list-input\" in cell.metadata.get(\"tags\", []):\n",
    "                domains_raw = cell.source\n",
    "                domains = [\n",
    "                    line.split('#')[0].strip() \n",
    "                    for line in domains_raw.splitlines() \n",
    "                    if line.strip() and not line.strip().startswith('#')\n",
    "                ]\n",
    "                return domains\n",
    "        print(\"⚠️ Warning: Could not find a cell tagged with 'url-list-input'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading domains from notebook: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"🚀 Generating SEMrush URLs for GAP analysis...\")\n",
    "\n",
    "domains = get_competitors_from_notebook()\n",
    "url_template = \"https://www.semrush.com/analytics/organic/positions/?db=us&q={domain}&searchType=domain\"\n",
    "\n",
    "if not domains:\n",
    "    print(\"🛑 No domains found. Please add competitor domains to the 'url-list-input' cell and re-run.\")\n",
    "else:\n",
    "    print(f\"✅ Found {len(domains)} competitor domains. Click the links below to open each report:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, domain in enumerate(domains):\n",
    "        full_url = url_template.format(domain=domain)\n",
    "        print(f\"{i+1}. {domain}:\\n   {full_url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Here's how it goes:\n",
    "### 1. Stack 'Em (All downloads to 1 file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nbformat # Make sure this import is here\n",
    "\n",
    "def get_all_domains_from_notebook(notebook_filename=\"GAPalyzer.ipynb\"):\n",
    "    \"\"\"Parses this notebook to get the full domain list from the 'url-list-input' cell.\"\"\"\n",
    "    try:\n",
    "        notebook_path = Path(notebook_filename)\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        for cell in nb.cells:\n",
    "            if \"url-list-input\" in cell.metadata.get(\"tags\", []):\n",
    "                domains_raw = cell.source\n",
    "                domains = [\n",
    "                    line.split('#')[0].strip() \n",
    "                    for line in domains_raw.splitlines() \n",
    "                    if line.strip() and not line.strip().startswith('#')\n",
    "                ]\n",
    "                return domains\n",
    "        print(\"⚠️ Warning: Could not find a cell tagged with 'url-list-input'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading domains from notebook: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"🥞 Stacking 'Em: Consolidating all SEMrush Excel files...\")\n",
    "\n",
    "all_domains = get_all_domains_from_notebook()\n",
    "\n",
    "if not all_domains:\n",
    "    print(\"🛑 No domains found. Please add your client and competitor domains to the 'url-list-input' cell.\")\n",
    "else:\n",
    "    your_domain = all_domains[0]\n",
    "    print(f\"✅ Client domain identified as: {your_domain}\")\n",
    "    \n",
    "    # --- NEW LOGIC ---\n",
    "    # 3. Apply the COMPETITOR_LIMIT\n",
    "    competitor_domains = all_domains[1:]\n",
    "    if COMPETITOR_LIMIT != -1:\n",
    "        competitor_domains = competitor_domains[:COMPETITOR_LIMIT]\n",
    "        print(f\"🔪 Limiting to Top {len(competitor_domains)} competitors based on 'url-list-input' order.\")\n",
    "    \n",
    "    # This is the final list of domains we will actually load files for\n",
    "    domains_to_process = [your_domain] + competitor_domains\n",
    "    # --- END NEW LOGIC ---\n",
    "    \n",
    "    downloads_dir = Path.home() / \"Downloads\"\n",
    "    all_excel_files = list(downloads_dir.glob(\"*.xlsx\"))\n",
    "    \n",
    "    list_of_dataframes = []\n",
    "    for file_path in all_excel_files:\n",
    "        try:\n",
    "            domain_name = file_path.name.split('-organic.Positions')[0]\n",
    "            \n",
    "            # --- MODIFIED: Use domains_to_process to filter ---\n",
    "            if domain_name in domains_to_process:\n",
    "                df = pd.read_excel(file_path)\n",
    "                df['domain'] = domain_name\n",
    "                df[\"Client URL\"] = df.apply(lambda row: row[\"URL\"] if row[\"domain\"] == your_domain else None, axis=1)\n",
    "                df[\"Competitor URL\"] = df.apply(lambda row: row[\"URL\"] if row[\"domain\"] != your_domain else None, axis=1)\n",
    "                list_of_dataframes.append(df)\n",
    "                print(f\"  -> ✅ Loaded and tagged {file_path.name}\")\n",
    "            # --- END MODIFIED ---\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  -> ⚠️ Could not process file {file_path.name}: {e}\")\n",
    "\n",
    "    if list_of_dataframes:\n",
    "        combined_df = pd.concat(list_of_dataframes, ignore_index=True)\n",
    "        print(\"\\n🎉 Success! All relevant Excel files have been consolidated.\")\n",
    "        display(combined_df.head())\n",
    "    else:\n",
    "        print(\"🤷 No matching Excel files were found in the Downloads folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Aggregate All Metrics 🧮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"⚙️ Aggregating all keyword metrics from the master list...\")\n",
    "\n",
    "if 'combined_df' not in locals():\n",
    "    print(\"❌ 'combined_df' not found. Please run the 'Stack \\'Em' cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Define the aggregation functions for each metric\n",
    "        # This is the \"smart\" logic from your blueprint\n",
    "        agg_funcs = {\n",
    "            'Position': 'min',  # Get the *best* (lowest) position\n",
    "            'Search Volume': 'max', # Get the *highest* (most accurate) volume\n",
    "            'CPC': 'mean',\n",
    "            'Keyword Difficulty': 'mean',\n",
    "            'Competition': 'mean',\n",
    "            'Number of Results': 'max',\n",
    "            'Timestamp': 'max',\n",
    "            'SERP Features by Keyword': 'first',\n",
    "            'Keyword Intents': 'first',\n",
    "            'Position Type': 'first',\n",
    "            'URL': 'first', \n",
    "            'Client URL': 'first',\n",
    "            'Competitor URL': 'first'\n",
    "        }\n",
    "        \n",
    "        # Select only columns that exist in the DataFrame to avoid errors\n",
    "        cols_to_agg = {k: v for k, v in agg_funcs.items() if k in combined_df.columns}\n",
    "        \n",
    "        print(f\"  -> Aggregating columns: {list(cols_to_agg.keys())}\")\n",
    "        \n",
    "        # Create the aggregate DataFrame\n",
    "        agg_df = combined_df.groupby('Keyword').agg(cols_to_agg).reset_index()\n",
    "        \n",
    "        # Add 'Number of Words' column\n",
    "        agg_df['Number of Words'] = agg_df[\"Keyword\"].apply(lambda x: len(x.split()))\n",
    "        \n",
    "        print(\"\\n🎉 Success! Aggregate metrics table created.\")\n",
    "        display(agg_df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred during aggregation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2. Join 'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"🧩 Joining 'Em: Pivoting data to map keywords against competitors...\")\n",
    "\n",
    "if 'combined_df' not in locals() or 'your_domain' not in locals():\n",
    "    print(\"❌ 'combined_df' or 'your_domain' not found. Please run the 'Stack \\'Em' cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        pivot_data = combined_df[['Keyword', 'domain', 'Search Volume']]\n",
    "        pivoted_df = pivot_data.pivot_table(\n",
    "            index='Keyword', \n",
    "            columns='domain', \n",
    "            values='Search Volume',\n",
    "            aggfunc='first'\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Reorder columns to put the client's domain first\n",
    "        if your_domain in pivoted_df.columns:\n",
    "            cols = [your_domain] + [col for col in pivoted_df if col != your_domain]\n",
    "            pivoted_df = pivoted_df[cols]\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Your specified domain '{your_domain}' was not found in the data.\")\n",
    "\n",
    "        print(\"\\n🎉 Success! Data has been pivoted into a competitive matrix.\")\n",
    "        display(pivoted_df.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred during the pivot process: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 3. Aggregate All Metrics 🧮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"⚙️ Aggregating all keyword metrics from the master list...\")\n",
    "\n",
    "if 'combined_df' not in locals():\n",
    "    print(\"❌ 'combined_df' not found. Please run the 'Stack \\'Em' cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Define the aggregation functions for each metric\n",
    "        agg_funcs = {\n",
    "            'Position': 'min',\n",
    "            'Search Volume': 'max',\n",
    "            'CPC': 'mean',\n",
    "            'Keyword Difficulty': 'mean',\n",
    "            'Competition': 'mean',\n",
    "            'Number of Results': 'max',\n",
    "            'Timestamp': 'max',\n",
    "            'SERP Features by Keyword': 'first',\n",
    "            'Keyword Intents': 'first',\n",
    "            'Position Type': 'first',\n",
    "            \n",
    "            # --- NEW LOGIC (from 40_GAP_Analysis.py) ---\n",
    "            'URL': 'first', # Keep a general URL for reference\n",
    "            'Client URL': 'first',      # This will find the client's URL (if any)\n",
    "            'Competitor URL': 'first'   # This will find the first competitor's URL\n",
    "            # --- END NEW LOGIC ---\n",
    "        }\n",
    "        \n",
    "        cols_to_agg = {k: v for k, v in agg_funcs.items() if k in combined_df.columns}\n",
    "        print(f\"  -> Aggregating columns: {list(cols_to_agg.keys())}\")\n",
    "        \n",
    "        agg_df = combined_df.groupby('Keyword').agg(cols_to_agg).reset_index()\n",
    "        agg_df['Number of Words'] = agg_df[\"Keyword\"].apply(lambda x: len(x.split()))\n",
    "        \n",
    "        print(\"\\n🎉 Success! Aggregate metrics table created.\")\n",
    "        display(agg_df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred during aggregation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 4. Merge, Tag & Score 'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"🧩🏷️ Joining, Tagging, & Scoring: Building the final analysis table...\")\n",
    "\n",
    "if 'combined_df' not in locals() or 'agg_df' not in locals() or 'your_domain' not in locals():\n",
    "    print(\"❌ Required DataFrames ('combined_df', 'agg_df') or 'your_domain' variable not found. Please run the previous cells first.\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. PIVOT ON POSITION (THE FIX)\n",
    "        # This creates the wide matrix of competitor *rankings*, not search volumes.\n",
    "        print(\"  -> Pivoting on 'Position' to create competitor rank matrix...\")\n",
    "        pivot_df = combined_df.pivot_table(\n",
    "            index='Keyword', \n",
    "            columns='domain', \n",
    "            values='Position', # <-- The correct value to pivot!\n",
    "            aggfunc='min'     # Get the *best* rank\n",
    "        ).fillna(0) # A 0 here now correctly means \"no rank\"\n",
    "\n",
    "        # 2. MERGE 'EM: Join the pivoted ranks with the aggregate metrics\n",
    "        # This is the \"flat-table to flat-table join\"\n",
    "        print(\"  -> Merging pivoted ranks with aggregate metrics...\")\n",
    "        analysis_df = pd.merge(\n",
    "            pivot_df.reset_index(),  # Start with (Keyword, your_domain_rank, comp1_rank, ...)\n",
    "            agg_df,                    # Join with (Keyword, Search Volume, CPC, Client URL, ...)\n",
    "            on='Keyword',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # 3. TAG 'EM: Identify competitor columns and find the gaps\n",
    "        competitor_cols = [col for col in pivot_df.columns if col != your_domain]\n",
    "        analysis_df['competitor_count'] = (analysis_df[competitor_cols] > 0).sum(axis=1)\n",
    "        \n",
    "        # Filter for gaps:\n",
    "        # - Your domain's rank is 0 (or NaN)\n",
    "        # - At least one competitor ranks (count > 0)\n",
    "        gap_df = analysis_df[\n",
    "            (analysis_df[your_domain].fillna(0) == 0) &\n",
    "            (analysis_df['competitor_count'] > 0)\n",
    "        ].copy()\n",
    "        \n",
    "        # 4. SCORE 'EM: Create the opportunity score\n",
    "        gap_df['opportunity_score'] = gap_df['Search Volume'] * gap_df['competitor_count']\n",
    "        \n",
    "        # 5. SORT & ORGANIZE:\n",
    "        metric_cols = [\n",
    "            'Search Volume', 'opportunity_score', 'competitor_count', \n",
    "            'Keyword Difficulty', 'CPC', 'Number of Words', 'Keyword Intents', \n",
    "            'SERP Features by Keyword', 'Position Type', 'Number of Results'\n",
    "        ]\n",
    "        url_cols = ['Client URL', 'Competitor URL'] \n",
    "\n",
    "        final_metric_cols = [col for col in metric_cols if col in gap_df.columns]\n",
    "        final_url_cols = [col for col in url_cols if col in gap_df.columns]\n",
    "        domain_cols = [your_domain] + competitor_cols\n",
    "        \n",
    "        # Combine all columns in the correct order\n",
    "        final_cols = (\n",
    "            ['Keyword'] + \n",
    "            final_metric_cols + \n",
    "            [col for col in domain_cols if col in gap_df.columns] +\n",
    "            final_url_cols # Add the URLs at the very end\n",
    "        )\n",
    "        \n",
    "        gap_df_sorted = gap_df[final_cols].sort_values(by='opportunity_score', ascending=False)\n",
    "\n",
    "        \n",
    "        print(\"\\n🎉 Success! Master gap analysis table created and scored.\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        print(\"Top 20 Content Opportunities (Head):\")\n",
    "        display(gap_df_sorted.head(20))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred during this step: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 5. Restrict Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(f\"✂️ Truncating results to the top {ROW_LIMIT} opportunities...\")\n",
    "\n",
    "if 'gap_df_sorted' not in locals():\n",
    "    print(\"❌ 'gap_df_sorted' not found. Please run the 'Tag \\'Em' cell first.\")\n",
    "else:\n",
    "    # 1. Get the original, full count of opportunities\n",
    "    original_count = len(gap_df_sorted)\n",
    "    \n",
    "    # 2. Truncate the DataFrame to the specified ROW_LIMIT\n",
    "    final_df_for_export = gap_df_sorted.head(ROW_LIMIT)\n",
    "    \n",
    "    # 3. Report the change\n",
    "    new_count = len(final_df_for_export)\n",
    "    print(f\"✅ DataFrame truncated from {original_count:,} rows to {new_count:,} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 6. Export to a Beautifully Formatted Excel File 🎨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Font, Alignment\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import platform\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def export_gap_analysis_to_excel(job: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Exports the GAPalyzer DataFrame to a professionally formatted Excel file\n",
    "    inside the 'output' folder.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"⚠️ DataFrame is empty, skipping file export.\")\n",
    "        return\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    excel_filename = output_dir / f\"{job}_gap_analysis.xlsx\"\n",
    "    \n",
    "    print(f\"🎨 Formatting and exporting data to Excel: {excel_filename}\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name='GAP_Analysis')\n",
    "        \n",
    "        worksheet = writer.sheets['GAP_Analysis']\n",
    "\n",
    "        # 1. Create an Excel Table for banded rows and filtering\n",
    "        table_range = f\"A1:{get_column_letter(worksheet.max_column)}{worksheet.max_row}\"\n",
    "        table = Table(displayName=\"GapAnalysisTable\", ref=table_range)\n",
    "        style = TableStyleInfo(name=\"TableStyleMedium9\", showFirstColumn=False,\n",
    "                               showLastColumn=False, showRowStripes=True, showColumnStripes=False)\n",
    "        table.tableStyleInfo = style\n",
    "        worksheet.add_table(table)\n",
    "\n",
    "        # 2. Define consistent column widths\n",
    "        width_map = {\n",
    "            \"Keyword\": 50,\n",
    "            \"Search Volume\": 18,\n",
    "            \"competitor_count\": 18,\n",
    "            \"opportunity_score\": 18,\n",
    "            \"Client URL\": 50,\n",
    "            \"Competitor URL\": 50\n",
    "        }\n",
    "        default_width = 15 # For competitor domains\n",
    "\n",
    "        # 3. Apply formatting\n",
    "        for col_idx, column_cell in enumerate(worksheet[1], 1):\n",
    "            column_letter = get_column_letter(col_idx)\n",
    "            header_text = str(column_cell.value)\n",
    "            \n",
    "            # A. Format header cell\n",
    "            column_cell.font = Font(bold=True)\n",
    "            column_cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "\n",
    "            # B. Set column width\n",
    "            width = default_width\n",
    "            # Find the matching width, even if header is 'your-domain.com'\n",
    "            for key, w in width_map.items():\n",
    "                if key.lower() in header_text.lower():\n",
    "                    width = w\n",
    "                    break\n",
    "            \n",
    "            # Special check for the client domain column\n",
    "            if header_text == your_domain:\n",
    "                width = 15 # Keep client domain column width standard\n",
    "\n",
    "            worksheet.column_dimensions[column_letter].width = width\n",
    "\n",
    "        # 4. Apply text wrapping and vertical alignment to all data cells\n",
    "        for row in worksheet.iter_rows(min_row=2):\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(wrap_text=True, vertical='top')\n",
    "\n",
    "    print(f\"✅ Success! Files saved in the '{output_dir}' folder.\")\n",
    "    \n",
    "    # --- Helper to open the output folder ---\n",
    "    def _open_folder(path_str: str = \".\"):\n",
    "        folder_path = Path(path_str).resolve()\n",
    "        print(f\"Attempting to open folder: {folder_path}\")\n",
    "        if not folder_path.exists() or not folder_path.is_dir():\n",
    "            print(f\"❌ Error: Path is not a valid directory: {folder_path}\")\n",
    "            return\n",
    "        system = platform.system()\n",
    "        try:\n",
    "            if system == \"Windows\": os.startfile(folder_path)\n",
    "            elif system == \"Darwin\": subprocess.run([\"open\", folder_path])\n",
    "            else: subprocess.run([\"xdg-open\", folder_path])\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to open folder. Please navigate to it manually. Error: {e}\")\n",
    "\n",
    "    # --- Display the \"Open Folder\" button ---\n",
    "    button = widgets.Button(\n",
    "        description=\"📂 Open Output Folder\",\n",
    "        tooltip=f\"Open {output_dir.resolve()}\",\n",
    "        button_style='success'\n",
    "    )\n",
    "    button.on_click(lambda b: _open_folder(\"output\"))\n",
    "    display(button)\n",
    "\n",
    "# --- Main Logic ---\n",
    "# ⭐️ MODIFIED: Check for the new 'final_df_for_export' variable\n",
    "if 'final_df_for_export' not in locals():\n",
    "    print(\"❌ 'final_df_for_export' not found. Please run the 'Truncate' cell first.\")\n",
    "else:\n",
    "    # ⭐️ MODIFIED: Pass the truncated DataFrame to the export function\n",
    "    export_gap_analysis_to_excel(job, final_df_for_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Todo\n",
    "- Make it work with Excel or CSV files\n",
    "- Bring your own CSV (Botify, GSC, etc) for joined rows & PageRank and other signals\n",
    "- A way to take ad hoc metrics into account with opportunity scores\n",
    "- Botify API integration in place of bring your own CSV\n",
    "- Each deliverable written to its own folder so clients don't see each other's files when Open Output is pressed\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
