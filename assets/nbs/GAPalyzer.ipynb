{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pipulate import pip\n",
    "import secretsauce\n",
    "import nest_asyncio\n",
    "import keys\n",
    "nest_asyncio.apply()\n",
    "\n",
    "job = \"gapalyzer-01\" # Give your session a unique name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Here are the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "pip.api_key(job, key=keys.google)\n",
    "botify_token = keys.botify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Here are your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "https://nixos.org/    # Linux\n",
    "https://pypi.org/     # Python\n",
    "https://neovim.io/    # vim\n",
    "https://git-scm.com/  # git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "def get_competitors_from_notebook(notebook_filename=\"GAPalyzer.ipynb\"):\n",
    "    \"\"\"Parses this notebook to get the domain list from the 'url-list-input' cell.\"\"\"\n",
    "    try:\n",
    "        notebook_path = Path(notebook_filename)\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        for cell in nb.cells:\n",
    "            if \"url-list-input\" in cell.metadata.get(\"tags\", []):\n",
    "                domains_raw = cell.source\n",
    "                domains = [\n",
    "                    line.split('#')[0].strip() \n",
    "                    for line in domains_raw.splitlines() \n",
    "                    if line.strip() and not line.strip().startswith('#')\n",
    "                ]\n",
    "                return domains\n",
    "        print(\"‚ö†Ô∏è Warning: Could not find a cell tagged with 'url-list-input'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading domains from notebook: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"üöÄ Generating SEMrush URLs for GAP analysis...\")\n",
    "\n",
    "domains = get_competitors_from_notebook()\n",
    "url_template = \"https://www.semrush.com/analytics/organic/positions/?db=us&q={domain}&searchType=domain\"\n",
    "\n",
    "if not domains:\n",
    "    print(\"üõë No domains found. Please add competitor domains to the 'url-list-input' cell and re-run.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(domains)} competitor domains. Click the links below to open each report:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, domain in enumerate(domains):\n",
    "        full_url = url_template.format(domain=domain)\n",
    "        print(f\"{i+1}. {domain}:\\n   {full_url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Turn 'em to Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"üöÄ Consolidating all SEMrush Excel files from Downloads...\")\n",
    "\n",
    "try:\n",
    "    # 1. Define the path to your Downloads folder\n",
    "    downloads_dir = Path.home() / \"Downloads\"\n",
    "    \n",
    "    # 2. Find all .xlsx files in the directory\n",
    "    # Using glob to find all Excel files, not just for one domain\n",
    "    all_excel_files = list(downloads_dir.glob(\"*.xlsx\"))\n",
    "    \n",
    "    if not all_excel_files:\n",
    "        raise FileNotFoundError(\"No .xlsx files found in your Downloads folder.\")\n",
    "        \n",
    "    print(f\"üîç Found {len(all_excel_files)} Excel files to process.\")\n",
    "    \n",
    "    list_of_dataframes = []\n",
    "    \n",
    "    # 3. Loop through each file, load it, and add the domain column\n",
    "    for file_path in all_excel_files:\n",
    "        try:\n",
    "            # 3a. Extract the domain name from the filename\n",
    "            # This splits the name at \"-organic.Positions\" and takes the first part\n",
    "            domain_name = file_path.name.split('-organic.Positions')[0]\n",
    "            \n",
    "            # 3b. Load the Excel file into a DataFrame\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # 3c. Add the new 'domain' column with the extracted name\n",
    "            df['domain'] = domain_name\n",
    "            \n",
    "            list_of_dataframes.append(df)\n",
    "            print(f\"  -> ‚úÖ Loaded and tagged {file_path.name} with domain '{domain_name}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ‚ö†Ô∏è Could not process file {file_path.name}: {e}\")\n",
    "\n",
    "    # 4. Concatenate all the individual DataFrames into one master DataFrame\n",
    "    if list_of_dataframes:\n",
    "        print(\"\\nConcatenating all DataFrames into a single master dataset...\")\n",
    "        combined_df = pd.concat(list_of_dataframes, ignore_index=True)\n",
    "        \n",
    "        print(\"\\nüéâ Success! All Excel files have been consolidated.\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # 5. Display info and head of the combined DataFrame\n",
    "        print(\"Combined DataFrame Info:\")\n",
    "        combined_df.info()\n",
    "        \n",
    "        print(\"\\nCombined DataFrame Head (First 5 Rows):\")\n",
    "        display(combined_df.head())\n",
    "        \n",
    "        # You can also check the last few rows to see data from another domain\n",
    "        print(\"\\nCombined DataFrame Tail (Last 5 Rows):\")\n",
    "        display(combined_df.tail())\n",
    "        \n",
    "    else:\n",
    "        print(\"ü§∑ No valid Excel files were processed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 3: Stack 'Em, Join 'Em & Tag 'Em! ü•ûüß†üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"üß© Joining 'Em: Pivoting data to map keywords against competitors...\")\n",
    "\n",
    "# --- USER CUSTOMIZATION ---\n",
    "# ‚ö†Ô∏è IMPORTANT: Replace this with your client's actual domain name.\n",
    "# This must exactly match the domain name extracted from the filenames.\n",
    "your_domain = \"your-client-domain.com\"\n",
    "# -------------------------\n",
    "\n",
    "if 'combined_df' not in locals():\n",
    "    print(\"‚ùå 'combined_df' not found. Please run the 'Stack \\'Em' cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. Select only the columns we need for the pivot. This is efficient.\n",
    "        pivot_data = combined_df[['Keyword', 'domain', 'Search Volume']]\n",
    "\n",
    "        # 2. Perform the pivot. This is the magic step.\n",
    "        # It turns the 'domain' rows into columns.\n",
    "        pivoted_df = pivot_data.pivot_table(\n",
    "            index='Keyword', \n",
    "            columns='domain', \n",
    "            values='Search Volume',\n",
    "            aggfunc='first' # Use 'first' since each keyword-domain pair should be unique\n",
    "        )\n",
    "\n",
    "        # 3. Fill missing values (NaN) with 0.\n",
    "        # This is CRITICAL. A 0 now explicitly means \"does not rank\".\n",
    "        pivoted_df = pivoted_df.fillna(0)\n",
    "\n",
    "        # 4. Ensure your domain is the first column for easy reference.\n",
    "        if your_domain in pivoted_df.columns:\n",
    "            cols = [your_domain] + [col for col in pivoted_df if col != your_domain]\n",
    "            pivoted_df = pivoted_df[cols]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Your specified domain '{your_domain}' was not found in the data.\")\n",
    "\n",
    "        print(\"\\nüéâ Success! Data has been pivoted into a competitive matrix.\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        print(\"Pivoted DataFrame Info:\")\n",
    "        pivoted_df.info()\n",
    "        \n",
    "        print(\"\\nPivoted DataFrame Head (First 5 Rows):\")\n",
    "        display(pivoted_df.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå An error occurred during the pivot process: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 4: Export to a Beautifully Formatted Excel File üé®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Todo\n",
    "- Make it work with Excel or CSV files\n",
    "- Bring your own CSV (Botify, GSC, etc) for joined rows & PageRank and other signals\n",
    "- A way to take ad hoc metrics into account with opportunity scores\n",
    "- Botify API integration in place of bring your own CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
