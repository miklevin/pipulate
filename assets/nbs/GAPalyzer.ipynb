{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# GAPalyzer üìê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import gap_analyzer_sauce\n",
    "from pipulate import pip\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import _config as keys\n",
    "\n",
    "job = \"gapalyzer-05\" # Give your session a unique name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Set all your Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "botify_token = keys.botify\n",
    "ROW_LIMIT = 3000\n",
    "COMPETITOR_LIMIT = 100\n",
    "BROWSER_DOWNLOAD_PATH = None\n",
    "GLOBAL_WIDTH_ADJUSTMENT = 1.5\n",
    "print(f'‚úÖ Configuration set: Final report will be limited to {ROW_LIMIT} rows.')\n",
    "if COMPETITOR_LIMIT:\n",
    "    print(f'‚úÖ Configuration set: Processing will be limited to the top {COMPETITOR_LIMIT} competitors.')\n",
    "else:\n",
    "    print(f'‚úÖ Configuration set: Processing all competitors.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. List all your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "# Enter one URL per line\n",
    "https://nixos.org/     # Linux\n",
    "https://jupyter.org/   # Python\n",
    "https://neovim.io/     # vim\n",
    "https://git-scm.com/   # git\n",
    "https://www.fastht.ml/ # FastHTML\n",
    "https://pipulate.com/  # AIE (Pronounced \"Ayyy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3. Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gap_analyzer_sauce # Import the new module\n",
    "\n",
    "# Call the function from the sauce module\n",
    "# This performs the extraction, stores domains via pip.set, prints URLs,\n",
    "# and returns the domains list if needed elsewhere (though we primarily rely on pip state now).\n",
    "competitor_domains = gap_analyzer_sauce.extract_domains_and_print_urls(job)\n",
    "\n",
    "# Optional: You could add a pip.get here for verification if desired\n",
    "# stored_domains = pip.get(job, 'competitor_domains', [])\n",
    "# print(f\"\\nVerification: Retrieved {len(stored_domains)} domains from pip state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### 4. Process the Rows\n",
    "\n",
    "## Verify Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gap_analyzer_sauce # Ensure module is imported\n",
    "\n",
    "# Call the function from the sauce module.\n",
    "# It handles moving files and storing relevant paths in pip state.\n",
    "# BROWSER_DOWNLOAD_PATH should be defined in a config cell near the top.\n",
    "semrush_dir, collected_files = gap_analyzer_sauce.collect_semrush_downloads(job, BROWSER_DOWNLOAD_PATH)\n",
    "\n",
    "# Optional verification (can be commented out for cleaner output)\n",
    "# if semrush_dir and collected_files:\n",
    "#    print(f\"\\nVerification: Files collected in '{pip.get(job, 'semrush_download_dir')}'\")\n",
    "#    print(f\"Files found/moved ({len(pip.get(job, 'collected_semrush_files'))}):\")\n",
    "#    # for f in pip.get(job, 'collected_semrush_files'): print(f\" - {Path(f).name}\") # Use Path for display if needed\n",
    "# elif semrush_dir:\n",
    "#    print(f\"\\nVerification: Destination directory '{pip.get(job, 'semrush_download_dir')}' confirmed, but no new files moved.\")\n",
    "# else:\n",
    "#    print(\"\\nVerification: File collection step encountered an error.\")\n",
    "\n",
    "# Call the function: It finds files, stores paths via pip.set, and returns Markdown summary\n",
    "# COMPETITOR_LIMIT should be defined in a config cell near the top\n",
    "markdown_summary = gap_analyzer_sauce.find_semrush_files_and_generate_summary(job, COMPETITOR_LIMIT)\n",
    "\n",
    "# Display the returned Markdown summary\n",
    "display(Markdown(markdown_summary))\n",
    "\n",
    "# Optional Verification (can be commented out)\n",
    "# stored_files = pip.get(job, 'collected_semrush_files', [])\n",
    "# print(f\"\\nVerification: Retrieved {len(stored_files)} file paths from pip state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Combine Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This one function now:\n",
    "# 1. Reads the file list from pip state.\n",
    "# 2. Loads and combines all SEMRush files into a master DataFrame.\n",
    "# 3. Applies the COMPETITOR_LIMIT.\n",
    "# 4. Stores the master DataFrame and competitor dictionary in pip state.\n",
    "# 5. Returns the master DataFrame (for the next step) and domain counts (for display).\n",
    "df2, domain_value_counts = gap_analyzer_sauce.load_and_combine_semrush_data(job, keys.client_domain, COMPETITOR_LIMIT)\n",
    "\n",
    "# Display the domain value counts for verification\n",
    "display(domain_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Make Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Pivoting df2 by Keyword/Domain.\n",
    "# 2. Calculating Competitors Positioning.\n",
    "# 3. Loading or creating the competitors_df and saving it to CSV.\n",
    "# 4. Printing summary statistics.\n",
    "# 5. Storing pivot_df and competitors_df in pip state.\n",
    "# It receives df2 directly from the previous cell's variable.\n",
    "pivot_df = gap_analyzer_sauce.pivot_semrush_data(job, df2, keys.client_domain)\n",
    "\n",
    "# Display the resulting pivot table\n",
    "display(pivot_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Pivot DF stored: {'keyword_pivot_df_json' in pip.read_state(job)}\")\n",
    "# print(f\"  Competitors DF stored: {'competitors_df_json' in pip.read_state(job)}\")\n",
    "# loaded_competitors = pd.read_json(pip.get(job, 'competitors_df_json', '[]'))\n",
    "# print(f\"  Competitors DF rows in state: {len(loaded_competitors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Filter Brand Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Loading competitors_df from pip state.\n",
    "# 2. Checking for and fetching missing homepage titles asynchronously.\n",
    "# 3. Updating competitors_df with new titles.\n",
    "# 4. Saving updated competitors_df to CSV and pip state.\n",
    "# 5. Generating the keyword filter list from domains and titles.\n",
    "# 6. Creating or updating the filter_keywords.csv file.\n",
    "# 7. Storing the filter keyword list in pip state.\n",
    "# It returns a status message.\n",
    "status_message = gap_analyzer_sauce.fetch_titles_and_create_filters(job)\n",
    "\n",
    "# Print the status message returned by the function\n",
    "print(status_message)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# updated_competitors_df = pd.read_json(StringIO(pip.get(job, 'competitors_df_json', '[]')))\n",
    "# print(f\"  Competitors DF rows in state: {len(updated_competitors_df)}\")\n",
    "# print(f\"  Example Title: {updated_competitors_df['Title'].iloc[0] if not updated_competitors_df.empty else 'N/A'}\")\n",
    "# filter_list = json.loads(pip.get(job, 'filter_keyword_list_json', '[]'))\n",
    "# print(f\"  Filter keywords stored: {len(filter_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Make Aggregate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Defining aggregation rules for each metric.\n",
    "# 2. Grouping df2 by Keyword and applying aggregations.\n",
    "# 3. Calculating 'Number of Words'.\n",
    "# 4. Dropping the aggregated 'Position' column.\n",
    "# 5. Storing the resulting agg_df in pip state.\n",
    "# 6. Returning agg_df for display and use in the next step.\n",
    "# It receives df2 directly from the previous cell's variable.\n",
    "agg_df = gap_analyzer_sauce.aggregate_semrush_metrics(job, df2)\n",
    "\n",
    "# Display the aggregated data\n",
    "display(agg_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Agg DF stored: {'keyword_aggregate_df_json' in pip.read_state(job)}\")\n",
    "# loaded_agg_df = pd.read_json(StringIO(pip.get(job, 'keyword_aggregate_df_json', '[]'))) # Use StringIO for verification\n",
    "# print(f\"  Agg DF rows in state: {len(loaded_agg_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Join Pivot & Aggregate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Merging pivot_df and agg_df.\n",
    "# 2. Reading the filter keyword list from the CSV file.\n",
    "# 3. Applying the brand/negative keyword filter.\n",
    "# 4. Reordering columns for readability.\n",
    "# 5. Dropping unnecessary columns (Traffic metrics, Previous position).\n",
    "# 6. Sorting the final DataFrame by Search Volume.\n",
    "# 7. Storing the final arranged_df in pip state.\n",
    "# It receives pivot_df and agg_df directly from previous cell variables.\n",
    "arranged_df = gap_analyzer_sauce.merge_filter_arrange_data(job, pivot_df, agg_df)\n",
    "\n",
    "# Display the final, arranged DataFrame\n",
    "display(arranged_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Arranged DF stored: {'filtered_gap_analysis_df_json' in pip.read_state(job)}\")\n",
    "# loaded_arranged_df = pd.read_json(StringIO(pip.get(job, 'filtered_gap_analysis_df_json', '[]'))) # Use StringIO\n",
    "# print(f\"  Final Arranged DF rows in state: {len(loaded_arranged_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Truncate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Truncate Data (Moved Upstream for Performance)\n",
    "#\n",
    "# We apply the ROW_LIMIT *before* merging with Botify data.\n",
    "# This speeds up the merge and all subsequent steps (clustering, Excel)\n",
    "# by operating on a much smaller, pre-filtered set of keywords.\n",
    "\n",
    "# %%\n",
    "# This function now handles:\n",
    "# 1. Iterating through volume cutoffs to find the best fit under ROW_LIMIT.\n",
    "# ... (comments) ...\n",
    "# 5. Returning the truncated DataFrame (aliased as 'df') for the next step.\n",
    "\n",
    "# It receives 'arranged_df' (the final_df from the previous step) and 'ROW_LIMIT' from config.\n",
    "# We will re-alias the output to 'arranged_df' so the next cell works.\n",
    "arranged_df = gap_analyzer_sauce.truncate_dataframe_by_volume(job, arranged_df, ROW_LIMIT)\n",
    "\n",
    "# Display the head of the final truncated DataFrame\n",
    "display(arranged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Download Botify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- START URGENT FIX: Bypassing stale kernel cache ---\n",
    "# We are redefining the function *locally* in this cell\n",
    "# to force the kernel to use the corrected version.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pipulate import pip # Make sure pip is imported\n",
    "import _config as keys # Make sure keys is imported\n",
    "\n",
    "# (Private helper functions _fetch_analysis_slugs, _export_data, etc., are assumed to be OK)\n",
    "# (If they are not, they would need to be pasted here too, but the error is in the main function)\n",
    "\n",
    "def fetch_botify_data_and_save(job: str, botify_token: str, botify_project_url: str):\n",
    "    \"\"\"\n",
    "    Orchestrates fetching data from the Botify API using pre-defined helpers,\n",
    "    handling slug detection, API calls with fallbacks, downloading, decompression,\n",
    "    and storing the final DataFrame in pip state.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Fetching data from Botify API...\")\n",
    "    report_name = None # Initialize report_name\n",
    "    csv_dir = None # Initialize csv_dir\n",
    "    botify_export_df = pd.DataFrame() # Initialize as empty DataFrame\n",
    "\n",
    "    # --- 1. Parse URL and get latest analysis slug ---\n",
    "    try:\n",
    "        cleaned_url = botify_project_url.rstrip('/')\n",
    "        url_parts = cleaned_url.split('/')\n",
    "        if len(url_parts) < 2:\n",
    "             raise ValueError(f\"Could not parse org/project from URL: {botify_project_url}\")\n",
    "\n",
    "        org = url_parts[-2]\n",
    "        project = url_parts[-1]\n",
    "        print(f\"  Parsed Org: {org}, Project: {project}\")\n",
    "\n",
    "        slugs = gap_analyzer_sauce._fetch_analysis_slugs(org, project, botify_token) # Call helper from module\n",
    "        if not slugs:\n",
    "            raise ValueError(\"Could not find any Botify analysis slugs for the provided project.\")\n",
    "        analysis = slugs[0] # Use the most recent analysis\n",
    "        print(f\"  ‚úÖ Found latest Analysis Slug: {analysis}\")\n",
    "\n",
    "    except (IndexError, ValueError, Exception) as e: \n",
    "        print(f\"  ‚ùå Critical Error during Botify setup: {e}\")\n",
    "        pip.set(job, 'botify_export_df_json', pd.DataFrame().to_json(orient='records')) # Use old key as fallback on error\n",
    "        return pd.DataFrame(), False, None, None \n",
    "\n",
    "    # --- 2. Define Paths and Payloads ---\n",
    "    try:\n",
    "        csv_dir = Path(\"data\") / f\"{job}_botify\"\n",
    "        csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "        report_name = csv_dir / \"botify_export.csv\"\n",
    "\n",
    "        payload_full = {\n",
    "            \"fields\": [\"url\", \"depth\", \"gsc_by_url.count_missed_clicks\", \"gsc_by_url.avg_ctr\", \"gsc_by_url.avg_position\", \"inlinks_internal.nb.unique\", \"internal_page_rank.value\", \"internal_page_rank.position\", \"internal_page_rank.raw\", \"gsc_by_url.count_impressions\", \"gsc_by_url.count_clicks\", \"gsc_by_url.count_keywords\", \"gsc_by_url.count_keywords_on_url_to_achieve_90pc_clicks\", \"metadata.title.content\", \"metadata.description.content\"],\n",
    "            \"sort\": []\n",
    "        }\n",
    "        payload_fallback = {\n",
    "            \"fields\": [\"url\", \"depth\", \"inlinks_internal.nb.unique\", \"internal_page_rank.value\", \"internal_page_rank.position\", \"internal_page_rank.raw\", \"metadata.title.content\", \"metadata.description.content\"],\n",
    "            \"sort\": []\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error defining paths/payloads: {e}\")\n",
    "        pip.set(job, 'botify_export_df_json', pd.DataFrame().to_json(orient='records')) # Use old key as fallback on error\n",
    "        return pd.DataFrame(), False, None, csv_dir \n",
    "\n",
    "    # --- 3. Main Logic: Check existing, call API with fallback ---\n",
    "    loaded_from_existing = False\n",
    "    if report_name.exists():\n",
    "        print(f\"  ‚òëÔ∏è Botify export file already exists at '{report_name}'. Reading from disk.\")\n",
    "        try:\n",
    "            botify_export_df = pd.read_csv(report_name, skiprows=1)\n",
    "            loaded_from_existing = True \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Could not read existing CSV file '{report_name}', will attempt to re-download. Error: {e}\")\n",
    "            botify_export_df = pd.DataFrame() \n",
    "\n",
    "    if not loaded_from_existing:\n",
    "        print(\"  Attempting download with Full GSC Payload...\")\n",
    "        status_code, _ = gap_analyzer_sauce._export_data('v1', org, project, payload_full, report_name, analysis=analysis)\n",
    "\n",
    "        if status_code not in [200, 201]: \n",
    "            print(\"    -> Full Payload failed. Attempting Fallback Payload (no GSC data)...\")\n",
    "            status_code, _ = gap_analyzer_sauce._export_data('v1', org, project, payload_fallback, report_name, analysis=analysis)\n",
    "\n",
    "        if report_name.exists():\n",
    "             try:\n",
    "                  botify_export_df = pd.read_csv(report_name, skiprows=1)\n",
    "                  print(\"  ‚úÖ Successfully downloaded and/or loaded Botify data.\")\n",
    "             except Exception as e:\n",
    "                  print(f\"  ‚ùå Download/decompression seemed successful, but failed to read the final CSV file '{report_name}'. Error: {e}\")\n",
    "                  botify_export_df = pd.DataFrame() \n",
    "        else:\n",
    "             print(\"  ‚ùå Botify export failed critically after both attempts, and no file exists.\")\n",
    "             botify_export_df = pd.DataFrame()\n",
    "\n",
    "    # --- 4. Store State and Return (THE FIX IS HERE) ---\n",
    "    has_botify = not botify_export_df.empty\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # We are storing the *path* to the CSV, not the *entire DataFrame*\n",
    "    # This avoids the TooBigError: string or blob too big\n",
    "    if has_botify:\n",
    "        pip.set(job, 'botify_export_csv_path', str(report_name.resolve()))\n",
    "        print(f\"üíæ Stored Botify CSV path in pip state for job '{job}': {report_name.resolve()}\")\n",
    "    else:\n",
    "        pip.set(job, 'botify_export_csv_path', None)\n",
    "        print(\"ü§∑ No Botify data loaded. Stored 'None' for path in pip state.\")\n",
    "    # --- END FIX ---\n",
    "\n",
    "    # Return necessary info for display logic in notebook\n",
    "    return botify_export_df, has_botify, report_name, csv_dir\n",
    "\n",
    "# --- END URGENT FIX ---\n",
    "\n",
    "\n",
    "# ... now your original cell content ...\n",
    "botify_export_df, has_botify, report_path, csv_dir_path = fetch_botify_data_and_save(\n",
    "    job,\n",
    "    keys.botify,\n",
    "    keys.botify_project_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Join Botify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Merging arranged_df with botify_export_df (if has_botify is True).\n",
    "# 2. Renaming Botify's 'url' to 'Full URL' for the merge.\n",
    "# 3. Inserting the new Botify columns neatly after the 'Competition' column.\n",
    "# 4. Cleaning up redundant URL columns used for the merge.\n",
    "# 5. Saving the intermediate 'unformatted.csv' file.\n",
    "# 6. Storing the final DataFrame in pip state ('final_working_df_json').\n",
    "# 7. Returning the final DataFrame (aliased as 'df') and a dict of data for display.\n",
    "\n",
    "# It receives arranged_df, botify_export_df, and has_botify from previous cells.\n",
    "df, display_data = gap_analyzer_sauce.merge_and_finalize_data(\n",
    "    job,\n",
    "    arranged_df,\n",
    "    botify_export_df,\n",
    "    has_botify\n",
    ")\n",
    "\n",
    "# --- Display Logic (Remains in Notebook, driven by return values) ---\n",
    "print(f\"Rows: {display_data['rows']:,}\")\n",
    "print(f\"Cols: {display_data['cols']:,}\")\n",
    "\n",
    "if display_data['has_botify'] and display_data['pagerank_counts'] is not None:\n",
    "    display(display_data['pagerank_counts'])\n",
    "elif display_data['has_botify']:\n",
    "    # This state means has_botify was true but 'Internal Pagerank' col was missing\n",
    "    print(\"‚ö†Ô∏è Botify data was merged, but 'Internal Pagerank' column not found for display.\")\n",
    "else:\n",
    "    # This state means has_botify was false\n",
    "    print(\"‚ÑπÔ∏è No Botify data was merged.\")\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Working DF stored: {'final_working_df_json' in pip.read_state(job)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Cluster Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one function now handles the entire clustering and finalization process:\n",
    "# 1. Loads/tests clustering parameters from a JSON cache file.\n",
    "# 2. Runs iterative ML clustering (TF-IDF, SVD, k-means) to find the best fit.\n",
    "# 3. Names the resulting clusters using n-grams.\n",
    "# 4. Performs the final column reordering.\n",
    "# 5. Saves the final 'unformatted_csv'.\n",
    "# 6. Prints the final cluster counts.\n",
    "# 7. Stores the final DataFrame in pip state ('final_clustered_df_json').\n",
    "# 8. Returns the final DataFrame for display.\n",
    "\n",
    "# It receives 'df' (the truncated DF) and 'has_botify' from previous cells.\n",
    "df = gap_analyzer_sauce.cluster_and_finalize_dataframe(job, df, has_botify)\n",
    "\n",
    "# Display the head of the final, clustered, and arranged DataFrame\n",
    "display(df.head())\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Clustered DF stored: {'final_clustered_df_json' in pip.read_state(job)}\")\n",
    "# loaded_clustered_df = pd.read_json(StringIO(pip.get(job, 'final_clustered_df_json', '[]')))\n",
    "# print(f\"  Clustered DF rows in state: {len(loaded_clustered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Deriving competitor info (list, lookup col) from pip state/keys.\n",
    "# 2. Creating the 'deliverables' directory and Excel file path.\n",
    "# 3. Normalizing and scoring the data for the \"Gap Analysis\" tab.\n",
    "# 4. Writing this first tab to the Excel file.\n",
    "# 5. Creating the \"Open Deliverables Folder\" button.\n",
    "# 6. Storing all necessary paths and lists ('final_xl_file', 'loop_list', 'competitors_list', etc.) in pip state.\n",
    "# 7. Returning the button and key variables for the next step.\n",
    "\n",
    "# It receives 'df' and 'has_botify' from previous cells.\n",
    "(\n",
    "    button, \n",
    "    xl_file, \n",
    "    loop_list, \n",
    "    competitors, \n",
    "    semrush_lookup, \n",
    "    TARGET_COMPETITOR_COL, \n",
    "    has_botify\n",
    ") = gap_analyzer_sauce.create_deliverables_excel_and_button(\n",
    "    job,\n",
    "    df,\n",
    "    keys.client_domain, # Pass the clean domain\n",
    "    has_botify\n",
    ")\n",
    "\n",
    "# Display the button\n",
    "display(button)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final XL File stored: {pip.get(job, 'final_xl_file')}\")\n",
    "# print(f\"  Loop List stored: {pip.get(job, 'loop_list')}\")\n",
    "# print(f\"  Competitors stored: {pip.get(job, 'competitors_list')}\")\n",
    "# print(f\"  Target Col stored: {pip.get(job, 'target_competitor_col')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Defining and finding the canonical client competitor column (TARGET_COMPETITOR_COL).\n",
    "# 2. Defining helper functions for reading/filtering keywords.\n",
    "# 3. Looping through all filter definitions (\"Important Keywords\", \"Best Opportunities\", etc.).\n",
    "# 4. For each filter:\n",
    "#    - Slicing the main 'df'.\n",
    "#    - Normalizing and scoring the slice.\n",
    "#    - Sorting the slice.\n",
    "#    - Appending it as a new tab to the existing Excel file.\n",
    "# 5. Re-attaching the click handler to the button.\n",
    "# 6. Returning the button for re-display.\n",
    "\n",
    "# It receives all necessary objects from the previous cells.\n",
    "button = gap_analyzer_sauce.add_filtered_excel_tabs(\n",
    "    job,\n",
    "    df,\n",
    "    semrush_lookup,\n",
    "    has_botify,\n",
    "    competitors,\n",
    "    xl_file,\n",
    "    TARGET_COMPETITOR_COL, # This was returned by the previous function\n",
    "    button\n",
    ")\n",
    "\n",
    "# Re-display the button (its on_click handler is preserved)\n",
    "display(button)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# try:\n",
    "#     wb = openpyxl.load_workbook(xl_file)\n",
    "#     print(f\"  Excel sheets: {wb.sheetnames}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"  Could not read Excel file to verify sheets: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font, Alignment, Border, Side\n",
    "from openpyxl.formatting.rule import ColorScaleRule\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "from openpyxl.utils import get_column_letter\n",
    "import re # Needed for is_safe_url\n",
    "import validators # Need to import validators for URL check\n",
    "import gap_analyzer_sauce # Ensure module is imported\n",
    "from IPython.display import display # Ensure display is imported\n",
    "\n",
    "# This is the final \"painterly\" step.\n",
    "# This one function now handles:\n",
    "# 1. Loading the Excel workbook from disk.\n",
    "# 2. Getting ALL sheet names from the workbook.\n",
    "# 3. Defining all helper functions for formatting (column mapping, cell finding, etc.).\n",
    "# 4. Defining all formatting rules (colors, widths, number formats, column groups).\n",
    "# 5. Iterating through each sheet and applying all 13+ formatting steps.\n",
    "# 6. Saving the final, polished Excel file.\n",
    "# 7. Returning the \"Open Folder\" button for re-display.\n",
    "\n",
    "# It receives all necessary objects from the previous cells.\n",
    "button = gap_analyzer_sauce.apply_excel_formatting(\n",
    "    job,\n",
    "    xl_file,\n",
    "    competitors,\n",
    "    semrush_lookup,\n",
    "    TARGET_COMPETITOR_COL,\n",
    "    has_botify,\n",
    "    GLOBAL_WIDTH_ADJUSTMENT, # This is from the config cell\n",
    "    button\n",
    ")\n",
    "\n",
    "# Re-display the button (its on_click handler is preserved)\n",
    "display(button)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification: Final formatting applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
