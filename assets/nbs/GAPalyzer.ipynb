{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import gap_analyzer_sauce\n",
    "from pipulate import pip\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import keys\n",
    "\n",
    "job = \"gapalyzer-03\" # Give your session a unique name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Set all your Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "secrets"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "botify_token = keys.botify\n",
    "ROW_LIMIT = 3000\n",
    "COMPETITOR_LIMIT = 3\n",
    "BROWSER_DOWNLOAD_PATH = None\n",
    "GLOBAL_WIDTH_ADJUSTMENT = 1.5\n",
    "print(f'‚úÖ Configuration set: Final report will be limited to {ROW_LIMIT} rows.')\n",
    "if COMPETITOR_LIMIT:\n",
    "    print(f'‚úÖ Configuration set: Processing will be limited to the top {COMPETITOR_LIMIT} competitors.')\n",
    "else:\n",
    "    print(f'‚úÖ Configuration set: Processing all competitors.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. List all your Foes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "url-list-input"
    ]
   },
   "source": [
    "# Enter one URL per line\n",
    "https://nixos.org/     # Linux\n",
    "https://jupyter.org/   # Python\n",
    "https://neovim.io/     # vim\n",
    "https://git-scm.com/   # git\n",
    "https://www.fastht.ml/ # FastHTML\n",
    "https://pipulate.com/  # AIE (Pronounced \"Ayyy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3. Save all of These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gap_analyzer_sauce # Import the new module\n",
    "\n",
    "# Call the function from the sauce module\n",
    "# This performs the extraction, stores domains via pip.set, prints URLs,\n",
    "# and returns the domains list if needed elsewhere (though we primarily rely on pip state now).\n",
    "competitor_domains = gap_analyzer_sauce.extract_domains_and_print_urls(job)\n",
    "\n",
    "# Optional: You could add a pip.get here for verification if desired\n",
    "# stored_domains = pip.get(job, 'competitor_domains', [])\n",
    "# print(f\"\\nVerification: Retrieved {len(stored_domains)} domains from pip state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### 4. Process the Rows\n",
    "\n",
    "## Verify Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gap_analyzer_sauce # Ensure module is imported\n",
    "\n",
    "# Call the function from the sauce module.\n",
    "# It handles moving files and storing relevant paths in pip state.\n",
    "# BROWSER_DOWNLOAD_PATH should be defined in a config cell near the top.\n",
    "semrush_dir, collected_files = gap_analyzer_sauce.collect_semrush_downloads(job, BROWSER_DOWNLOAD_PATH)\n",
    "\n",
    "# Optional verification (can be commented out for cleaner output)\n",
    "# if semrush_dir and collected_files:\n",
    "#    print(f\"\\nVerification: Files collected in '{pip.get(job, 'semrush_download_dir')}'\")\n",
    "#    print(f\"Files found/moved ({len(pip.get(job, 'collected_semrush_files'))}):\")\n",
    "#    # for f in pip.get(job, 'collected_semrush_files'): print(f\" - {Path(f).name}\") # Use Path for display if needed\n",
    "# elif semrush_dir:\n",
    "#    print(f\"\\nVerification: Destination directory '{pip.get(job, 'semrush_download_dir')}' confirmed, but no new files moved.\")\n",
    "# else:\n",
    "#    print(\"\\nVerification: File collection step encountered an error.\")\n",
    "\n",
    "# Call the function: It finds files, stores paths via pip.set, and returns Markdown summary\n",
    "# COMPETITOR_LIMIT should be defined in a config cell near the top\n",
    "markdown_summary = gap_analyzer_sauce.find_semrush_files_and_generate_summary(job, COMPETITOR_LIMIT)\n",
    "\n",
    "# Display the returned Markdown summary\n",
    "display(Markdown(markdown_summary))\n",
    "\n",
    "# Optional Verification (can be commented out)\n",
    "# stored_files = pip.get(job, 'collected_semrush_files', [])\n",
    "# print(f\"\\nVerification: Retrieved {len(stored_files)} file paths from pip state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Combine Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This one function now:\n",
    "# 1. Reads the file list from pip state.\n",
    "# 2. Loads and combines all SEMRush files into a master DataFrame.\n",
    "# 3. Applies the COMPETITOR_LIMIT.\n",
    "# 4. Stores the master DataFrame and competitor dictionary in pip state.\n",
    "# 5. Returns the master DataFrame (for the next step) and domain counts (for display).\n",
    "df2, domain_value_counts = gap_analyzer_sauce.load_and_combine_semrush_data(job, keys.client_domain, COMPETITOR_LIMIT)\n",
    "\n",
    "# Display the domain value counts for verification\n",
    "display(domain_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Make Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Pivoting df2 by Keyword/Domain.\n",
    "# 2. Calculating Competitors Positioning.\n",
    "# 3. Loading or creating the competitors_df and saving it to CSV.\n",
    "# 4. Printing summary statistics.\n",
    "# 5. Storing pivot_df and competitors_df in pip state.\n",
    "# It receives df2 directly from the previous cell's variable.\n",
    "pivot_df = gap_analyzer_sauce.pivot_semrush_data(job, df2, keys.client_domain)\n",
    "\n",
    "# Display the resulting pivot table\n",
    "display(pivot_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Pivot DF stored: {'keyword_pivot_df_json' in pip.read_state(job)}\")\n",
    "# print(f\"  Competitors DF stored: {'competitors_df_json' in pip.read_state(job)}\")\n",
    "# loaded_competitors = pd.read_json(pip.get(job, 'competitors_df_json', '[]'))\n",
    "# print(f\"  Competitors DF rows in state: {len(loaded_competitors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Filter Brand Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Loading competitors_df from pip state.\n",
    "# 2. Checking for and fetching missing homepage titles asynchronously.\n",
    "# 3. Updating competitors_df with new titles.\n",
    "# 4. Saving updated competitors_df to CSV and pip state.\n",
    "# 5. Generating the keyword filter list from domains and titles.\n",
    "# 6. Creating or updating the filter_keywords.csv file.\n",
    "# 7. Storing the filter keyword list in pip state.\n",
    "# It returns a status message.\n",
    "status_message = gap_analyzer_sauce.fetch_titles_and_create_filters(job)\n",
    "\n",
    "# Print the status message returned by the function\n",
    "print(status_message)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# updated_competitors_df = pd.read_json(StringIO(pip.get(job, 'competitors_df_json', '[]')))\n",
    "# print(f\"  Competitors DF rows in state: {len(updated_competitors_df)}\")\n",
    "# print(f\"  Example Title: {updated_competitors_df['Title'].iloc[0] if not updated_competitors_df.empty else 'N/A'}\")\n",
    "# filter_list = json.loads(pip.get(job, 'filter_keyword_list_json', '[]'))\n",
    "# print(f\"  Filter keywords stored: {len(filter_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Make Aggregate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Defining aggregation rules for each metric.\n",
    "# 2. Grouping df2 by Keyword and applying aggregations.\n",
    "# 3. Calculating 'Number of Words'.\n",
    "# 4. Dropping the aggregated 'Position' column.\n",
    "# 5. Storing the resulting agg_df in pip state.\n",
    "# 6. Returning agg_df for display and use in the next step.\n",
    "# It receives df2 directly from the previous cell's variable.\n",
    "agg_df = gap_analyzer_sauce.aggregate_semrush_metrics(job, df2)\n",
    "\n",
    "# Display the aggregated data\n",
    "display(agg_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Agg DF stored: {'keyword_aggregate_df_json' in pip.read_state(job)}\")\n",
    "# loaded_agg_df = pd.read_json(StringIO(pip.get(job, 'keyword_aggregate_df_json', '[]'))) # Use StringIO for verification\n",
    "# print(f\"  Agg DF rows in state: {len(loaded_agg_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Join Pivot & Aggregate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Merging pivot_df and agg_df.\n",
    "# 2. Reading the filter keyword list from the CSV file.\n",
    "# 3. Applying the brand/negative keyword filter.\n",
    "# 4. Reordering columns for readability.\n",
    "# 5. Dropping unnecessary columns (Traffic metrics, Previous position).\n",
    "# 6. Sorting the final DataFrame by Search Volume.\n",
    "# 7. Storing the final arranged_df in pip state.\n",
    "# It receives pivot_df and agg_df directly from previous cell variables.\n",
    "arranged_df = gap_analyzer_sauce.merge_filter_arrange_data(job, pivot_df, agg_df)\n",
    "\n",
    "# Display the final, arranged DataFrame\n",
    "display(arranged_df)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Arranged DF stored: {'filtered_gap_analysis_df_json' in pip.read_state(job)}\")\n",
    "# loaded_arranged_df = pd.read_json(StringIO(pip.get(job, 'filtered_gap_analysis_df_json', '[]'))) # Use StringIO\n",
    "# print(f\"  Final Arranged DF rows in state: {len(loaded_arranged_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Download Botify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This one function now handles the entire Botify data fetching process...\n",
    "# (comments remain the same)\n",
    "botify_export_df, has_botify, report_path, csv_dir_path = gap_analyzer_sauce.fetch_botify_data_and_save(\n",
    "    job,\n",
    "    keys.botify,\n",
    "    keys.botify_project_url\n",
    ")\n",
    "\n",
    "# --- Display Logic (Remains in Notebook) ---\n",
    "# (Display logic remains the same)\n",
    "if has_botify:\n",
    "    print(\"\\n--- Botify Data Summary ---\")\n",
    "    if \"Internal Pagerank\" in botify_export_df.columns:\n",
    "        display(botify_export_df[\"Internal Pagerank\"].value_counts())\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è 'Internal Pagerank' column not found for display.\")\n",
    "    print(\"-------------------------\\n\")\n",
    "    if report_path:\n",
    "        print(f\"üìÅ Botify data saved to: {report_path.resolve()}\")\n",
    "    if csv_dir_path:\n",
    "        print(f\"üìÇ Containing folder: {csv_dir_path.resolve()}\")\n",
    "else:\n",
    "    print(\"\\nNo Botify data loaded or available to display summary.\")\n",
    "\n",
    "# Optional verification (using pip.get and StringIO)\n",
    "# from io import StringIO\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Botify DF stored: {'botify_export_df_json' in pip.read_state(job)}\")\n",
    "# loaded_botify_df = pd.read_json(StringIO(pip.get(job, 'botify_export_df_json', '[]')))\n",
    "# print(f\"  Botify DF rows in state: {len(loaded_botify_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Join Botify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Merging arranged_df with botify_export_df (if has_botify is True).\n",
    "# 2. Renaming Botify's 'url' to 'Full URL' for the merge.\n",
    "# 3. Inserting the new Botify columns neatly after the 'Competition' column.\n",
    "# 4. Cleaning up redundant URL columns used for the merge.\n",
    "# 5. Saving the intermediate 'unformatted.csv' file.\n",
    "# 6. Storing the final DataFrame in pip state ('final_working_df_json').\n",
    "# 7. Returning the final DataFrame (aliased as 'df') and a dict of data for display.\n",
    "\n",
    "# It receives arranged_df, botify_export_df, and has_botify from previous cells.\n",
    "df, display_data = gap_analyzer_sauce.merge_and_finalize_data(\n",
    "    job,\n",
    "    arranged_df,\n",
    "    botify_export_df,\n",
    "    has_botify\n",
    ")\n",
    "\n",
    "# --- Display Logic (Remains in Notebook, driven by return values) ---\n",
    "print(f\"Rows: {display_data['rows']:,}\")\n",
    "print(f\"Cols: {display_data['cols']:,}\")\n",
    "\n",
    "if display_data['has_botify'] and display_data['pagerank_counts'] is not None:\n",
    "    display(display_data['pagerank_counts'])\n",
    "elif display_data['has_botify']:\n",
    "    # This state means has_botify was true but 'Internal Pagerank' col was missing\n",
    "    print(\"‚ö†Ô∏è Botify data was merged, but 'Internal Pagerank' column not found for display.\")\n",
    "else:\n",
    "    # This state means has_botify was false\n",
    "    print(\"‚ÑπÔ∏è No Botify data was merged.\")\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Working DF stored: {'final_working_df_json' in pip.read_state(job)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Truncate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Iterating through volume cutoffs to find the best fit under ROW_LIMIT.\n",
    "# 2. Handling edge cases (e.g., if filtering removes all rows).\n",
    "# 3. Printing the truncation log.\n",
    "# 4. Storing the truncated DataFrame in pip state ('truncated_df_for_clustering_json').\n",
    "# 5. Returning the truncated DataFrame (aliased as 'df') for the next step.\n",
    "\n",
    "# It receives 'df' (the final_df from the previous step) and 'ROW_LIMIT' from config.\n",
    "df = gap_analyzer_sauce.truncate_dataframe_by_volume(job, df, ROW_LIMIT)\n",
    "\n",
    "# Display the head of the final truncated DataFrame\n",
    "display(df.head())\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Truncated DF stored: {'truncated_df_for_clustering_json' in pip.read_state(job)}\")\n",
    "# loaded_truncated_df = pd.read_json(StringIO(pip.get(job, 'truncated_df_for_clustering_json', '[]')))\n",
    "# print(f\"  Truncated DF rows in state: {len(loaded_truncated_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Cluster Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one function now handles the entire clustering and finalization process:\n",
    "# 1. Loads/tests clustering parameters from a JSON cache file.\n",
    "# 2. Runs iterative ML clustering (TF-IDF, SVD, k-means) to find the best fit.\n",
    "# 3. Names the resulting clusters using n-grams.\n",
    "# 4. Performs the final column reordering.\n",
    "# 5. Saves the final 'unformatted_csv'.\n",
    "# 6. Prints the final cluster counts.\n",
    "# 7. Stores the final DataFrame in pip state ('final_clustered_df_json').\n",
    "# 8. Returns the final DataFrame for display.\n",
    "\n",
    "# It receives 'df' (the truncated DF) and 'has_botify' from previous cells.\n",
    "df = gap_analyzer_sauce.cluster_and_finalize_dataframe(job, df, has_botify)\n",
    "\n",
    "# Display the head of the final, clustered, and arranged DataFrame\n",
    "display(df.head())\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final Clustered DF stored: {'final_clustered_df_json' in pip.read_state(job)}\")\n",
    "# loaded_clustered_df = pd.read_json(StringIO(pip.get(job, 'final_clustered_df_json', '[]')))\n",
    "# print(f\"  Clustered DF rows in state: {len(loaded_clustered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Deriving competitor info (list, lookup col) from pip state/keys.\n",
    "# 2. Creating the 'deliverables' directory and Excel file path.\n",
    "# 3. Normalizing and scoring the data for the \"Gap Analysis\" tab.\n",
    "# 4. Writing this first tab to the Excel file.\n",
    "# 5. Creating the \"Open Deliverables Folder\" button.\n",
    "# 6. Storing all necessary paths and lists ('final_xl_file', 'loop_list', 'competitors_list', etc.) in pip state.\n",
    "# 7. Returning the button and key variables for the next step.\n",
    "\n",
    "# It receives 'df' and 'has_botify' from previous cells.\n",
    "(\n",
    "    button, \n",
    "    xl_file, \n",
    "    loop_list, \n",
    "    competitors, \n",
    "    semrush_lookup, \n",
    "    TARGET_COMPETITOR_COL, \n",
    "    has_botify\n",
    ") = gap_analyzer_sauce.create_deliverables_excel_and_button(\n",
    "    job,\n",
    "    df,\n",
    "    keys.client_domain, # Pass the clean domain\n",
    "    has_botify\n",
    ")\n",
    "\n",
    "# Display the button\n",
    "display(button)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# print(f\"  Final XL File stored: {pip.get(job, 'final_xl_file')}\")\n",
    "# print(f\"  Loop List stored: {pip.get(job, 'loop_list')}\")\n",
    "# print(f\"  Competitors stored: {pip.get(job, 'competitors_list')}\")\n",
    "# print(f\"  Target Col stored: {pip.get(job, 'target_competitor_col')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function now handles:\n",
    "# 1. Defining and finding the canonical client competitor column (TARGET_COMPETITOR_COL).\n",
    "# 2. Defining helper functions for reading/filtering keywords.\n",
    "# 3. Looping through all filter definitions (\"Important Keywords\", \"Best Opportunities\", etc.).\n",
    "# 4. For each filter:\n",
    "#    - Slicing the main 'df'.\n",
    "#    - Normalizing and scoring the slice.\n",
    "#    - Sorting the slice.\n",
    "#    - Appending it as a new tab to the existing Excel file.\n",
    "# 5. Re-attaching the click handler to the button.\n",
    "# 6. Returning the button for re-display.\n",
    "\n",
    "# It receives all necessary objects from the previous cells.\n",
    "button = gap_analyzer_sauce.add_filtered_excel_tabs(\n",
    "    job,\n",
    "    df,\n",
    "    semrush_lookup,\n",
    "    has_botify,\n",
    "    competitors,\n",
    "    xl_file,\n",
    "    TARGET_COMPETITOR_COL, # This was returned by the previous function\n",
    "    button\n",
    ")\n",
    "\n",
    "# Re-display the button (its on_click handler is preserved)\n",
    "display(button)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification:\")\n",
    "# try:\n",
    "#     wb = openpyxl.load_workbook(xl_file)\n",
    "#     print(f\"  Excel sheets: {wb.sheetnames}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"  Could not read Excel file to verify sheets: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% editable=true slideshow={\"slide_type\": \"\"}\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font, Alignment, Border, Side\n",
    "from openpyxl.formatting.rule import ColorScaleRule\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "from openpyxl.utils import get_column_letter\n",
    "import re # Needed for is_safe_url\n",
    "import validators # Need to import validators for URL check\n",
    "import gap_analyzer_sauce # Ensure module is imported\n",
    "from IPython.display import display # Ensure display is imported\n",
    "\n",
    "# This is the final \"painterly\" step.\n",
    "# This one function now handles:\n",
    "# 1. Loading the Excel workbook from disk.\n",
    "# 2. Getting ALL sheet names from the workbook.\n",
    "# 3. Defining all helper functions for formatting (column mapping, cell finding, etc.).\n",
    "# 4. Defining all formatting rules (colors, widths, number formats, column groups).\n",
    "# 5. Iterating through each sheet and applying all 13+ formatting steps.\n",
    "# 6. Saving the final, polished Excel file.\n",
    "# 7. Returning the \"Open Folder\" button for re-display.\n",
    "\n",
    "# It receives all necessary objects from the previous cells.\n",
    "button = gap_analyzer_sauce.apply_excel_formatting(\n",
    "    job,\n",
    "    xl_file,\n",
    "    competitors,\n",
    "    semrush_lookup,\n",
    "    TARGET_COMPETITOR_COL,\n",
    "    has_botify,\n",
    "    GLOBAL_WIDTH_ADJUSTMENT, # This is from the config cell\n",
    "    button\n",
    ")\n",
    "\n",
    "# Re-display the button (its on_click handler is preserved)\n",
    "display(button)\n",
    "\n",
    "# Optional verification\n",
    "# print(\"\\nVerification: Final formatting applied.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
