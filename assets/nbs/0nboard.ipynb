{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Welcome to Pipulate ðŸ‘‹\n",
    "\n",
    "You are looking at a **Jupyter Notebook**. It is a living document where text and code exist together. \n",
    "\n",
    "Right now, the system is asleep. Let's wake it up. \n",
    "\n",
    "**Click on the gray code block below, and press `Shift + Enter` on your keyboard (or click the â–¶ button at the top).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipulate import pip  # <-- the Pipulate \"magic wand\"\n",
    "\n",
    "pip.speak(\"Hello. I am Pipulate. Your local environment is active, and I am ready to begin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### You just ran your first Python code! ðŸŽ‰\n",
    " \n",
    "Did you hear a voice? If not, don't worryâ€”the text printed above the cell proves the engine is running.\n",
    "\n",
    "Pipulate is designed to give you **Local-First Sovereignty**. That means we don't rely on expensive cloud subscriptions. If your computer is powerful enough, you can run an AI directly on your own hardware to help you with your workflows. \n",
    "\n",
    "Let's check if you have a local AI brain installed. Press `Shift + Enter` on the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import socket\n",
    "\n",
    "def check_for_ollama():\n",
    "    pip.speak(\"Scanning your system for a local AI brain...\")\n",
    "    \n",
    "    # Try multiple common local addresses to bypass DNS/IPv6 routing quirks\n",
    "    addresses_to_try = [\n",
    "        \"http://127.0.0.1:11434/api/tags\",\n",
    "        \"http://localhost:11434/api/tags\",\n",
    "        \"http://0.0.0.0:11434/api/tags\"\n",
    "    ]\n",
    "    \n",
    "    for url in addresses_to_try:\n",
    "        try:\n",
    "            req = urllib.request.Request(url)\n",
    "            with urllib.request.urlopen(req, timeout=2) as response:\n",
    "                if response.getcode() == 200:\n",
    "                    data = json.loads(response.read())\n",
    "                    models = [model['name'] for model in data.get('models', [])]\n",
    "                    if models:\n",
    "                        pip.speak(f\"Excellent! I detect Ollama is running. You have {len(models)} models installed.\")\n",
    "                        print(f\"\\nâœ… Installed Models: {', '.join(models)}\")\n",
    "                    else:\n",
    "                        pip.speak(\"Ollama is running, but you don't have any models downloaded yet.\")\n",
    "                    return True\n",
    "        except (urllib.error.URLError, socket.timeout, ConnectionRefusedError):\n",
    "            continue # Try the next address\n",
    "        except Exception as e:\n",
    "            # We catch specific errors above. If something weird happens, let's see it.\n",
    "            print(f\"  [Debug] Error trying {url}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # The Fallback State (only reached if ALL addresses fail)\n",
    "    pip.speak(\"I do not detect a local AI brain on your system.\")\n",
    "    print(\"\\nâ„¹ï¸  Ollama is not running or not installed.\")\n",
    "    print(\"Pipulate works perfectly fine without it, but an AI 'riding shotgun' makes the experience much better.\")\n",
    "    print(\"\\nTo upgrade your environment:\")\n",
    "    print(\"1. Go to https://ollama.com/\")\n",
    "    print(\"2. Download the installer for your operating system (Mac/Windows/Linux).\")\n",
    "    print(\"3. Install it, and run this cell again.\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "has_ai = check_for_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipulate import pip\n",
    "pip.set(\"job1\", 'favorite_metric', 'Organic Traffic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "**Restart the Kernel**. Hit `Esc` then `0`, `0`. Then run the cell below. That's *in-Notebook* memory!\n",
    "\n",
    "This is a trust exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipulate import pip\n",
    "my_metric = pip.get(\"job1\", 'favorite_metric')\n",
    "pip.speak(f\"Your favorite metric is {my_metric}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Giving the Machine Eyes ðŸ‘€\n",
    "\n",
    "We have given the system a voice and a memory. Now, we must give it eyes.\n",
    "\n",
    "In the AI era, true power comes from **Browser Automation**. If an AI can control a browser, it can read any website, take screenshots, and process the actual rendered code of the internet. \n",
    "\n",
    "We use a battle-tested technology called Selenium to do this. Because you are running Pipulate locally, we can make the browser visible so you can see exactly what the machine is doing.\n",
    "\n",
    "**Run the next cell. Keep your hands off the mouse and watch what happens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will tell the system to visit a simple test page and take a screenshot.\n",
    "from pipulate import pip\n",
    "target_url = \"https://example.com\"\n",
    "pip.speak(f\"Initializing browser automation. I am now navigating to {target_url}.\")\n",
    "\n",
    "# Note: headless=False means the browser window will pop up on your screen!\n",
    "result = await pip.scrape(\n",
    "    url=target_url, \n",
    "    take_screenshot=True, \n",
    "    headless=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if result.get('success'):\n",
    "    pip.speak(\"Navigation complete. I have successfully captured the page data.\")\n",
    "    print(\"âœ… Scrape Successful!\")\n",
    "else:\n",
    "    pip.speak(\"I encountered an error while trying to navigate.\")\n",
    "    print(f\"âŒ Scrape Failed: {result.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### The Side Effects (The Magic of the File System)\n",
    "\n",
    "The browser popped up, went to the page, and closed. But what actually happened?\n",
    "\n",
    "When Pipulate scrapes a page, it doesn't just read the text. It acts like a Difference Engine, taking the raw material of the web and forging it into structured gears that an AI can use. \n",
    "\n",
    "It drops these files into a specific folder on your computer: `browser_cache/looking_at/`. \n",
    "\n",
    "Let's look at the \"side effects\" of that visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# We use the explicit URL-encoded path\n",
    "cache_dir = Path(\"browser_cache/example.com/%2F\")\n",
    "\n",
    "if cache_dir.exists():\n",
    "    pip.speak(\"Let's examine the artifacts I extracted. Click the button to open the folder on your computer.\")\n",
    "    print(f\"ðŸ“ Contents of {cache_dir}:\\n\")\n",
    "    \n",
    "    for item in cache_dir.iterdir():\n",
    "        if item.is_file():\n",
    "            size_kb = item.stat().st_size / 1024\n",
    "            print(f\" - {item.name} ({size_kb:.1f} KB)\")\n",
    "            \n",
    "    # Create the \"Open Folder\" button\n",
    "    button = widgets.Button(\n",
    "        description=f\"ðŸ“‚ Open Folder\",\n",
    "        tooltip=f\"Open {cache_dir.resolve()}\",\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    def on_button_click(b):\n",
    "        pip.open_folder(str(cache_dir))\n",
    "        \n",
    "    button.on_click(on_button_click)\n",
    "    display(button)\n",
    "\n",
    "else:\n",
    "    print(\"Directory not found. The scrape may not have completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The 'looking_at' directory is where Pipulate drops the immediate results of a scrape\n",
    "cache_dir = Path(\"browser_cache\")\n",
    "\n",
    "if cache_dir.exists():\n",
    "    pip.speak(\"Let's examine the artifacts I extracted from that website.\")\n",
    "    print(f\"ðŸ“ Contents of {cache_dir}:\\n\")\n",
    "    \n",
    "    # List the files and their sizes\n",
    "    for item in cache_dir.iterdir():\n",
    "        if item.is_file():\n",
    "            size_kb = item.stat().st_size / 1024\n",
    "            print(f\" - {item.name} ({size_kb:.1f} KB)\")\n",
    "            \n",
    "    print(\"\\nNotice the files:\")\n",
    "    print(\"â€¢ A raw HTML snapshot (`source.html`)\")\n",
    "    print(\"â€¢ A clean, readable Markdown version (`seo.md`)\")\n",
    "    print(\"â€¢ A structural map of the page (`dom_hierarchy.txt`)\")\n",
    "    print(\"â€¢ A physical screenshot (`screenshot.png`)\")\n",
    "    \n",
    "    pip.speak(\"These files are the building blocks of AI SEO. We can now feed these files into an LLM for deep analysis.\")\n",
    "else:\n",
    "    print(\"Directory not found. The scrape may not have completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Locate the pristine data we just scraped\n",
    "md_file = Path(\"browser_cache/example.com/%2F/accessibility_tree.json\")\n",
    "\n",
    "if md_file.exists():\n",
    "    content = md_file.read_text()\n",
    "    \n",
    "    # We will just use the first 2000 characters to keep it fast for the demo\n",
    "    prompt = f\"Based on the following DevTools accessibility tree extracted from a scrape, what is this page about? Answer in exactly 3 short bullet points.\\n\\n{content[:2000]}\"\n",
    "    \n",
    "    # 2. Find a local model to use (we checked this in Cell 4)\n",
    "    req_tags = urllib.request.Request(\"http://localhost:11434/api/tags\")\n",
    "    try:\n",
    "        with urllib.request.urlopen(req_tags, timeout=2) as response:\n",
    "            available_models = [m['name'] for m in json.loads(response.read()).get('models', [])]\n",
    "            \n",
    "        if available_models:\n",
    "            target_model = available_models[0] # Just grab the first available model\n",
    "            \n",
    "            pip.speak(f\"I am now interrogating the scraped data using your local AI brain, {target_model}. This analysis costs exactly zero cents.\")\n",
    "            \n",
    "            # 3. Ask the local AI to analyze the local file\n",
    "            req_generate = urllib.request.Request(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                data=json.dumps({\"model\": target_model, \"prompt\": prompt, \"stream\": False}).encode(\"utf-8\"),\n",
    "                headers={\"Content-Type\": \"application/json\"}\n",
    "            )\n",
    "            \n",
    "            with urllib.request.urlopen(req_generate) as ai_response:\n",
    "                result = json.loads(ai_response.read())\n",
    "                analysis = result.get(\"response\", \"\")\n",
    "                \n",
    "                print(f\"ðŸ¤– Analysis from {target_model}:\\n\")\n",
    "                print(analysis)\n",
    "                \n",
    "                pip.speak(\"Analysis complete. As you can see, I can read and summarize local files instantly, with total privacy.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not complete local AI analysis: {e}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Could not find {md_file}. Did the previous step complete successfully?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---\n",
    "*(Developer Tools below this line)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
